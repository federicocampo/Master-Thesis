\documentclass[11pt]{report}
%\documentclass[12pt]{article}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\pagestyle{plain}
\linespread{1}


%\usepackage[margin=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage[font=small,font = sl, labelfont=bf]{caption}
%\usepackage{epstopdf}
\usepackage{float}
%To create colored tables
%\usepackage[table,x11names]{xcolor}
\usepackage[dvipsnames]{xcolor}

\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{dirtytalk}
\usepackage{mathtools}
\usepackage{hyperref}

%\usepackage[style=numeric,sorting=none]{biblatex}
%\addbibresource{library.bib}

\usepackage{booktabs}
\usepackage{makecell}
%To create unnumbered section
\usepackage{blindtext}

% Packager for Spread-LaTeX
\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{soul}% for underlines
%\usepackage[table]{xcolor} % for cell colors
\usepackage{changepage,threeparttable} % for wide tables

\usepackage{comment}
%To write mathematical letters such as R, C etc..
\usepackage{amsfonts}

\usepackage{amsmath}
%To create a list of item in the same row
\usepackage[inline]{enumitem}

%To create a "note" space
\newlist{notes}{enumerate}{1}
\setlist[notes]{label=Note: ,leftmargin=*}


\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%\newcommand{\virgolette}[1]{``#1''}
%\title{Titolo Tesi}

\author{Federico}

%\graphicspath{{/home/federico/magistrale/Imagess/}}


\urlstyle{same}
%\hypersetup{
%colorlinks=true,
%linkcolor=black,
%citecolor=blue,
%}



\begin{document}

\begin{titlepage}
\begin{figure}[t]
\centering
\includegraphics[scale=0.55]{cherubino_pant541}
\end{figure}

\begin{center}
	\textbf{University of Pisa \\ Department of Physics E. Fermi\\ Master's degree in Physics\\}
	\vspace{20mm}
    {\LARGE{\bf Identification of alterations in functional brain connectivity with explainable Artificial Intelligence analysis of multicenter MRI data}}
\end{center}

\vspace{36mm}
\begin{minipage}[t]{0.47\textwidth}
	{\large{\bf Supervisors:\\ Prof. Alessandra Retico\\ Prof. Piernicola Oliva}}
\end{minipage}\hfill\begin{minipage}[t]{0.47\textwidth}\raggedleft
	{\large{\bf Candidate: \\ Federico Campo}}
\end{minipage}

\vspace{50mm}
\hrulefill
\\\centering{\large{\bf Academic year: 2021/2022}}

\end{titlepage}


\tableofcontents

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Autism spectrum disorder (ASD) is a neurodevelopmental disorder manifesting from the early ages and involving behavioral and cognitive impairments. So far, no univocal biomarker have been detected to identify this disorder, and the most accurate diagnostic tool remains a behavioural and attitudinal test.
In this work we focus on brain functional connectivity data to study the difference between patients with ASD and healthy controls. To tackle this challenge, we  employed machine learning and deep learning models.
Our database are Magnetic Resonance scans provided by the ABIDE dataset consisting on structrural and functional scans of approximately 2200 subjects collected from different medical centers.
One half of them belong to normal individuals and the other half to people with diagnosed ASD.

Starting from these data, the workflow of this thesis can be summarised in the following steps:

\begin{itemize}
\item MRI and fMRI data preprocessing and normalization to a common template, and extraction of temporal series of different brain areas for each patient from the fMRI scans.
\item Creation of a connectivity map for each patient. This connectivity matrix is created computing a correlation coefficient for every pair of timeseries of a single patient. Correlation coefficients were computed using two different approaches: a linear correlation of the two timeseries known as Pearson correlation, and a correlation based on time-frequency analysis of a timeseries pairs by means of wavelet transform.
\item Implementation of a harmonization procedure to correct for potential biases and distinctive traits of data, linked to acquisition scan procedures employed in a medical center which can differ between different centers.
\item Study and classification of connectivity matrices with deep learning methods. The two different connectivity matrices described above are compared to assess which one is able to yield the best separability between Controls and ASD.
Classification of raw data and harmonized data implemented with different procedures are compared, and in addition a harmonization implemented inside a deep learning model through an adversarial learning technique was tested.
\item Deep learning model is compared to a Random Forest classifier, a standard machine learning model, to determine if a deep model brings a benefit to this kind of analysis.
\item After the definition of the appropriate harmonization procedure, we implemented an interpretation method called SHAP to explain which features contributed the most to the final outcome of machine learning classification, and from them we determined the most relevant brain areas that allow a discerning between controls and subjects with ASD.
\end{itemize}

\chapter*{Organization of contents}
\addcontentsline{toc}{chapter}{Organization of contents}

\textbf{Background:} In this section we explain relevant theory arguments that lay the groundwork to all the thesis work.
In chapter \ref{chap:introduction} we outline the main problems associated with the Autism Spectrum Disorders, from the biological development to the social issues it generates. We briefly review some studies carried out so far to study this challenging issue. We also provide a brief overview of the main techniques employed in this work: from data acquisition to data analysis.

Chapter \ref{chap:mri_introduction} provides an overview of the basic physical principles of MRI imaging and the main acquisition sequences. It also includes a description of the physical and biological principles underlying the functional MRI modality which is the one we will examine in this work.

Chapter \ref{chap:machine_learning} provides a brief explanation of machine learning and deep learning models diving into the mathematical formulation of the main concepts. We aim to give an idea of the structure of a machine learning models, the learning procedure, and the important metrics for a proper evaluation of model performances.
Ultimately, we describe a common way to reduce data dimensionality keeping as much information as possible and reducing the number of uninformative or redundant features from data.

Chapter \ref{chap:shapley_values} describes the theory behind a specific strategy to inspect a deep learning model and understand the processes that brought it to a certain output given some input data.

\textbf{MATERIALS \& METHODS:} This section describes the instruments we used to run the analysis: dataset, the preprocessing steps, and the data preparation pipeline employed to obtain the correlation matrices.

Chapter \ref{chap:dataset} describes and analyze the ABIDE dataset, which is a publicly available collection of more than 2000 scans of healthy subjects and patients with ASD, collected from different medical centers.

Chapter \ref{chap:image_preprocessing} provides an overview of the most important preprocessing steps to normalize and align all these scan to a common template to run meaningful statistical analysis and to exctract brain timeseries form different brain areas aligned to a common coordinate system. We describe C-PAC which is the software employed to this image preprocessing and how we used it to conduct our processing.

Chapter \ref{chap:connectivity_coefficients} describes the two methods we used to calculate connectivity coefficients and create a functional connectivity matrix for each patient. We computed the connectivity by using the Pearson correlation; we illustrate the formalism of the wavelet transform for time-frequency analysis and the process to extract a correlation coefficient from time-frequency data of two timeseries.

Chapter \ref{chap:harmonization_theory} describes the regression model upon which the harmonization procedure is based. This is an analytical method to remove inter-site variability from our data and avoid biases towards the acquisition source.

Chapter \ref{chap:domain_adversarial_theory} describes the structure and the general idea behind a domain adversarial neural network: a deep neural network that aims
to make control vs ASD classification following a procedure that makes data independent from the acquisition site, while learning how to discriminate controls from subjects with ASD.

Chapter \ref{chap:shap} describes the main aspects of the SHAP algorithm for machine learning model explanation. We discuss the general idea behind its implementation and the quantities to determine the contribution of a feature to the output of a machine learning model.

\textbf{Implementation \& RESULTS:} In this last section, the results obtained applying methods to our materials are presented. We illustrate the results of harmonization applied to our data and classification result of Pearson-based correlation coefficients and wavelet-based coefficients and in the end we study the most relevant feature from our data, that played an important role in discrimination of control/ASD.
Chapter \ref{chap:harmonization_results} Presents the results of the analytical harmonization to our data to have a visual understanding of how this process affects a feature distribution.
Chapter \ref{chap:results_deeplearning} shows all the classification results obtained with a deep neural network on harmonized and non-harmonized (raw) data and the results of the domain adversarial neural network and compares them with results obtained using a simpler Random Forest classifier. Results of data dimensionality reduction are presented as well to assess if this is a meaningful procedure to reduce dimensionality on this dataset.
At the end, chapter \ref{chap:shap_results} shows the implementation of a feature importance assessment procedure and the results obtained with machine learning in the previous chapter. We show what feature contributed the most to the outcome of a prediction, and then to the discrimination of healthy and ASD patients, and subsequently we study what brain areas are mainly involved in this discernment.



\addcontentsline{toc}{chapter}{BACKGROUND}
%\chapter*{BACKGROUND}


\chapter{Introduction}\label{chap:introduction}
\section{Autism spectrum disorder} \label{chap:autism}
Autism spectrum disorder (ASD) is a neurological disorder that is drawing more and more attention, for its social impact on families and society and for the raise, in the last decades, in the number of diagnosed cases.
One in 44 children has been identified with ASD according to a recent study \cite{maenner2021}.
ASD is classified as a neurodevelopmental behavioural disorder \cite{guze-1995} \cite{who-1993} and refers to a broad range of conditions manifesting as deficits in social communication and interaction such as reduced sociability or empathy, repetitive behaviours, resistance to changes, and sometimes even speech difficulties.\cite{rapin-2008}
%\cite{hyman-2020}
To the present days, ASD is diagnosed through a comportamental assessment test since there is not a definite biological test.
Early signs start appearing by the age of 2 - 3, ages at which children usually start interacting with parents and with other children.
Parents are asked to answer a set of questions about every-day behaviour of their children like the checklist for Autism in Toddler \cite{robins-2009}.
However, are not uncommon cases where ASD is discovered only after the adolescence.
Currently ADI-R (Autism Diagnostic Interview - Revised) and ADOS (Autism Diagnostic Observation Schedule) are considered the ‘gold standard’ tools for diagnosis of ASD \cite{ozonoff-2015} \cite{lecouteur-2008}.

Nevertheless, besides these behavioural test, to achieve a more accurate diagnosis, biological information should be taken into account as well.
For a better treatment of this condition, it would be of great importance to make an early diagnosis, in order to provide in time the help and support people need.
Hopefully, treating children when this disorder is still it in its early stage, could bring to an improvement in their quality of life.

The main biological factors that bring to the developing of autism are not very clear so far, and is acquiring more consensus between neuroscientists and medical doctors that there is not just a single cause, but a concatenation and coexistence of various triggering factors.
Among them, the most strongly suspected are identified to be genetic and environmental, the latter including air pollution.

Genetic origin of this disorder is associated with a rare gene mutation, such as a deletion, duplication or inversion, and even though most of the mutation that increase the risk of developing autism are not been traced.
It has been assessed though, that it has high inheritance traits. From studies on twins it has been assesses that between omozygotes twins there is around 90 \% or probability that both of them develop ASD, while for heterozygote this probability falls to around 7\% for men and 1-2\% for women
\cite{freitag-2007}

%Ambiental factors such as prenatal air pollution or certain pesticides are also suspected to be associeted with a increase of the risk to develop this disorder, even thouh the etiology is way more complex and it can't be assessed so accurately.
%Currently ASD is diagnosided by syntoms-based tests concerning behaviour assessment.
From a neurological point of view, ASD may affect a brain both in its structure and in its functionality.

Structural studies usually focus on volumentric and morphometric analyzes to examine differences in brain anatomy.
It has been studied how ASD could alter the symmetry between the two emispheres \cite{postema2019} of the brain. In children it has been observed an increase in total brain volume as well as an enlargement of the left superior temporal gyrus. However but this trend is not well defined for older ages \cite{riddle-2017}.


Functional neuroimaging researches mainly focus on impaired functionality. Different studies pointed out a reduced information processing due to synaptic dysfunction that manifests in a reduced or altered brain functional connectivity.
Functional connectivity measures aim to describe statistical dependencies between brain areas in time, by studying temporal correlation between different brain parts even between those which are spatially separated.
Altered brain activity is found by different but controversal studies on functional connectivity (FC) both hyper-FC and hypo-FC were detected in ASD patients. The coexistence of an hyper- and hypo-FC traits has also been found between different areas of the brain.
In particular it seems that FC abnormalities have an age-dependent trend: over connectivity is usually observed in young children while under connectivity in adolescences and adults \cite{supekar-2013} \cite{spera-2019}.

\section{fMRI as a functional connectivity investigation tool}

In the last decades, different approaches to study ASD have been proposed, to find a relation between different brain areas involved in lower or higher functional connectivity.
Different diagnostic tools are being used to investigate this matter, from different perspectives and at different scales, such as magnetic resonance imaging (MRI), functional-MRI (fMRI), or electroencephalography (EEG).

Magnetic resonance imaging is employed to obtain images of brain both for structural studies and for functional through the  technique of functional MRI (fMRI).
An other tool employed to study temporal signals of the brain is the EEG which differs from fMRI because of the type of signal it is sensitive to and because of the different resolution in both signal and spatial domains. With EEG it is possible to study signals in a time scale comparable with the neuronal activation time (timescale $\sim \mu s$), while in fMRI is reported a signal due to the hemodynamic response following a neuronal activation (timescale $\sim s$).

fMRI is a diagnostic tool that investigates physiological changes linked to blood flow variations across the whole brain structure.
Measures of blood properties are strictly linked with measures of the neuronal activity: an increase of neuronal activity leads to a boost in blood flow and an increase of vessel size within a specific brain area, because of the bigger demand of oxigen and other nutrients to the neurons in that area.

The different neural connections are identified by measuring the blood-oxigen-level dependent (BOLD) fluctuations, from different areas of the brain, using the signal difference between oxy (diamagnetic) and deoxy-hemoglobin (paramagnetic).
%Oxyhemoglobin appears then brighter than deoxy-hemoglobin
The spontaneous fluctuations in the BOLD signal during resting state are considered a strong indicator for the assessment of the properties of the brain system.

fMRI can be acquired during the resting state of a patient or during a task.
We refer to these procedures as rs-fMRI and task-based fMRI. While task-based fMRI is a good instrument to investigate the local activation of a single brain area during a task, rs-fMRI was proven to be suitable for examining functional connectivity among all the brain regions, and highlight the difference between a normal brain and one affected by a disorder.
Resting state fMRI is drawing attention for the investigation of a number of disease evolutions including ASD.

Resting state fMRI is an fMRI acquisition carried out while the patient is in a relaxed state, and is not performing any active task.
The patient is simply asked to stay still with eyes closed or open while fixating a reference.
%either resting or, more in general in a task-negative state.
Following this setup, the brain is at rest and its activity is not perturbed by active tasks, such as moving an arm, or passive actions, like being exposed to different visual stimuli, which are common activities for task-based functional connectivity.
Resting state setup revealed to be a powerful tool to investigate the intrinsic generated brain activity and study the altered functional connectivity networks in subjects with mental disorders.

\section{Machine learning to deal with non-linear problems}\label{sec:intro_ml}
During the study of characteristic traits between controls and ASD patients, different attributes are involved and affect data in a strong and evident way, the most important of which are age, sex and the full intelligence quotient (FIQ).
Age, for example, affects both structural and functional data, with an observed overgrowth of the brain volume and hyper-connectivity, in toddlers. Both traits tend to decrease with increasing age.
Moreover, on control subjects, even though it is not fully characterized, brain structure and functional connectivity seem to decrease with age \cite{zhangC-2016}.

Sex is the other factor that gives characteristic traits to brain features.
Some brain areas exhibit an increased functional connectivity in female subjects, and, from a combined analysis of both age and sex, it appears that in men some brain structures show more pronounced aging effect than women \cite{coffey-1998}.

In addition, if we limit our focus to functional data, the eye status at scan plays an important role.
Functional connectivity with open steady eyes results to be different from that when data are acquired with closed eyes, with strong differences and higher connectivity in different brain areas between the two cohorts of subjects \cite{costumero-2020}.
Furthermore, an other aspect to be take into account regarding patients with closed eyes, is that during the scan acquisition, they may fall asleep, and this would heavily modify the functional brain activity, resulting in a modified functional connectivity pattern.

It is clear that the distinction of ASD patients from healthy subjects is not a straightforward task and a simple univariate analysis would not be sufficient, because of the presence of different confounding factors contributing to the characterization of data.

To take on this challenge, one of the most promising tools that allow us to deal with complex, non-linear problems is the use of machine learning algorithms to study data extracted from fMRI such as pairwise correlations between different regions of the brain
%or image of brain themselves from MRI scans to studies with convolutional neural networks (CNN).
%One of the most promising and powerful tool to tackle non-linear problems is machine learning.
Machine learning belongs to artificial intelligence (AI) tools and make use of algorithms that learn from data, and modify their parameters with the aim of recognizing distinctive properties from data and make predictions or classification, on new unseen data, trying to gradually reduce the error and make more accurate predictions.

A relevant aspect that makes machine learning and artificial neural networks so popular is the ability to deal with non-linear problems, by introducing several non-linear functions during training.
This allows to obtain non-linear outputs from each input data, which would hopefully help in the distinction between characteristic traits of healthy people and of ASD patients.

A drawback of machine learning is that, to perform well, an algorithm needs to be trained on datasets of large dimension, because the bigger the dataset, the more the algorithm is able to generalize information and the better are its performances on new data.
This is true for all kinds of machine learning algorithms and especially for Deep Neural Networks (DNNs) which are the principal family of algorithms we are going to employ in this work.

\section{The need for a data harmonization procedure}

To achieve the goal of obtaining a large dataset, particularly in medical field where data are not easily available from a single center or are not in a sufficient amount to perform a large-scale analysis, data from different acquisition centers need to be put together.
In the last years, this procedure is becoming increasingly popular and several multicenter medical datasets such as the Human Connectome Project, the Alzheimer’s Disease Neuroimaging Initiative (ADNI)\footnote{https://adni.loni.usc.edu/} or Autism Brain Imaging Data Exchange (ABIDE)\footnote{https://fcon\_1000.projects.nitrc.org/indi/abide/} were created to obtain large collections of data to make significant statistical analysis.
Unfortunately, this procedure brings with it a downside regarding the unavoidable bias towards the site that data belong to.
This bias is a result of hardware and scan procedure differences between centers and it is not feasible to ask to require that all the medical centers across the world use exactly the same instrumentation and uniform to a single common acquisition protocol.

To work out this problem and try to mitigate the inter-scan variability, we need a harmonization procedure to remove site-dependent information, leaving all the rest of important information unchanged.

In this work, two different approaches are proposed: an analytical harmonization and a deep learning approach.

Analytical harmonization is a procedure that modifies features extracted from images acquired at multiple sites with shifts and rescaling to remove only inter-site related effects, while trying to preserve all other information, as biological-related effects.

The second approach which is based on deep learning, tries to carry out the classification of control/ASD data extracting from input data both the information related to the class (control/ASD) and to the site.
Then it uses the latter to remove the bias linked to sites before discriminating subjects with ASD from controls.


\chapter{Principles of Magnetic Resonance Imaging}\label{chap:mri_introduction}
 In this chapter we report some basic principles that underlie medical imaging diagnostic modality based on the Nuclear Magnetic Resonance phenomenom.
We only provide a glimpse on what are the main principles behind this technique and the main acquisition sequences employed in diagnostics, whereas we remind to textbook for a more complete description \cite{brown-2014}.

\section{Physical principles}\label{sec:mri_introduction}
Magnetic Resonance Imaging (MRI), is a non-invasive imaging technique widely employed in radiology to obtain anatomical images or to study physiological processes, taking advantage of its flexible sensitivity to different tissues.

\begin{wrapfigure}[25]{l}{0.4\textwidth}
\centering
\includegraphics[width=1\linewidth]{mri/precessione_}
\caption{Representation of a angular moment vector $\vec{\mu}$ under a static magnetic field $\vec{B_0}$ and the resulting moment $\vec{\tau}$  which causes the precession}
\label{fig:precession}
\end{wrapfigure}


The physical principle this technique is based on, is the interaction of a nuclear spin of any tissue molecule, with an external magnetic field $\textbf B_0$.
Molecules that contribute the most to the signal are water, and specifically, hydrogen nuclei are the dominant source of signal in MRI.

When a nucleus is subjected to an external magnetic field, the interaction would make the nuclei align with the magnetic field, but, since protons and nuclei have an intrinsic spin, and hence an angular moment $\overrightarrow{\mu}$, they start precessing.

Precession occurs because of the torque created by the interaction of a static magnetic field with the angular moment of a spinning nucleus.
The relation is given by

\begin{equation}
\overrightarrow{\tau} = \overrightarrow{\mu} \times \overrightarrow{B_0}
\end{equation}

Precession occurs around the symmetry axis given by the direction of the magnetic field $\overrightarrow{B_0}$ as schematically shown in figure \ref{fig:precession}.

When a nucleus starts this rotatory movement, the characteristic frequency $\omega_0$ is uniquely determined by the properties of the nucleus itself and by the strength of the magnetic field; the relation is given by the Larmor equation and the frequency $\omega_0$ is called \textbf{Larmor frequency}

\begin{equation}\label{eq:Larmorfrequency}
\omega_0 = \gamma B_0
\end{equation}

where  $\gamma$ is a constant called \emph{gyromagnetic factor} and is it characteristic of each nucleus.
The gyromagnetic factor of a proton in water has a value of $\gamma = 2.7\cdot 10^8 \frac{rad\cdot s}{T}$.
For a typical scanner with a magnetic field of 2-3 T, the Larmor frequency assumes values of tens of MHz i.e. in the radio frequency (RF) range.

When protons in a body are exposed to a magnetic field, they can assume two states in relation to the direction of the external field: parallel and anti-parallel (as an example in figure \ref{fig:precession} a parallel configuration is represented).

\textcolor{ForestGreen}{
Proton with different orientation with respect to the magnetic field have different energies.
The potential energy of a particle with magnetic moment $\vec{\mu}$ immersed in a magnetic field $\vec{B_0}$ is given by
\begin{equation}
U = -  \vec{\mu} \cdot \vec{B_0}
\end{equation}
If we suppose $\vec{B_0}$ along the z axis, it will have only component along z, so the scalar product will just concern the z component of the magnetic moment $\mu_z$.
Recalling the relation between the angular moment and the magnetic moment $\vec{\mu} = \gamma \vec{J}$, and the quantization of the angular moment $J_z = m_s \hbar$.
$m_s$ represents the spin of the proton and can assume values $-\frac{1}{2} \text{ or } +\frac{1}{2}$.
The potential energy of a proton when subjected to a magnetic field can be written as:
\begin{equation}
U = -\vec{\mu} \cdot \vec{B_0} = - \mu_z B_z = - \gamma m_s \hbar B_z = - m_s \hbar \omega_0 = \pm \frac{1}{2} \hbar \omega_0 \ .
\end{equation}
In the formula above, energy with negative sign (associated to proton with spin $+1/2$) corresponds to proton aligned in parallel with the magnetic field, while energy with positive sign belongs to proton in an anti-parallel state.
For a thermodynamic system, with a given temperature T, the probability to find particles with energy $\epsilon$ is given by the Boltzmann distribution
\begin{equation}
P(\epsilon) = \frac{1}{Z} e^{-\frac{\epsilon}{kT}}
\end{equation}
where k is the Boltzmann's constant, and Z is the partition function of the system $Z = \sum_\epsilon e^{-\epsilon/kT}$.
Using this relation we can compute the difference between the number of proton in the two states parallel and anti-parallel
\begin{equation}
\Delta N = N^+ - N^- = \frac{N}{Z}(e^{-\frac{\hbar \omega_0}{kT}} - e^{\frac{\hbar \omega_0}{kT}}) \approx \frac{N}{2}\frac{\hbar \omega_0}{kT}
\end{equation}
where the approximation of the exponential since for human body is $\hbar \omega_0 \ll kT$.
What we obtain with this formula is that the number of parallel and anti-parallel protons is not the same, and the reason behind this comes from energetic considerations since protons with spin parallel to the magnetic field have a lower potential energy.
This imply that when a body is exposed to a magnetic field, this small excess of number of protons in an parallel state cause the body to have an intrinsec magnetization $\vec{M}$.
%The difference in the population of the two energy levels above is responsible for a net magnetization of the sample.
}

%If we compute the difference between the number of proton in these two states we can determine the net magnetization of a body, expressed by a vector pointing parallel or anti-parallel to the direction of $\overrightarrow{B_0}$.
This net magnetization can be represented by a single vector pointing parallel to the direction of the external field and if we want to detect a signal related to this magnetization, we have to perturb this vector from its equilibrium state.
The perturbation would cause the precession of this magnetization vector around the direction of the magnetic field, with its own Larmor frequency.
This precession will produce a changing \textcolor{ForestGreen}{magnetic} flux which can be detected by an external coil.
% only if it is perturbated from its equilibrium state.
To kick this vector away from its equilibrium state, a second external magnetic field is employed under the form of a radiofrequency pulse, with a frequency resonating with the Larmor frequency of the precessing spins.
\textcolor{ForestGreen}{
This radiofrequency pulse is usually applied to rotate spins in a direction ortogonal to the static magnetic field $\vec{B_0}$ which we can suppose to be along the z axis $\vec{B_0} = B_0 \hat z$
}
%A common way to apply this radiofrequency impulse is with the purpose of flipping the magnetization vector and hence the spins in a direction usually perpendicular to the original along $\overrightarrow{B_0}$ one, which we can suppose to be along $\hat z$ axis.
We create this way a magnetization component onto the x-y plane, aligned with the direction of the radiofrequency pulse.
This magnetization vector on transverse (x-y) plane is called \emph{transverse magnetization},\textcolor{ForestGreen}{ and can be denoted as $\vec{M_\perp} = M_x \hat x + M_y \hat y$.}
After this radio frequency pulse, protons will start rotating in the x-y plane, but, since this pulse is not persistent, after it is switched off they gradually lose their initial energy and tend to realign to the z-axis.

This process of realigning to the z-axis is called longitudinal relaxation (longitudinal with respect to the original $B_0 \hat z$ direction).
%In other words, we create a transverse magnetization by applying a rf-pulse, so that when we have a transverse magnetization, we lost the longitudinal magnetization because the vector is flipped and lie on the x-y plane.
%Gradually, though, the transverse vector lose its magnitude in the x-y plane because of the gradually loss of energy of protons and the longitudinal magnetization reacquire its previous magnitude (longitudinal in relation to the original $B_0 \hat z$ direction).

The evolution of the longitudinal and transverse magnetization are modeled with the introduction of two time constant, $T_1 \text{ and } T_2$, respectively.
Longitudinal relaxation occurs because the system goes from higher energy state (when it is flipped on the x-y plane) to a state of thermodynamic equilibrium with its surrondings, regaining its previous magnitude.

This process follows an exponential decay in time shown in equation \ref{eq:spinlattice}, with a time constant $T_1$, which for the physical reason underlying it, is also called spin-lattice relaxation time:

\begin{equation}\label{eq:spinlattice}
M_z(t) = M_0 (1-e^{-t/T_1}) \ .
\end{equation}


The transverse magnetization vector follows a different evolution.
%Before introducing the T2 parameter associated with its evolution, we establish a notation to indicate this magnetization vector: since it lies on the x-y plane it has two component so we can write $\overrightarrow{M_\perp} = M_x \hat x + M_y \hat y$.
When studying the evolution \textcolor{ForestGreen}{of $\vec{M_\perp}$}, an other effects has to be taken into account: the spin-spin interaction between protons.
If we consider a physical system where protons are immersed in a magnetic field, each proton interacts with this external field plus all the small fields created by the surrounding protons and their associated spins. This leads to the presence of a local field for each proton, slightly different from the external magnetic field $\vec{B_0}$.
These different local fields affect the evolution of the transverse magnetization because they cause a relative dephasing of protons among each other.
According to this, if immediately after the RF pulse all the spins can be imagined as aligned to the x axis, they start to fan out.
This effect speeds up the loss of the total transverse magnetization on the x-y plane.

If we study this evolution from the rotating reference frame, (rotating with the same Larmor frequency of protons) the module of the transverse magnetization follows an exponential decay in the form of
\begin{equation}
M_\perp(t) = M_\perp(0) e^{-t/T_2} \ .
\end{equation}.

Since $T_2$ constant comprises the spin-spin interaction process plus the spin-lattice interaction which cause the realignment of spins along the $\hat z$ axis, it is always smaller than $T_1$.
For protons in human tissue, $T_1$ ranges from 10 to 100 milliseconds, while $T_2$ is usually of the order of 10 milliseconds.


In a real phyisical system, however, there is and additional dephasing source, coming from external field inhomogeneities.
This effect is often taken into account by the introduction of a different decay time $T_2'$ which along with $T_2$ bring to a overall time costant given by
\begin{equation}
\frac{1}{T_2*} = \frac{1}{T_2}+\frac{1}{T_2'}
\end{equation}


\section{Image acquisition and k-space}

%fem
The physical process underlying signal acquisition is Faraday induction, according to which an electrical potential, called electromotive force (emf), is generated by the variation of a magnetic flux over time, $emf = -\frac{d\Phi}{dt}$, being $\Phi$ the varying flux through the receiving coil.

A simple setup in MRI to generate a varying flux over time is the application of a single RF pulse, \textcolor{ForestGreen}{which is able to make the magnetization rotate by an angle of $\pi/2$ (flip angle) with respect to the direction of the magnetic field $\vec{B_0}$}.
The variation of magnetic flux occurs while tipped spins are rotating in the x-y plane and the magnetization vector is relaxing towards the longitudinal axis.
This induced emf signal can be detected by properly oriented and tuned RF coils.
This simple experiment is referred as Free Induction Decay (FID) and is usually performed in any MRI scan to tune RF coils and optimise system response.

Usually the varying flux we are interested in is the one along the transversal plane.
The reasons behind this are mainly two:
\begin{enumerate*}
\item the weak signal along the longitudinal axes would be saturated by the strong magnetic field of the static magnetic field $ \vec{B_0}$.
\item The signal coming from the transverse magnetization $M_\perp$ is representative of all the main information we need for the analysis of a material: $T_1, T_2$ and the proton density $\rho$.
\end{enumerate*}

\hfill

To properly create an image based on these information we need to spatially encode each signal received \textcolor{ForestGreen}{by the changing magnetic flux}.
In order to relate a signal with a spatial position, in addition to the initial static magnetic field $\vec{B_0}$ we have to place a second field which causes a controlled local modification of the magnetic field $\vec{B_0}$. This additional field $\vec{B'}$ needs to be non-uniform and to have a lower magnitude of $\vec{B_0}$ for each point.

Its distribution follows a spatial gradient so that the total magnetic field along $\hat z$-axis is given by the sum of this two contributes, and the signal contains space-varying frequency components according to equation \textcolor{ForestGreen}{\ref{eq:Larmorfrequency}} which can be rewritten as
\begin{equation}
\omega(z) = \gamma B(z)
\end{equation}
being z the spatial coordinate and where $B(z)$ is the total magnetic field now given by the relation
\begin{equation}
B(z, t) = B_0 + z\cdot G(t) \qquad G(t) =\partial B'/\partial z
\end{equation}

This spatial changes in magnetic field causes different parts of the body along the z axis to have different Larmor frequencies.
This gradient along the longitudinal direction of the static field $\vec{B_0}$ is usually referred as Slice Selection gradient $G_{ss}$.
Choosing the z axis as the direction of the slice selection, we acquire images on the transversal x-y plane.


When acquiring an image, data are collected under the form of a matrix called k-space.
K-space is a coordinate system used to store spatial frequencies information. From these information we can retrieve the usual MRI image (containing spatial, anatomical information) by applying the inverse 2D Fourier transform.

In a k-space matrix, low spatial frequencies, corresponding to large object across the whole real image, are encoded at the center of the matrix and high spatial frequencies corresponding to small objects and finer details, are encoded in the peripheries.

The construction of k-space is done step by step in relation to the combination of applied RF pulse and fields time by time, producing a trajectory on the k-space.

To acquire spatial information in the x-y plane, we need to introduce two new gradients (one for each direction): a readout (or frequency encoding) gradient and a phase encoding gradient.

The frequency encoding gradient acts on one direction in the transversal plane; let us identify this direction as the x axis for a clearer visualization.
Just like the slice selection gradient, it consists of a linear changing magnetic field to modify the Larmor frequency of the spins along the x axis. According to the acquisition technique we want to perform, it can be applied forward and after a while, reversed. This allows a refocusing of all the dephased spins due to spin-spin interaction; we will see more in detail this concept when we will discuss acquisition sequences. For now, we just need to know that the refocusing of all the dephased spins occurs after a time interval called echo time (TE).

The phase encoding gradient acts the exact same way of the previous one, but it acts on the y axis. It is switched on and acts by modifying the Larmor frequency but it is just a temporary change. When it is switched off, all the spins along this direction continue precessing all at the same frequency, but with a relative phase between them.

The combined action of these two gradients allow us to acquire different lines of the k-space.
%Each spatial line of k-space is acquired after a time interval called \textbf{Repetition time (TR)}.


%For further and more detailed information we refer to the book \cite{brown-2014}

\section{Acquisition sequences}
Depending on the type of analysis we want to carry out, we have to focus on some magnetic properties rather than others.
For example, for a an anatomical (structural) image, we may be interested in a good contrast between different tissues, while in a functional scan we are interested in detecting temporal signal changes.

Two main parameters drive the differences between different acquisition sequences.
For the moment we define them, but a contextualized use of them can help to clarify their meaning.
This can be found in the following sections describing different acquisition sequences.

\begin{itemize}
\item \textbf{Repetition time (TR)} is the time interval between one RF pulse and the next.
\item \textbf{Echo time (TE)} is the time interval between a RF pulse and the echo peak
\end{itemize}

Changes in TR, TE and RF pulses characteristics (such as angle or number of pulses), allow to perform different acquisition sequences and to emphasise one of the three fundamental contrasts between different tissues based on the parameters $T_1, T_2$ or the proton density $\rho$.

As a general rule:
\begin{itemize}
\item $T_1$-weighted images are acquired with small values of TR. This avoid that all nuclei are back to their longitudinal position, allowing a better tissue contrast. TE has to be very short to avoid contribution due to $T_2$-related effects.
\item $T_2$-weighted images are acquired with high values of TR ($TR \gg T_1$) and values of TE comparable with $T_2$ time constant.
\item To enhance the spin density contrast, TR has to be chosen as long as possible while TE needs to be short.
\end{itemize}


\subsection{Spin-echo sequence}

%Sometimes, though, this additional $T_2'$ is so small compared to $T_2$, that dominates over it, resulting in a rapid loss of dephasing information.
%This lost phase information can be recovered employing a specific RF pulse sequence called \textbf{spin-echo acquisition sequence}.
Spin echo (SE) sequence is one of the most used one.
It lets us retrieve lost information due to spin dephasing caused by magnetic field inhomogeneities.

The spin echo method employs two RF pulses: the first \textcolor{ForestGreen}{flips protons by an angle of $\pi/2$} with respect to the $\vec{B_0}$ direction and the second of an angle $\pi$ in relation to the direction of the first pulse.
All the sequence can be summarized by the following steps:
\begin{enumerate}
% Add a figure like on page 123
\item The first radio-frequency pulse is applied with an angle of $\pi/2$ in relation to the direction of the external magnetic field $\overrightarrow{B_0}$. This process as mentioned in section \ref{sec:mri_introduction} causes the spin flip onto the transversal plane x-y.
Just as an example, we imagine they are flipped into the x direction. They gradually start dephasing both because of spin-spin interactions and small external field inhomogeneities, which are different from point to point.
Because of this, they start fanning out, because some rotate faster and some are delayed with respect to the average magnetization vector.

\item At the instant t = $\tau $ the second RF pulse is sent. It is created \textcolor{ForestGreen}{in order to flip protons by angle of $\pi$ with respect to their direction after the first impulse, so if the first impulse flips them along the $\hat x$ direction, this overturns them along $-\hat x$.
This has the effect of turning all spins with a phase $\phi$ to a phase $\pi - \phi$.}

\item \textcolor{ForestGreen}{Dephasing spins continue accumulating a phase, however, since they were flipped by 180 degrees, the same process according to which they were previously fanning out, push them to converge.
In fact, in an interval $\Delta t$ two spins have accumulated a phase
$\Delta \phi (t+\Delta t) = -\gamma \delta B \Delta t$,
due to an inhomogeneity in magnetic field $\delta B_0$, after they are turned over, this relative phase becomes
$\Delta \phi (t+\Delta t)= + \gamma \delta B \Delta t$. $\delta B_0$,
though, continue causing a dephasing, that after an further interval $\Delta t$ results in a total dephasing
$\Delta \phi (t+2 \Delta \phi) = + \gamma \delta B \Delta t - \gamma \delta B \Delta t = 0$.
This rephasing results in a recreated transverse magnetization vector (echo) in the opposite direction of the first one created after the $\pi/2$ pulse.
}
\end{enumerate}

\textcolor{ForestGreen}{
With this procedure, it is possible to recover the loss of transversal magnetization due to inhomogeneities of the magnetic field.
}
The time interval covering the application of the first impulse $\pi/2$ to the instant at which phases turn back to zero is called \textbf{echo time} TE.
Between the application of the $\pi$ pulse and the echo time, the acquisition sampling of k-space starts.


This method is an effective strategy to remove the nuisance effect due to field inhomogeneities associated to $T_2'$, but do not reduce the effect of $T_2$ due to local fields and spin-spin interactions. The reason behind this is that static field inhomogeneities remain the same even after the $\pi$ pulse, so they act the same way even after the pulse, while local fields which cause spin-spin interactions change and the rate at which they accumulate phase changes with them. In general no refocus strategy is possible to correct for this effect.
Fortunately, this is not a critical issue for liquids since the time interval over which data are collected is usually smaller than the $T_2$ time constant.



\subsection{Gradient Echo sequences}
The sequence employed to acquire functional imaging is the Echo Planar Imaging (EPI) which is a particular sequence from the family of gradient echo (GE) sequences.
%A different acquisition sequence is the gradient echo.
Gradient echo sequences differ from spin echo because they allow a faster acquisition.
They use a single excitation pulse with a flip angle less than 90 deg, this allows a minor time interval to make spins back to their longituginal component as they are not completely flipped on the transverse plane.

The transverse component just created decay and dephase according to $T_2*$.
%If along the z-axis a gradient magnetic field is added to the primary magnetic field $\vec{B_0}$, spins dephase faster because of this additional field perturbation.
At this point in spin echo sequence, the $\pi$ pulse would be applied; in its place, in GE sequences, the gradient along the x axis is reversed.
This gradient inversion causes the spins to rephase but what is relevant is that this gradient inversion does not act neither on dephasing due to field inhomogeneities nor on dephasing due to chemical shifts, but just on dephasing due to the gradient field.
This is the most important aspect that makes GE different from SE: the relaxation is dictated by $T_2*$ and not just by $T_2$.
In gradient echo sequence, the echo peak lies on $T_2*$ decay curve while in a spin echo, it lies on $T_2$ decay curve.
Thanks to this property, this image acquisition is susceptible to any chemical variation such as that due to hemoglobin shift.



\subsection{Echo Planar Imaging}\label{sec:epi}
Echo Planar Imaging (EPI) refers to a technique to acquire the entire 2D k-space after a single RF pulse. This is achieved by applying intermittent small-phase encoding gradients called blips: triangular gradients that are switched on and off between gradient echoes in a multi-echo acquisition.

To illustrate this method can be useful to give a look at figure \ref{fig:epi_gradients} where all the gradients in the game are illustrated, and to discuss line by line what they represent:
\begin{enumerate}
\item A single RF pulse with an angle of 90 degrees is applied to the sample;
\item Together with the RF pulse, a slice of the body is selected, corresponding to a position along the z axis, by applying the slice selection gradient;
\item Now we focus on the single 2D slice to begin the k-space data acquisition starting from the bottom line of the diagram in figure \ref{fig:epi_kspace}: both gradient of phase encoding and frequency encoding are switched on;
\item The frequency encoding (readout) gradient is reversed to create the first echo;
\item When the first echo occurs, the first line of k-space (along the $k_x$ or $k_{readout}$ direction) is acquired. In figure \ref{fig:epi_kspace} this lines corresponds to the horizontal bottom line;
\item Now we have to move to higher values of $k_y$ or $k_{phase}$: to accomplish this, the phase encoding gradient performs a small blip: it is switched on for a brief time and then switched off. This dephase spins a little further so that we move up along the $k_y$ direction in figure \ref{fig:epi_kspace} and the system is ready to acquire the other line of k-space;
\item the frequency encoding gradient is reversed again and the second horizontal line along $k_x$ is acquired.
\item This process is iterated: these last two steps are repeated until all the k-space is covered and the slice data is fully acquired.
\end{enumerate}

This acquisition technique allows the acquisition of a single slice in a short time (around 50-100 ms). The whole k-space for a single slice is acquired sequentially, following a snake-like pattern and the number of k-space line acquired after each single RF pulse is named differently according to the scan brand factory: examples are \emph{EPI factor} or \emph{Echo train length}.

Since slice data are collected after GE inversions, this imaging technique is sensitive to the $T_2^*$ time constant, which makes it suitable for functional imaging. 
For this reason this kind of imaging is often called $T_2^*$-weighted imaging.
In fact, the period of the readout gradient is the echo time TE, and is modulated such that is sensible to $T_2^{\ast}$ setting $TE \approx T_2^{\ast}$.



\begin{figure}
\begin{subfigure}{0.7\textwidth}
\includegraphics[width=\textwidth]{mri/epi_gradients_}
\caption{\textcolor{ForestGreen}{Gradients activation sequences for a EPI acquisition where a complete image is acquired after a single RF impulse. $G_{SS}$ is the slide selecting gradient, $G_{PE}$ is the phase encoding gradient, with tiny blips to dephase spins along the y direction, $G_{R}$ the readout gradient, or frequency encoding gradient, reversed each time for the acquisition of a line along the x axis. Finally, the ADC is switched on to record signals during the echo peack.}
}
\label{fig:epi_gradients}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{mri/epi-kspace}
\caption{\textcolor{ForestGreen}{Acquisition trajectory on the k-space for a single slice. Acquisition starts to negative values of $k_x$ ($k_{readout}$) and $k_y$ ($k_{phase}$), then, changing the direction of the readout gradient, the acquisition proceeds back and forth, gradually moving towards higher value of $k_y$ at each blip of the phase encoding gradient.}}
\label{fig:epi_kspace}
\end{subfigure}
\caption{Gradients of EPI acquisition sequence}
\label{fig:epi}
\end{figure}
%Negative spatial frequencies are associated to a decreasing net phase (positive are increasing net phase). From a Fourier decomposition arise both positive and negative frequencies.



\section{Functional MRI}\label{sec:fmri}

Functional MRI (fMRI) is a non-invasive diagnostic tool which aims to measure the neural activity of different parts of the brain.
The vast majority of scan acquisitions is carried out using EPI sequences (section \ref{sec:epi}) and consists of several 3D volumes acquired every 1.5-2 seconds \textcolor{ForestGreen}{(corresponding to the repetition time TR)} for the entire duration of a scan which lasts from 5 to more than 8 minutes, depending on the scanner model.

The detected signal comes from the blood oxygenation level fluctuations following a neuronal activation, and for this reason this kind of analysis is referred as blood oxygenation level-dependent (BOLD) imaging.

Blood can be portrayed as a colloidal mixture where blood cells constitute around 40-45~\% of volume. \cite{brown-2014}

From a physical point of view, the variation in oxygen content in blood affects the local magnetic susceptibility of the blood.
Signal coming from blood cells is mainly due to the presence (in each blood cell) of several molecules of hemoglobin: a protein containing 4 heme groups each one including an iron atom which binds an oxygen molecule and carries it throughout veins and capillaries.
Hemoglobin can be in two states: oxy-hemoglobin and deoxy-hemoglobin.
These two molecules differ in the presence of the bounded oxygen molecule, which reflects in differences in their magnetic susceptibility as oxy-hemoglobin has diamagnetic properties while deoxy-hemoglobin is paramagnetic.
This difference is due to unpaired iron electrons in the deoxy-hemoglobin which lead to an unshielded molecule against the external magnetic field.
The presence of deoxy-hemoglobin causes local field inhomogeneities, leading to a reduction of the main signal coming from water molecules.

This signal weakening is due to spins going out of phase between each other more quickly, because of these additional field inhomogeneities, which result in reducing the total magnetization.
This means that nuclei would lose their magnetization faster than the typical $T_2^*$ decay constant.
For this reason if the scanner is tuned to the $T_2^*$ relaxation time, it would be possible to appreciate this chemical shift between oxygenated and deoxygenated areas. This gives the alternative name of BOLD-images as $T_2^{\ast}$-weighted images \cite{huettel-2009}.

Since the presence of deoxy-hemoglobin causes a weakening of the signal, when a brain area is activated, it demands a greater amount of oxygen carried by oxy-hemoglobin molecules which results in a signal increase.

The process that, from a neuronal stimulus leads to a measurable blood signal is governed by the hemodynamic response function (HRF) shown in figure \ref{fig:hrf}. This figure represent the blood response immediately after a neuronal stimulus (which occurs on a timescale of $\approx$ 10-100 $\mu$s). The BOLD signal takes $\approx$ 4-6s to reach a peak and a total of 20-30 s to return to its zero baseline.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{mri/hrf}
\caption{Evolution of BOLD signal of an adult brain during time. At the instant t=0 the neuronal stimulus occurs; after that, it takes around six seconds to reach a maximum of signal, and a total of more than twenty-five seconds to return to its baseline value}
\label{fig:hrf}
\end{figure}


Since we are interested in these slow signal trends, the TR of 1.5-2 seconds is the minimum necessary to have an analysis sensitive to these signals.
According to the Shannon-Nyquist sampling theorem, the maximum detectable frequency in a signal is equal to 1/2 of the sampling frequency.
For this reason with a TR of 1.5 seconds for example, we are able to detect signals with a period $\geq 3$ seconds.

\textcolor{ForestGreen}{During an MRI and fMRI scan session, data are usually acquired slice by slice, and the thinner is the slice, the more spatial resolution it is possible to accomplish.
But there is a trade-off between spatial and temporal resolution: decreasing the slice thickness leads to a better space resolution but at the cost of increasing repetition time to maintain the same Signal to Noise Ratio (SNR).
Thie main reason is that signal is proportional to the number of hydrogen nuclei, which is proportional to the slice volume.
In order to obtain a strong BOLD signal, echo time plays a significant role as well: to obtain the maximum strenght signal echo time has to be set $TE \approx T_2^{\ast}$ that means TE around 30 ms.
Images acquired using a shorter echo time have a weaker BOLD signal because of the weaker signal to detect. \cite{jenkinson2018}
}


\chapter{Machine learning}\label{chap:machine_learning}

%Machine learning is a branch of Artificial Intelligence (AI) that aims to learn from data and gradually improve its accuracy.
Machine learning is a branch of Artificial Intelligence (AI) that aims to learn from data and apply this knowledge on new, unseen data.
It is possible to make predictions or perform classification of data and, through the training process, gradually improve accuracy on the task a model performs.
Regarding the training process of an algorithm, as a rule of thumb, the bigger and homogeneous is the dataset to train it, the better accuracy and generalization can be achieved. % (namely performing well on different datasets)

According to the analysis we aim to perform, and the type of dataset we are working on, machine learning algorithms can be divided into two macro areas: supervised and unsupervised learning.
\begin{itemize}
\item From a dataset, consisting on a collection of data, each one containing a certain number of features, an \textbf{unsupervised} algorithm goal is to learn the principal properties from the dataset structure and to extract information from data without human labour to annotate and label each data.
Analysis with unsupervised algorithms include clustering or dimensionality reduction.

Clustering is maybe the most simple and intuitive example of an unsupervised learning: it is basically a classification process through which unlabeled data are reorganized and classified into subgroups according to some common properties or distance measures that arise from the analysis.
After this process, data of each group, called cluster, share a certain degree of similarity in feature probability distribution.


Dimensionality reduction is another important example of unsupervised learning process: it is performed with the task of finding a different representation of the data, with a lower dimensionality, and preserve as much information as possible from them.
This can be accomplished either by compressing data into a lower-dimensional space, or by searching the main source of variance across the data and create a representation in such a way that the dimensions of the new representation are statistically independent.



%They usually aim to learn the probability distribution of the features in our data, and to do so, they search for a new representation of the data, often simpler than the initial one.
%Simpler can be associated with
%\begin{itemize}
%\item lower dimensionality of data namely the reduction of the space where data lie of informally a reduction of the number of features in our data
%\item sparse representation of data transform the data in a representation containing zeros in most entries, to try to aligh input data along this new  representation's axis. Usually resulting in an increasing of data dimensionality to avoid information lost deriving from setting most entries to zero
%\item Independent representaiton focus on the main sources of variance across data, trying to create statistically independent representation ofa initial data.
%\end{itemize}

\item On the other hand, when we are dealing with \textbf{supervised learning}, we work with a dataset where each data is associated with a label, specifying the class data belong to.
Thus, the algorithm is trained to learn the common features for each class and to predict the correct label associated to each input data.

Generally speaking, given an input data x and an associated label y, a model tries to estimate the probability of obtaining y given x: $p(y|x)$.
These algorithms are referred as supervised because of the human labour necessary to label each input data.
\end{itemize}

A subset of Machine Learning is Deep Learning, the main difference lies on the structure of its algorithms and on the learning techniques.
Traditional machine learning methods consist on algorithms like Random Forest, Support Vector Machines (SVM), K-nearest neighbor (KNN) and several more models, while deep learning includes models as Artificial neural networks (ANN), Convolutional Neural Networks (CNN) and more.

%As we will see in furter paragraphs, they have a common structure similar to a neural structrure, made by different neurons connected to each other.

\section{Random forest}
Random Forest classifier is a common machine learning algorithm that belongs to the category of \emph{ensemble} classifiers.
It combines multiple decision tree models to reduce overfitting of data (see section \ref{sec:deep_learning_theory}) and to create a more powerful model that achieve good generalizability.
Therefore, to properly understand a random forest algorithm, we need to start understanding how a Decision Tree classifier works.

\paragraph{Tree classifiers} \hfill

\noindent A decision tree aims to learn distictive traits from input data by asking yes/no questions.
Focusing on one single input feature, the classifier splits the dataset based on the value of that feature according to an if/else statement like \textquotedblleft if feature\_i $>$ a \textquotedblright.
Thus, each branch of a decision tree consists of a question that causes the split of the dataset into two smaller sub datasets.
In a dataset containing multiple features, the process is recursively repeated until it reaches an end point called \emph{leave}, corresponding to the ultimate partition, containing data points belonging to a single class.

The goal of a tree is to construct branches so that the partition are informative about the class labels.
In a practical way, given a dataset X made of different data samples, each one containing n features, the algorithm construct a tree following these simple steps:
starting from the top node called \emph{root}, it searches among the n features the one which allows the best split between classes and split the dataset into two subsets, each one constituting a node. Then, for each node, the process is repeated until a single leaf is left.

The best split is performed according to a pre-defined objective function we want to maximize throughout the construction of a tree.
Usually these functions regard measures of the homogeneity of class labels in a node, usually referred as impurity measures.
%such as the difference between the impurity of a parent node and the sum of the impurity of the child.
By lowering the impurity of a node, it seeks for the maximization of the information gain, defined as the difference between the parent node impurity and the weighted sum of the child nodes impurities.
%\footnote{https://spark.apache.org/docs/1.3.0/mllib-decision-tree.html}

%Three common impurity measures or splitting criteria are commonly used in decision trees: Gini impurity, entropy and classification error.
A commonly used impurity measure is Giny Impurity. \cite{raschka-2019}.
It can be interpreted as the probebility to misclassify an observation.
If a node contains a sub-dataset Q with a total number of data n, belonging to k different classes, the Gini Impurity measure H(Q) is obtained by the simple relation
\begin{equation}
H(Q)=\sum_k p_{k}(1-p_{k}) = 1- \sum_k p_k^2
\end{equation}
where $p_k$ is the fraction of data in Q belonging to class k.


Following this principle, the splitting process is iterated and the tree is grown.

The structure of a tree can be visualized in a plot that contains all the information necessaries to understand the tree, such as the feature according to which each split is made, and the impurity measure of each node.
This property is one of those that make decision trees so popular: they are easy to interpret since their structure and the information of each node can be visualized in a clear plot.
Other positive sides of decision trees are, their easy to use implementation which requires little data preparation and their rapidity since their complexity goes like $O(n_{features}n_{samples}^2log(n_{samples}))$ \cite{scikit-2011}

An example of a decision tree dealing with flower classification is shown in figure \ref{fig:decisiontree}.
We choose this example because is easier to understand how a decision trees works with this data where each feature is labeled with an intuitive name and contains a physical quantity easily comparable with everyday life such as petal lenght expressed in centimeters.
This example is taken from the scikit-learn website \footnote{https://scikit-learn.org/stable/modules/tree.html\#}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{ml/decisiontree_example.png}
\caption{A graphical representation of a decision tree for classification of the Iris dataset consisting on data belonging to three different classes of iris: Setosa, Versicolour, and Virginica. Each node of the tree shows the criterion used for its split, the Gini values, the total number of samples in that node and the main class that data belong to}
\label{fig:decisiontree}
\end{figure}

Instead of looking at the whole tree, we may be interested in what feature contributed the most to the final output.
To this end, a decision tree usually weights feature with a number from 0 to 1 according to how much a feature contributed to the impurity decrease along the tree.
The importance of a feature is calculated as the decrease in node impurity weighted by the probability of reaching that node, where this probability is just the number of samples that reach that node divided by the total number of samples.
%\footnote{https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3s}
\cite{raschka-2019} \cite{muller-2017}

Unfortunately though, one of the main drowbacks of decision trees is that the iterating process towards the reaching of a leaf, can bring to the creation of a over-complex model that overfits the training data.
That is, the model accurately learns the training dataset but is not able to generalize well to a test dataset.
To prevent overfitting, one possible strategy is to early stop the iteration towards the leaf, and leave a node with more than a single sample left.
An other strategy, which leads to the construction of a Random Forest, is the creation of different trees and put information together, in order to obtain stronger model, more prone to generalize information and reducing overfitting.

\paragraph{Random Forest} \hfill

A Random Forest is a collection of decision trees where each tree is different from the others, each tree tends to overfit, but following different patterns.
Taking advantage of this process, we could reduce overfitting by averaging different results.
Random forest gets its name because of the insertion of randomness during the construction of different trees.
Given an input dataset X with N different samples, there are two main steps where randomicity plays an important role: when selecting the number of samples to grow each tree on, and when selecting the number of features to consider when looking for the best split of a node.

In a random forest, one of the main parameters is the number of tree to grow.
Once selected this number, the process can start where each tree is constructed according to the following steps:
\begin{enumerate}
\item Randomly select n$\leq$N samples from the input dataset X, this operation is called bootstrap.
\item Grow a decision tree from this bootstrap sample. For each node, the algorithm randomly limit the number of feature available and compute the best split on this remaining subset of features. This avoids correlations between trees and results in better performances.
\item Repeat steps 1) and 2) as many times as the number of trees to build.
\item Once all the trees are created, the forest is ready to make predictions. Each tree is used to make a prediction and all the results are collected.
The final prediction of the forest is assessed by majoring vote: the predicted class will then be the one predicted by the majority of classifiers.
\end{enumerate}

%This steps are implemented in the Python's package Scikit-Learn

%Randomness within trees of a single model is introduced with three strategies:
%\begin{enumerate*}
%\item Randomly select some input data and buiding a tree with this subset of data
%\item Randomly select a different subset of features at each split
%\item A combination and implementation of both the previous strategies results in a model in which each tree has reduced predictive power, but when put %all together, it results in an improved predictive power with a drastical reduction of overfitting.
%\end{enumerate*}

%Introducing randomness in a model results in a lower correlation between models and this brings to a reduced variance between outputs.

Just as we discussed for tree classifiers, it is possible to extract information about features for a random forest as well.
Important features for a random forest are assessed by extracting important features from each tree and by averaging these results.

%information regarding features importance can be extracted from a random forest classifier from the ones of each tree, and by averaging the results of feature importance extracted from each tree.


\section{Deep Learning: ANN}\label{sec:deep_learning_theory}

In the previous paragraph we discussed one of the most important machine learning algorithm, but, as mentioned before, there is a subset of machine learning algorithm with a common typical structure, thanks to which they get their name of deep neural networks.
The structure of algorithms belonging to this family are inspired by a brain neuronal structure, and even if in a simplified way, they try to emulate the learning process of a brain.
These algorithms own the adjective \textquotedblleft deep \textquotedblright to their structure: they are organised in layer, each one containing several fundamental unit called neuron.
Neurons between layers are connected to each other like synapsis transmit a signal between neurons in a biological brain.
Thanks to this structure which reminds a neuronal brain structure,they are called artificial neuronal networks
%Some kind of algorithms are called Artificial Neural Networks (ANN) and, such as a neuronal structure, they are made of several neurons organized into layers; each layer contains certain amount of neurons each one linked to each others.

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{ml/perceptron_example}
\caption{A schematic representation of a perceptron: this perceptron receives an input vector t, with n features $x_1, ... x_n$, each one weighted with a different weight $w_1, ... w_n$ and a offset $w_0$, and compute a linear combination of them and returns an output, activated by a step function: it returns 0 if the linear combination's value doesn't reach a certain threshold, or returns the value itself if it does.}
\label{fig:perceptron}
\end{figure}

The foundamental unit which constiture a neuronal network is an artificial neuron, one of the most relevant example of artificial neurons is the \emph{perceptron} shown in figure \ref{fig:perceptron}.

A perceptron is the foundamental unit of a supervised deep learning algorithm: it takes as input a data, for example an n-dimensional array $\textbf{x} = \left\{ x_1, x_2, ..., x_n\right\}$ and returns an output: a real number computed by applying a function to a linear combination of all the inputs $ y = f(z) = f \left ( \sum_i w_{i} x_i + b \right )$.
The function that determines the output is called \emph{activation function}, and can be either a simple step function, or a more complex function.
We will briefly discuss some of the most important activation functions in section \ref{sec:activation_functions}.


\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{ml/ann_example}
\caption{Schematic representation of an artificial neural network containing an input layer, three hidden layers and an output layer: a single vector contained n features (input 1, ... input n) is given as input to the first Input layer i. All the inputs are linked to every neurons of the first hidden layer, which would compute a linear combination of its inputs. All the hidden layers are linked to the other hidden layers, and at the end, n different outputs are returned.
}
\label{fig:ann}
\end{figure}


A deep neural network is a hierarchical organization of neurons into layers, connected to each other.
Input data are passed to the first input layer where each neuron acting like a perceptron, produces an output using the activation function.
All the neurons in a layer have the same activation function, but it can differ from the activation function of other layers.
Once collected every output from the first layer neurons, they are passed to the second layer becoming the input to each neuron belonging to this layer.
This is reiterated through all the layers up to the final output layer.
As schematized in figure \ref{fig:ann} an artificial neural network is mainly composed of three parts: an input layer, some middle layers also called \emph{hidden layers} and a final output layer.
Each neuron of a layer is linked to all the neurons of the previous and next layer and each connection is weighted.
At the beginning, weights are randomly set, but during training they are updated in order to improve network performances.

%When reach the end of this process, we obtain the output: a numerical value related to every single input data $\textbf{x}$.

Using matrix formalism the entire input-output process can be written as
\begin{equation}
\mathbf{y} = \sum_j^n w_{ij} x_j + b_i
\end{equation}
 where $\textbf{x} \in \Re^n \ \textbf{y} \in \Re^m $ being m the output dimension determined by the number of neurons in the output layer. \textbf{y} represents the output, or prediction of the model.
The process through which given an input data \textbf{x} we obtain an output \textbf{y} is called \emph{forward propagation}.

In order to train a model, the prediction is compared with the actual value of the label associated to the input data point, and during a process called \emph{backpropagation} the algorithm modifies its weights in order to minimize the difference between the actual and the predicted value.

The goal of a machine/deep learning algorithm is to learn from data using a train dataset, and generalize information in order to perform well on an unseen dataset called test dataset.
Performing well means to produce a low error on the test dataset after minimizing the error on train dataset.
\hfill

\noindent To fully define a neural network we just need some parameters called hyperparameters, the principals of which are:

\begin{itemize}
\item Number of layers
\item Number of neuron for each layer
\item Activation function for each layer
\item Number of epochs (or iterations)
\item Learning rate
\end{itemize}


Number of layers and numbers of neurons are connected to one of the most important concept in machine learning that characterize a network: the concept of \emph{capacity}.
Capacity qualitatively refers to the level of complexity that a model is able to learn.
This is strictly linked to two critical issues in machine learning: the concept of underfitting and overfitting.
\begin{itemize}
\item Underfitting occurs when the model is not able to learn the required amount of information during training.
This usually happens for shallow network, when the model has a small number of parameters in regard to the number needed to explain the input features.
To have low parameters results in poor performances because the model is not able to learn the underlying structure of a complex data set.

\item On the contrary, overfitting occurs when the model has too many parameters compared to those required to learn the input features.
What happens then is that the model memorizes all the data it sees during train but it is not able to generalize information.
As a result, the model performs well on the train dataset and has low performances in the unseen test dataset.
\end{itemize}

To understand this concept is can be helpful to provide an example in a two-dimensional space.
If we have to fit a dataset consisting of some points sampled from a quadratic curve, we can choose different functions, for example a linear function, a quadratic function and a polynomial function with grade equal to the number of data points.
As shown in figure \ref{fig:underoverfitting} the linear model is not able to describe the data distribution while the polynomial function has too many parameters, and it is able to fit the train distribution, but it does not suit to describe data points belonging to the test, sampled from the same quadratic distribution.
Furthermore, if the number of parameters (the grade of the polynomial in this case) is greater than the number of data points, we obtain that an infinite number of different curves are suitable to fit our data, and finding the one which performs well even on the test dataset is an hard task.
We should therefore pay attention when choosing the number of parameters on a model to avoid under- or more likely over- fitting.


To monitor the evolution of the training process of a model it is a common practice to split the train dataset into two subsets: one that will actual constitute the train dataset, and a smaller one, called validation dataset used to make constant checkups on how the model is learning with training data.
The validation dataset is used for the fine tuning of hyperparameters and it typically consists of 10-30\% of the train dataset.
Just in this paragraph to distinguish between the two train dataset we will denote as \textquotedblleft Train \textquotedblright, the whole train dataset and as \textquotedblleft train \textquotedblright the subset of the Train dataset left after its split into a train and a validation set.
The whole dataset will then be divided into approximately 70\% Train and 30\% test, and the Train dataset itself will be divided into 70\% actual train and 30\% validation.
When the best combination of hyperparameters is found, it is possible to actually train the model using the entire Train dataset.


\begin{figure}
\centering
%\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.6\textwidth]{ml/underoverfitting_mod}
\caption{Underfitting, overfitting and appropriate fit in a 2D dataset: three models with different capacities are trained on blue data, if the model is too simple, in this case if the fit function has a few free parameters, it is not able to properly fit data, on the other hand, if it has too much parameters in respect to how much we would need to fit our data, it perfectly match train data distribution, passing through each data point, but performs very poorly on the test dataset (orange points). With an appropriate number of parameters, (middle figure) the model is able to fit train data, and generalizes well to test data.
}
\label{fig:underoverfitting}
%\end{subfigure}
%\begin{subfigure}{0.4\textwidth}
%\includegraphics[width=\textwidth]{ml/underoverfitting_grafico}
%\caption{}
% 
%\end{subfigure}
%\caption{General, qualitative trend of a loss function vs. capacity of model: the more capacity it has, the more is able to reduce the error on train set (blue dotted line) with a tendence to overfit train data. Beyond a certain capacity the error on the test dataset (red line) increases because overfitting of training data results in a lower ability to generalize to new data.
%}
%\label{fig:underoverfitting}
\end{figure}

\paragraph{Regularization strategies} \hfill

\noindent Some regularization procedures are often implemented to avoid overfitting. The strategy is to build a model with a capacity slightly higher than the necessary in order to perform well on the training dataset, and then, to avoid overfitting, implement some regularization techniques to achieve good generalization performances.

Some of the most popular are Dropout and Batch-Normalization

\begin{itemize}
\item Dropout is a regularization strategy that inserts randomness during the training of a model.
It is usually applied to the neurons of hidden layers and it randomly drop a certain fraction of hidden neurons and their connections, during each training cycle.
The percentage of neurons to drop, corresponding to the probability for a single neuron to be dropped, is set by the user.
A visual representation of what occurs is illustrated in figure \ref{fig:dropout}.
When discarding some neurons in a layer, the remaining neurons needs to rescale their weights to account the missing connection; doing so, every neuron cannot rely on the the input of all the preceding neurons and the network is forced to learn more robust patterns from the data.
With this strategy we achieve the same results as we would obtain by training different models with different structures and average all the outputs.

\item BatchNormalizzation is a regularization scheme that has been commonly adopted since its introduction in 2015 \cite{ioffe2015}.
It is based on the observation that a neural network works and performs better when its input are normalized because this prevents the saturation of its neurons.
A neuron can in fact saturate and settle to a certain value because of an high input, this causes the neuron outputs value to be always close to the asymptotic end of its activation function (see section \ref{sec:activation_functions}), resulting in a biased and less accurate prediction.
What is essentially done is then a simple scaling of each neuron input: for a layer $l$ with d neurons its input $\mathbf{x} = \{x_1^l, x_2^l, ..., x_d^l\}$ is normalized by removing the mean value across all the input data, and by dividing for their variance $x_i^l \rightarrow \tilde x_i^l = \frac{x_i^{l} - \mathbb{E}[x_i^l]}{\sqrt{Var(x_i^l)}} $

\end{itemize}

\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{ml/dropout.png}
\caption{Schematic representation of a 0.5 dropout procedure: in a standard neural network, each neuron is linked to all the others, but if we implement a 0.5 dropout, for each iteration, neurons on hidden layers have a 50\% probability to be dropped and excluded from the computation of outputs}
\label{fig:dropout}
\end{figure}

\paragraph{Cross validation procedures} \hfill

\noindent When dealing with small datasets, dividing them into train and test can be a non trivial issue because of the shortage of data for a proper train procedure.

To work this out, a procedure called k-fold cross validation can be implemented, at the cost of increasing computational costs.
It consists on the creation of k different partitions of the main whole dataset (before splitting it into train + test).
From these partitions, k-1 are used as train, and the last is used as test.
Every subset of partitions is used so that each different partition is used as test, and the others as train.
Thus, for each k-th run there are two subsets (a train and a test) of the original dataset.
Following this procedure, at each iteration, the train dataset is different, and the model is tested on a different dataset each time.
An image showing this procedure is \ref{fig:kfold}.
In practice what is usually done, is not a sequential partitioning the dataset as shown in figure \ref{fig:kfold} but a shuffled partition of data.
This avoids creating folder containing all the same label in case the dataset was ordered, but randomly picking data in order to create subsets containing the same proportion between classes as in the main dataset.

Overall, we can imagine this process like the creation of k different models, each one trained on a different partition (k-1 folds) and tested on the remaining k-th fold.
We evaluate each of these models, and we take as a result the average score across all the outputs.


\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{ml/kfold_example}
\caption{Schematic representation of a k-fold cross validation procedure with k = 5: Dataset is partitioned into 5 subsets, four of which are used for training and the remaining one for the testing dataset. during each iteration a model is trained over 4 different folds and tested on the remaining one, until each fold has been used at least once as part of training and once as test.
}
\label{fig:kfold}
\end{figure}


\section{Activation functions}\label{sec:activation_functions}
The activation function of a neuron, and consequently of a layer defines the output of each neuron belonging to that layer.
There are different activation functions, linear or non-linear.
A linear activation function outputs a value $f(z) = a \cdot z + b$ where we denote as z the scalar product between an input vector \textbf{x} and the weights vector \textbf{w} plus an eventual offset $w_0$, and as \textbf{z} the matrix-vector product between the matrix of weights \textbf{w} and the input vector \textbf{x} and an offset vector $\mathbf{w_0}$.

In practice, however, it would not be very useful to introduce a linear activation function since the combination of linear functions is a linear function itself.
In fact, in a neural model, we seek to introduce non-linearity in order to deal with more complex tasks.
There are several non-linear functions that can be employed depending on what data we are working on, or on what kind of classification task we are performing.
Some of the most popular are:


\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{ml/relu}
\caption{Rectified linear unit (ReLU) function: it returns zero if its argument is negative and returns the argument itself if it is positive, according to equation \ref{eq:relu}.}
\label{fig:relu_example}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{ml/sigmoid}
\caption{Sigmoid function: returns a value between 0 and 1, according to equation \ref{eq:sigmoid}}
\label{fig:sigmoid_example}
\end{subfigure}
\caption{}
\end{figure}




\begin{itemize}

\item The ReLU function shown in figure \ref{fig:relu_example} is defined as
\begin{equation}\label{eq:relu}
\phi(z) = max(0, z) = \begin{cases} 0 \ for \ z<0 \\ z \ for \ z>0 \end{cases}
\end{equation}
and returns the maximum value between the input and zero, it essentially puts to zero all negative inputs and leaves the positives untouched.
\begin{itemize}
\item A modified version of the ReLU function called Leaky ReLU was introduced defined as
$\phi_{leaky}(z) = \begin{cases} \alpha z \ for \ z<0 \\ z \ for \ z>0 \end{cases}$ where $\alpha$ is a coefficient usually $< 1$, typically of the order of $10^{-2}$
\end{itemize}

\item The sigmoid function, or logistic function show in figure \ref{fig:sigmoid_example} is defined as
\begin{equation}\label{eq:sigmoid}
\sigma (z) = \frac{1}{1+e^{-z}}
\end{equation}
and outputs a real number between 0 and 1.
For this reason is a common choice when we have to model a probability.
For example in a binary classification task, when it is employed as activation function of the last layer, it can be interpreted as the probability of the input data to belong to the class 0 or 1.


\item The softmax function, is a generalization of the logistic function, and is commonly used for multiclass classification. It is defined as
\begin{equation}
\phi(z) = p(y = i|z) = \frac{e^z_i}{\sum_{j = 1}^M e^{z_j}}
\end{equation}
It is applied to the vector \textbf{z} and describes the probability of \textbf{x} to belong to the class $i$ over a total of M classes.
With this function, classes are regarded as mutually exclusives, so if the probability to belong to class $i$ is $p$, the probability to belong to one of the other classes is $1-p$.
To hold this property, softmax can't be applied independently to each input $z_i$, since it depends on all elements of $\textbf{z} = \{z_1, ..., z_M\}$
\end{itemize}
%ricorda: x softmax, ogni connessione è pesata! così se ho n nodi in input e m nodi in output ho una matrice n x m. Per sigmoid ho n nodi in input e 1 in uscita, quindi la matrice è nx1 è un vettore.



\section{Loss functions}\label{sec:loss_functions}
%Ricorda: \hat y_i = sigma(x_i) che è il mio output del modello
%Articoli da cui ho preso info: ml for physicists e dispende di baldini (x la likelihood)

In order to train a network and improve its performances we compare the predicted output value with the value of the label associated to a data, and compute an error, or distance between these two values.

This error is computed by using a \emph{loss function}.
In common classification tasks, the goal of a network is to gradually reduce the error, looking for a minimum of these functions,
%The function to minimize is denoted as the \emph{loss function}
%Computing the loss, returns a value that represents the distance between the prediction and the actual label.
%According to this value, the weights of the network are gradually modified in order to reduce it and searching for a minimum of the loss function,
according to a process called backpropagation, that we discuss in section \ref{sec:backpropagation_theory}


The most common loss function for classification problems is the cross-entropy loss.
It relies on the concept of cross-entropy between two distributions $\hat y \ and \ y$ defined as
\begin{equation}\label{eq:crossentropy}
H(y, \hat y) = -\sum_{m= 0}^{M-1} y_i \cdot log(\hat y_i)
\end{equation}
Where the sum is intended over all the M possible values a variable y can assume (all the M possible classes in a classification problem).

If classification only concerns two classes is called a binary classification, and its associated label usually have values $y = \{0, 1\}$.
If we explicit the sum of equation \ref{eq:crossentropy} for M = 2, cross entropy for a binary classification, for one observation can be calculated as
\begin{equation}
\ell_i = -(y_i log(\hat y_i) + (1-y_i)log(1-\hat y_i))
\end{equation}

If we have N data belonging to two classes, we can write the \emph{binary cross-entropy} loss as
\begin{equation}\label{eq:binary_crossentropy}
L = \frac{1}{N} \sum_i ^N \ell_i = -\frac{1}{N} \sum_{i= 1}^N y_i log(\hat y_i) + (1-y_i)log(1-\hat y_i)
\end{equation}

Binary crossentropy loss can be generalized to the case of multi-class classification.
In this case, if data belong to M classes, labels $y$ can assume values $y \in \{0, 1, ..., M-1\}$.
To define a loss, each label $y_i$ must be one-hot encoded (see chapter \ref{chap:deep_models}) in order to create a binary vector of dimension M, with all but one entry equal to zero, and the position of the only entry equals to 1 specifies the class.

In this case the loss function is called \emph{categorical cross-entropy}
\begin{equation}\label{eq:categorical_crossentropy}
L = -\frac{1}{N} \sum_{i=1}^N \sum_{m=0}^{M-1}y_{im} log(\hat y_{im}) + (1-y_{im})log(1- \hat y_{im}))
\end{equation}

In equations \ref{eq:binary_crossentropy} and \ref{eq:categorical_crossentropy} $\hat y$ are probability of the input data $i-th$ to belong to a class.
For binary classification, this probability is usually modeled with a sigmoid function, while for multi-class classification, a softmax function is usually employed.

\paragraph{Relation of the loss function with a Maximum Likelihood Extimation}
\hfill

\noindent The estimation of the minimum of the loss function can be seen as a process of Maximum Likelihood Extimation.
Given a probabilistic model depending on m different parameters $\theta_1 ... \theta_m$, the likelihood is a function that describes the probability of observing a value $\hat y_i$ as a function of set of parameters $\{\theta_i \}_1 ^m$.
If we set values of these parameters we obtain the probability of observing the value $\hat y_i$ under the given model \cite{baldini2021}.

\begin{equation}
 \mathcal{L} = (\theta_1, ..., \theta_m | \hat y_i) = P(\hat y_i|\theta_1, ...\theta_m).
\end{equation}
For a subset of observed value we can write the likelihood as $\mathcal{L} = P(\hat y_1, ..., \hat y_N | \theta_1, ...\theta_m)$ .
If all the observed values are independent, we can write the total probability as the product of single probabilities of observing each sample $\hat y_i$ given a model with parameters $\{\theta_i \}_1 ^m$:

\begin{equation}\label{eq:likelihood_product}
\mathcal{L} =\prod_{i=1}^n p(\hat y_i|\theta_1, ...\theta_m).
\end{equation}
%Our goal is to perform the maximization of this function during a logistic regression:
%A logistic regression is the estimate of the best model's parameters that models the discrete probability of two events:
We consider the problem of a binary classification using a deep neural network.
With this model, parameters $\{\theta_i \}_1 ^m$ correspond to the inner weights of the networks $\mathbf{w} = \left\{w_i \right\}_1^m$.
Input data are a set of N datapoints $\left\{ x_1 ... x_N\right\}$ each one associate with a label  $\left\{ y_1 ... y_N\right\}$ which can be either 0 or 1.
Overall, the whole dataset can be denoted as D = $\left\{ (\mathbf{x_i}, y_i)\right\}$.

Given an input data $\mathbf{x_i}$  and an activation function (sigmoid, for example, since we are performing binary classification)
%(see section \ref{sec:activation_functions}, since we use the same notation of $\mathbf{z_i} = \mathbf{x_i}\mathbf{w}$)
we model the probability of $\mathbf{x_i}$ to belong to the class $y_i = 1$ as:
%or in other words, the probability to observ $y_i$ given an input $x_i$ is given by

\begin{equation}
 P(y_i = 1 | x_i, \mathbf{w}) = \sigma(z_i) = \frac{1}{1+e^{-z_i}}
\end{equation}
where z is defined as in section \ref{sec:activation_functions} $\mathbf{z_i} = \mathbf{x_i}\mathbf{w}$
%which in terms of a regression can be wrote as 1 - the probability to belong to the negative class namely
And since we are dealing with a binary classification, where $y_i$ can be either 0 or 1, the probability to belong to one class is 1 minus the probability to belong to the other:
\begin{equation}
P(y_i = 0) = 1 - P(y_i = 1)
\end{equation}

Using this result for a set of data $D = \{ (\mathbf{x_i}, y_i)\}_1 ^N$ and substituting the formula for a single probability in the likelihood function \ref{eq:likelihood_product} we obtain the likelihood of observing the dataset D under a model with parameters $\mathbf{w}$
%of observing our data under our model
\begin{equation}\label{eq:like_no_log}
\mathcal{L} = P(D|\mathbf{w}) = \prod_{i=1}^N \sigma(\mathbf{x_i w})^{y_i} (1-\sigma(\mathbf{x_i w}))^{1-y_i}
\end{equation}

To perform a Maximum Likelihood Estimation we look for a maximum in this function by computing partial derivatives.
However, since computing the derivative of a product is a nontrivial task, we can consider the logarithm of $\mathcal{L}$ and compute its maximum  since the logarithm is a monotonic function and compute its maximum corresponds to computing the maximum of the function itself.
The logarithm of equation \ref{eq:like_no_log} is

\begin{equation}\label{eq:like_si_log}
log(\mathcal{L}) = \sum_{i=1}^N y_i log(\sigma(x_i \mathbf{w})) + (1-y_i)log(1-\sigma(x_i \mathbf{w}))
\end{equation}

which taken with a negative sign, and averaged over all the N data samples corresponds to the cross-entropy function in equation \ref{eq:binary_crossentropy}.

Thus, minimizing the binary-crossentropy loss is the equivalent of a maximum likelihood estimation for the parameters of our model.

%The best parameters are those that maximize the likelihood:
%$\begin{cases} \frac{\partial L}{\partial w_1}(w_1, ..., w_m) = 0 \\ ... \\ \frac{\partial L}{\partial w_1}(w_1, ..., w_m) = 0 \end{cases}$
%To compute the derivative of a product is a nontrivial task, hence is much simpler to compute the logarithm (since the logarithm is a monotonic function, so compute the maximum of a function is the same as computing the maximum of the logarithm of that function).


%The algorithm employed by a feedforward network for weights' fine tuning is called \emph{backpropagation} (backward propagation of errors).

\section{Gradient descent and Backpropagation}\label{sec:backpropagation_theory}

During the training of a model, after the calculation of the loss function, the network moves to the estimation of the best parameters through an algorithm called \emph{backpropagation}.
To perform the minimization of a loss function, even if it would be theoretically possible to find a minimum by means of an analytical way, in practice, usually the number of weights is so huge that numerical methods must be employed.
The most popular method to compute gradients and optimize parameters is the \textbf{gradient descent}.

We denote a generic loss function as $L(\mathbf{w}) = \frac{1}{N}\sum_{i = 1}^N \ell(y_i, \hat y_i(\mathbf{w}))$ where $\mathbf{w}$ is a vector of weights, the minimum of this function corresponding to the vector $\mathbf{w}^0$ can be found by following the subsequent steps:

\begin{enumerate}
\item Choose a random initial guess for $\mathbf{w}^{0}$ and start iterating
\item At iteration $i+1$ we reach a weights vector $\mathbf{w}^{i+1}$ given by the formula
\begin{equation}\label{eq:weights_update}
\mathbf{w}^{i+1} = \mathbf{w}^{i} - \eta \nabla_\mathbf{w} L(\mathbf{w}^i)
\end{equation}
\end{enumerate}

 where the $\nabla_\mathbf{w}$ indicate the gradient of the cost function with respect to $\mathbf{w}$ components, and $\eta$ is a parameters called \emph{learning rate}.
 It specifies the dimension of each step during the descent toward the minimum of the cost function.

%To find the right variation to the weights we can use the gradient descent algorithm, so we compute the gradient of the cost function $\nabla J(w)$ so that we can compute the new weight $w = w + \Delta w$ where $\Delta w = - \eta \nabla J(w)$ which expressed in component becomes $\Delta w_j = -\eta \frac{\partial J(w)}{\partial w_j} = -\eta \cdot \sum (\phi(z^{(i)}) - y^{(i)} )x_j^{(i)}$.

A drawback of this gradient descent algorithm is that it works by computing all the gradients for all the data points before updating the weights.
So the weight is updated only after the whole dataset is been seen, resulting in a huge computational cost.

An optimized version of this algorithm is called \textbf{Stochastic Gradient Descent} (SDG).
It is often used because it brings some advantages such as decreasing computational cost and, introducing stochasticity.
The insertion of randomicity during the gradient computation results in a reduced chance for the algorithm to get stuck in local minima.

SGD works by approximating the gradient of the cost function calculated with all the input data, with a gradient computed using only a small subset of input data called \emph{mini-batch}.

With a dataset comprising N input data, we can create subsets containing m elements and obtain N/m minibatches.
The gradient is computed on a mini-batch and weights are updated.
This process is repeated and when the gradient is computed over all the mini-batches it is said that the training proces completed an \emph{epoch}.
%mettere equatione ml_physicists p.17  (19)???
The number of epochs is one of the hyperparameters to set when choosing a training strategy of a model.

%whose difference is that the upgrade of the weights is sequential, for each training sample, so that the summation is not computed.
Even though SGD performs quite well, it can be further improved by introducing the concept of momentum.
Momentum is represented by a parameter $0\le \gamma \le 1$ and it takes track of the descending direction by running an average over all the preceding encountered gradients.
This process helps the algorithm speeding up the descending process if in a certain direction the gradient is constant and it does not change slope.

The descending process can be further improved by taking into account even the steepness all over the dimensions.
To accomplish so the algorithm has to be improved by adding the calculation of second order momenta also called \emph{uncentered variance}.
This procedure, would ideally bring to the calculus of the hessian matrix but at the cost of increasing computational costs.
Recently introduced algorithm can accomplish this task by approximating the calculus of the second momenta.

With this improvement, the algorithm keeps track of the curvature of the loss function in the space of parameters, and takes big leaps in steepest direction and small steps in flatter ones, allowing us to adaptively change the descending step size.

One of the most popular among these algorithms is \textbf{Adam}: it accomplish this computation by making use of two different optimization algorithms: AdaGrad and RMSProp.
It is a powerful algorithms since it adapts the learning rate taking into account both the first and the second momenta.
This leads it to perform better and quicker in finding the minima than simpler SGD with momentum algorithm.
\hfill

\noindent The process of computing gradients is an important step of the backpropagation algorithm, which is the process through which the weights of a netwrork are updated.
A neural network with L layers, given an input data, produces an output through the feedforward propagation, and with this value, the loss is calculated.
Denoting with $z_j^l$ the weighted input to the $j-th$ neuron of the $l-th$ layer, the first step of backpropagation is to compute the error
\[
\Delta_j^L =\partial L/\partial z_j^L
\]
On the last layer. Then, using this quantity it is possible to calculate $\Delta_j^l$ for all the layers by exploiting the chain rule of derivatives
\[
\Delta_j^l = \sum_k \frac{\partial L}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l}
\]
Once computed all the errors, it is possible, for each layer, to compute the loss with respect to the weights of the networks and modify them according to equation \ref{eq:weights_update}.


\section{Dimensionality reduction: PCA}\label{sec:pca_theory}



Principal Components Analysis is a method to perform dimensionality reduction.
It is an unsupervised learning algorithm which aims to reduce the dimensionality of input data, preserving as much information as possible from it.
It does so by learning a new representation of data with lower dimensionality than the initial one and whose element have no linear correlation with each other.
It performs then a projection of data on to this new space, created such that its axes lie on the directions along which data variance is the biggest.

\begin{figure}[h]
\centering
\includegraphics[width=.5\linewidth]{ml/pca_example.png}
\caption{Example of the first two principal components in a 2D dataset: the first principal component is placed along the direction where variance is greater,  and the second component is in the orthogonal direction.}
\label{fig:pca_example}
\end{figure}

PCA seeks to find an orthonormal basis so that the first base vector corresponds to the direction along which the variance of data is the greatest.
This vector is called first principal component (PC).
The second principal component is defined as the vector orthogonal to the first, that explains the most variance left once the first component is removed.
And so on, the $i-th$ PC is the direction orthogonal to the first $(i-1)-th$ vectors that maximize the left variance.
In figure \ref{fig:pca_example} is reported a graphic example of two PC extracted from a set of two-dimensional data.

Principal components are calculated as the eigenvectors of the covariance matrix, and the most popular way to implement this calculation is through Singular Value Decomposition (SVD) of the matrix of input data.
SVD is preferred over the simpler calculation of the covariance matrix and its eigendecomposition because there are algorithms that can deal more quickly with SVD and avoid the explicit calculation of the covariance matrix.

Concretely, if we consider a set of n input data vectors lying in a space $\Re^m$, we can represent them as a matrix $X \in \Re^{n \times m}$ where $n$ is the total number of input data and $m$ is the dimensionality of each data (the number of features in each data vector).

We can suppose without loss of generality that feature distributions across data have zero mean, so that for each feature $f_i$ with i = 1, ..., m, $\mu_i = \mathbb{E}[f_i]=0$.

%PCA aims to compute the eigenvectors of t
The covariance matrix $\Sigma \in \Re^{m \times m}$ of input data X is given by
\begin{equation}
\Sigma = \frac{1}{n-1} X^T \cdot X
\end{equation}
because each entry can be written as $\sigma_{jk} = \frac{1}{n-1}\sum_{i = 1}^n (x^{(i)}_j - \mu_j)(x^{(i)}_k - \mu_k)$, and, in the hypotesis of zero mean $\mu_j = \mu_k = 0$, we obtain $\sigma_{jk} = \frac{1}{n-1}\sum_{i = 1}^n (x^{(i)}_j x^{(i)}_k)$.

\noindent Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal of $\Sigma$ we actually have the variances of each feature. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal.

We are interested in finding a new basis such that covariance matrix is diagonal in this basis and then perform a rotation of data using this basis.
%The eigenvectors of the principal components represent the direction of the maximum variance
%and the corresponding eigenvalues, defines their magnitude.
%The principal components of the matrix X are given by the eigenvectors of the matrix $X^T X$ so that we can write $X^T X = W \Lambda W^T$ where $\Lambda$
%PCA finds a representation through a linear transformation z = $X^T W$.
To find it, PCA makes use of Singular Values Decomposition of X.

SVD is basically a factorization of a n x m matrix X that (in the case X is a real matrix), allows to rewrite it as X = $U S W^T$ where U and V are respectively m x m  and n x n orthogonal matrices whose columns are called \emph{left} and \emph{right singular vectors} of X, and S is a  m x n rectangular diagonal matrix.
Diagonal values of S: $s_i$ are uniquely determined by X and are called \emph{singular values} of X.

Using this decomposition it is possible to write
\begin{equation}
\begin{aligned}
X^T X & = (U S W^T)^T(U S W^T) \\
& = W S^T U^T U S W^T \\
& = W S^2 W^T
\end{aligned}
\end{equation}
where we used the definition of orthogonal matrix for U $U^T U$ = I.

We therefore rewrite the covariance matrix
%$Var[X] = \Sigma$ as
\begin{equation}
\Sigma = \frac{1}{n-1}W S^2 W^T.
\end{equation}
%ricorda: la scomposizione agli autovalori di una SQUARE matrix A è a = Q D Q^T where Q is an orthogonal matrix whose columns are eigenvectors of A, and D is a diagonal matrix whose entries are the eigenvalues of A.
which means that the singular values of X: $s_i$ are related to eigenvalues of the covariance matrix by the relation $\lambda_i = s_i^2/(n-1)$, while the right singular vectors of X represents the eigenvectors of the covariance matrix.

Using this result, we consider the transformation of data given by Z = $X W$, to show that with these rotated data, the covariance matrix is diagonal.

\begin{equation}
\begin{aligned}
Var[Z] & = \frac{1}{n-1}Z^T Z \\
 & =\frac{1}{n-1} W^T X^T X W   \\
 & = \frac{1}{n-1} W^T W S^2 W^T W \\
 & = \frac{1}{n-1}S^2
\end{aligned}
\end{equation}

This shows that if we use right-singular vector of X to perform the transformation, the covariance matrix of the transformed data is in a diagonal form.
%when we project the data x to z using the linear transformation W, the resulting transformation has a diagonal covariance matrix which implies that the individual elements of z are mutually uncorrelated.

To actual reduce the dimensionality of our data, we need to extract the first $\tilde m \leq m$ eigenvalues from the covariance matrix, order them in a descending order of magnitude, and collect the corresponding eigenvectors from the matrix W.

With them we can construct the projection matrix $\tilde W_{\tilde m} \in \Re^{m \times \tilde m}$ of the first $\tilde m$ eigenvectors, and use it to project data in order to obtain a new dataset $Y = X \tilde W_{\tilde m}$ made of $n$ new data of dimensionality $\tilde m$.
\hfill

\noindent An important parameters to quantify the amount of variance that PCA is able to explain, is the \emph{variance explained ratio}, given by the ratio of an eigenvalue of the covariance matrix, and the sum of all the eigenvalues.

\begin{equation}
\frac{\lambda_i}{\sum_{k = 0}^m \lambda_k}
\end{equation}

\begin{notes}
\item The maximum number of principal components is limited by the number of data we can use to compute them.
If we have a dataset of $n\_samples$ samples, each with $n\_features$ features, the maximum number of PC is given by $N\_pc = min\{n\_samples, n\_features\}$.
This is because in a $n\_feature$-dimensional space, our points lie in a $n\_samples$-dimensional hyperplane and all the variance can be explained inside this subspace.
For a better understanding of this concept, we can imagine a dataset made of only two data, each containing 3 features. If we represent these data, we construct a 3D space where each orthogonal axes corresponds to a feature, and we insert these two points, each corresponding to a data point.
If we want to explain the maximum variance we need to find a new axis, the first principal component, but through two points passes just one straight line so we can at most limit our analysis to the plane passing through that line.
\end{notes}

%ration lambda_i/sum lambda_i ??

\section{How to assess network performances}
After the model has been trained, it is evaluated on the test set to assess its performances on a new dataset.
A comomon metric for evaluating model performances is the \emph{accuracy}: it is simply defined as the ratio between the total number of correct prediction and the total number of predictions (total number of data in the test dataset).
For a binary classification, indicating as T and F, true and false, and P and N, positive and negative, it is possible to give some important definitions:
\begin{itemize}
\item TP: data classified as P, belonging to class P
\item TN: data classified as N, belonging to class N
\item FP: data classified as P, actually belonging to class N
\item FN: data classified as N, actually belonging to class P
\end{itemize}

Using these definitions, accuracy is defined as
\begin{equation}
Accuracy = \frac{TP + TN}{TP+TN+FP+FN}
\end{equation}

Even though accuracy may seem a good parameter, it is not the most accurate one, when we're dealing with a unbalanced test dataset.
As an example, in a binary classification concerning classes A and B, we can be in a situation where a model is not able to make distinctions between the two classes, and classifies all the data as belonging to A.
If we test this model using a dataset consisting on 10 data, with just 2 samples belonging to B and the rest to A, the model predicts every data belonging to the class A, yet we would get a score of 80 \% accuracy, which is not a thruthful result.

For this reason there are other ways to evaluate performances of a network that overcome this problem.
One of this is based on the introduction of two quantities: precision and recall.
Precision is the ratio between true positive and total positive classified cases (TP + FP), which is a measure of how many positive predicted cases are actually positive.
Recall measures the percentage of actual positives that were correctly classified, or in other words it represents the true positive rate.
\begin{equation}
Precision = \frac{TP}{TP+FP} \qquad \qquad Recall = \frac{TP}{TP+FN}
\end{equation}

Unfortunately precision and recall are linked in a sense that often enhancing one leads to the decrease of the other and vice-versa.
%esempio page 283
It can be set a trade-off between recall and precision according to what quantity is more crucial for the classification task we are performing.
For example we could be willing to take the risk to have more false positive, in order to achieve a greater number of true positive.

To reach a threshold of positive missing $<x\%$ means set the recall to (100-x)\%, this operation of establish a threshold is usually referred as \textquotedblleft setting the operating point\textquotedblright.
This threshold though is not always set at the beginning, since the best operating point is not always clear a priori.
For this reason, what is done is to study the model under all the possible thresholds, and plot results creating a curve called precision-recall curve, an example of which is reported in figure \ref{fig:precisionrecall}.
The closer a curve lies on the upper right corner (high precision and high recall), the more correctly the model is working.
One way to summarize the information of a precision-recall curve can provide us, is to compute the area under the curve, known as \textquotedblleft average precition \textquotedblright

Similarly to the precision-recall curve, a other curve is usually employed to study the effect of different thresholds. It is called the \emph{Receiver Operating Characteristics curve}, usually referred as ROC curve.
% Insert ROC curve computed using the random forest already implemented in my data.
The ROC curve is constructed using the true positive rate (recall) and the false positive rate (FPR) and shows the evolution of TPR vs FPR for diefferent thresholds.
The ideal curve would be close to the top left (high TPR and low FPR) and the less accurate is the model, the more this curve tend to lay down to the bisector line.
To summarize the model's performances with a single number using ROC curve information, we compute the Area Under the Curve AUC.
The reference value for an AUC is 0.5 which is obtained when a model is just randomly predicting and it corresponds to a curve laying on the bisector line.

The AUC can be regarded as the probability that a randomly picked point from the positive class, will have an higher score (according to the model) than a randomly picked point from the negative class.
In other words, the percentage of the AUC value is an estimate of the probability that the model is able to distinguish between the two classes.
An example of ROC curve is shown in figure \ref{fig:roccurve}


\begin{figure}
\centering
\begin{subfigure}[t]{0.4\textwidth}
\includegraphics[width=1.\textwidth]{ml/precisionrecall_example.png}
\caption{Precision-recall curve representing values of precision and recall for different thresholds values}
\label{fig:precisionrecall}
\end{subfigure}
\begin{subfigure}[t]{0.43\textwidth}
\includegraphics[width=1.\textwidth]{ml/roccurve_example.png}
\caption{ROC curve representing values of True positive and False positive rates for different thresholds values}
\label{fig:roccurve}
\end{subfigure}
\caption{}
 
\end{figure}



\chapter{Explainable AI: a game theory approach}\label{chap:shapley_values}
\textcolor{ForestGreen}{
As methods to learn pattern from data becomes more complex, they become harder to interpret.
Deep learning represents an example of a technique to search for nonlinear relations between data, but introducing nonlinearity, makes results difficult to be explained.
Because of this, it is not uncommon to consider a machine learning model as black box where results are collected without any knowledge of the inner processes that produced them.
However, in order to obtain more reliable results, it is a crucial issue to get rid of the black box idea and provide an explanation of the main features that characterized the output of a model
One state-of-art explainatory algorithm is called SHAP, and it is built making use of an important result from game theory.
For this reason, in the following section, we briefly discuss some important concepts related to this field, that were subsequently readapted, for the implementation of this important machine learning explainatory model.
}

\section{Shapley values}

Game theory is a branch of mathematics related to the study of mathematical models to  conceive social situations among competitive players \cite{ross-2021}

It had a great development in the XX century especially during and after the Second World War thanks to the contribution of mathematicians like John Von Neumann, John Nash and Lloyd Shapley.
Game theory mainly focuses on two major research areas: non-coperative and cooperative games.
\begin{itemize}
\item Non-cooperative games concern competition between individual players who don't cooperate each other and can't form coalitions. The main task on non-cooperative games can be summarised as the search for a good strategy for each player. Each player's objective is in fact the maximization of his own utility function.
A big contribution to the development of this theory came from von Neumann, or John Nash with the concept of Nash equilibrium \cite{nash-1950}.

\item Cooperative games (or coalition games) concern competition between groups of player forming coalitions.
Each coalition plays as a single participant and earns a payoff.
One of the the main tasks related to cooperative games is to find a way to divide the total utility among the member of a coalition in an equally way proportionally of how much they contributed to the final score.
\end{itemize}
In 1951 Lloyd Shapley introduced a way to compute the exact amount of payoff for each player, making use of what were named after him: Shapley values \cite{shapley1951-I}\cite{shapley1953}.
Shapley introduced those values in the field of coalition games, therefore to properly understand his work, we need to briefly illustrate what a coalition game consists of.

A coalition game involves N players and different subsets, called coalitions, created from these N players.
Each subset of players S gains a payoff at the end of the game.
A function $\nu$ maps every subset to its payoff $\nu(S)$ = payoff(S) $\in \Re$.
On the basis of which player had more influence in this final score, we ask how to split this payoff in a fair way between each player of the subset, where \textquotedblleft fair \textquotedblright is to be intended as, proportional to his own contribution.
A solution for this problem comes from \textbf{Shapley values} $\phi_i(\nu)$, specific for each player $i\in N$  in a coalition $S \subseteq N$.
The idea behind them is the marginal contribution of that player to the final score, where marginal contribution is defined as the difference on the score of the coalition when player i joins the coalition.
In other words they are the difference between the coalition's score with player i and the coalition's score without him $\text{marginal contribution = }\nu (S \cup \{ i \}) - \nu(S)$. \cite{algaba-2021}

Shapley defined these coefficients (Shapley values) for a player $i$ as a weighted average of marginal contribution values, over all the possible subsets that include player $i$.

The mathematical formulation that Shapley introduced for $\phi_i(\nu)$ is
\begin{equation}
\begin{aligned}
\phi_i(\nu)  & = \frac{1}{N }\sum_{S\subseteq N\ \setminus \{ i \} }  {\binom{N- 1}{|S|}}^{-1} [\nu (S \cup \{ i \}) - \nu(S)] \\
& = \sum_{S\subseteq N\ \setminus \{ i \} }  \frac{S! (N-1-S)!}{N!} [\nu (S \cup \{ i \}) - \nu(S) ]
\end{aligned}
\end{equation}
%That can be interpreted as $\frac{1}{N} \sum \frac{\text{marginal contribution of i to the coalition}}{\text{number of coalitions excluding i of the same size os S}}$
which exactly represent the amount of reward for each player i.



\addcontentsline{toc}{chapter}{MATERIALS \& METHODS}
\chapter*{MATERIALS \& METHODS}

\chapter{Dataset: ABIDE I \& II}\label{chap:dataset}


Data we are going to work on belong to the ABIDE dataset (Autism Brain Images Data Exchange): a project founded with the aim of investigating ASD using magnetic resonance structural images and resting state fMRI scans.
%To address the problem with these two different approaches they collected data acquired over the years from different medical centers, and put them together in a single dataset.

The whole ABIDE dataset was published in two releases: ABIDE I released in August 2012 and containing 1112 patient scans, and ABIDE II released in June 2016 containing 1114 scans.
For each site autism was diagnosed either by gold standard diagnostic tests, clinical judgment or a combination of clinical gold standard procedures.

ABIDE I includes scans collected from 17 different sites, and the 1112 patients consist on 539 patients with ASD and 573 typical control patients.
ABIDE II includes scans collected from 19 different sites, and the 1114 patients consist on 593 patients with ASD and 521 typical control patients.
Not every site belonging to ABIDE II is different from those of ABIDE I, but, even though some medical centers are the same, the scanner type, or acquisition pipeline and parameters may have been changed during the time interval between the two releases, so in all our analysis, they are regarded as different acquisition sites.
\textcolor{ForestGreen}{
Furthermore, even within a single release, such as ABIDE II, there are sites that released two samples of data.
For this reason, some of these samples are labeled with a number like \_1 or \_2. 
For a better understanding of the different sites and samples, and the main acquisition parameters employed, a list is reported in table \ref{tab:scan_sites}.
In this table, the acronym of each site and sample, which is employed to label them in the following figures, is associated to the actual site name.
}





\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrr}\toprule
Acronym &Site name &Scanner name &TE [ms] &TR [s] &$B_0$ [T] \\\midrule
CALTECH &California Institute of Technology &Siemens TrioTim &30 &2 &3 \\
CMU &Carnegie Mellon University &Siemens Verio &30 &1.5 &3 \\
KKI &Kennedy Krieger Institute &Philips Achieva &10 &2.34 &3 \\
LEUVEN\_1 &University of Leuven &Philips Intera &33 &1.68 &3 \\
LEUVEN\_2 &University of Leuven &Philips Intera &33 &1.65 &3 \\
MAX\_MUN &Ludwig Maximilians University Munich &Siemens Allegra &30 &3 &3 \\
NYU &NYU Langone Medical Center &Siemens Allegra &15 &2 &3 \\
OHSU &Oregon Healt and Science University &Siemens TrioTim &30 &25 &3 \\
OLIN &Olin Neuropsychiatry Research Center &Siemens Allegra &27 &1.5 &3 \\
PITT &University of Pittscurgh School of Medicine &Siemens Allegra &25 &1.5 &3 \\
SBL &Social Brain Lab &Philips Intera &30 &3.2 &3 \\
SDSU &San Diego State University &GE MR750 &30 &2 &3 \\
STANFORD &Stanford University &GE SIGNA &30 &2 &3 \\
TRINITY &Trinity Centre for Health Sciences &Philips Achieva &28 &2 &3 \\
UCLA\_1 &University of California Los Angeles &Siemens TrioTim &28 &3 &3 \\
UCLA\_2 &University of California Los Angeles &Siemens TrioTrim &30 &2 &3 \\
UM\_1 &University of Michigan &GE Signa &30 &2 &3 \\
UM\_2 &University of Michigan &GE Signa &30 &2 &3 \\
USM &University of Utah School of Medicine &Siemens TrioTim &28 &2 &3 \\
YALE &Yale Child Study Center &Siemens TrioTim &25 &2 &3 \\
BNI\_1 &Barrow Neurological Institute &Philips Ingenia &25 &3 &3 \\
EMC\_1 &Erasmus University Medical Center Rotterdam &GE MR750 &30 &2 &3 \\
ETH\_1 & ETH Zurich &Philips Achieva &25 &2 &3 \\
GU\_1 &Georgetown University &Siemens TrioTim &30 &2 &3 \\
IP\_1 &Institute Pasteur \& Robert Debrè Hospital &Philips Achieva &45 &2.7 &1.5 \\
IU\_1 &Indiana University &Siemens TrioTim &28 &0.813 &3 \\
KKI\_1 &Kennedy Krieger Institute &Philips Achieva &30 &2.5 &3 \\
KUL\_3 &Katholieke Universiteit Leuven &Philips Achieva &30 &2.5 &3 \\
NYU\_1 &NYU Langone Medical Center &Siemens Allegra &78 &5.2 &3 \\
NYU\_2 &NYU Langone Medical Center &Siemens Allegra &78 &5.2 &3 \\
OHSU\_1 &Oregon Healt and Science University &Siemens TrioTim &30 &2.5 &3 \\
OILH\_2 &Olin Institute of Living Hartford &Siemens Allegra &27 &1.5 &3 \\
SDSU\_1 &San Diego State University &GE MR750 &30 &2 &3 \\
SU\_2 &Stanford University &GE Signa &30 &2 &3 \\
TCD\_1 &Trinity Centre for Health Sciences &Philips Achieva &27 &2 &3 \\
UCD\_1 &University of California Davis &Siemens TrioTim &24 &2 &3 \\
U\_MIA\_1 &University of Miami &GE &30 &2 &3 \\
USM\_1 &University of Utah School of Medicine &Siemens TrioTim &28 &2 &3 \\
\bottomrule
\end{tabular}
\caption{Table of the sites included in ABIDE dataset, with the corresponding acronym.
This acronym labels the site and the corresponding sample with a number such as \_1 or \_2.
For each site, the main information about scan type, static magnetic field $B_0$ echo time TE and repetition time TR are provided.}\label{tab:scan_sites}
\end{table}






In addition to scan images, ABIDE provides every information related to each patient, as age, sex, intelligence quotient (FIQ), eye status during the scan (open or closed), and every additional clinical information provided by patients.
The vast majority of patients are males as shown in figure \ref{fig:mf_site}, for a total amount of 1804 males and 422 females.
The number of males is greater than the number of females as a consequence of the greater probability for male to be affected by ASD, the ratio between males and females has been estimated to be approximately 4:1 \cite{fombonne2009}.
Control/ASD patient number is more or less balanced for every site, with the exception of KKI\_1 (Kennedy Krieger Institute, sample 1) that provided two times more controls than ASD patients, and KUL\_3 (Katholieke Universiteit Leuven, sample 3) and NYU\_2 (NYU Langone Medical center, sample 2) that only provided ASD cases. 
For a visual comparison, the number of controls/ASD for each site is displayed on the histogram in figure \ref{fig:controlcase_site}.

\begin{figure}[h]
\centering
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/mf_site.png}
\caption{Histogram of male and females subjects per site from the whole ABIDE I \& II dataset}
\label{fig:mf_site}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/controlcase_site.png}
\caption{Histogram of control and ASD subject per site from the whole ABIDE I \& II dataset}
\label{fig:controlcase_site}
\end{subfigure}
\caption{}
 
\end{figure}


Patients in ABIDE dataset have ages ranging from 4 to $>$ 50 years old, but as shown in the distribution in figure \ref{fig:abideages} the vast majority of participant are younger than 40, precisely $>$ 97 $\%$ of participant are under 40 y.o., also, the majority of sites provide only young patients in a restricted age range, but  as can be seen from figure \ref{fig:abide_age_site} there are some sites that acquired patients with a wide age range (MAX\_MUN or BNI\_1 for example).


Intelligence quotient, whose distribution is shown in figure \ref{fig:abidefiq}, was not provided for every participant, in fact 171 patients out of 2226, coming from sites UM1 and EMC1 (figure \ref{fig:abide_fiq_site}) lack this information.
In our further analysis, before proceeding these values were replaced by the average value of all the other provided values.

The lack of a common acquisition protocol is also evident from the eye status at scan feature: as shown in figure \ref{fig:abideeyesite} each site acquired scans either with open eyes or with closed eyes, without a common procedure, and sometimes this information is not even specified.
Overall, the whole dataset consists of more than 70\% of patient acquired with open eyes, as shown in figure \ref{fig:abideeye}.

Considering all the information above, we can limit our further analysis in order to work on a more homogeneous dataset. In this way, we try to \textcolor{ForestGreen}{remove from the very beginning} some source of variability due to unavoidable difference due to sex or age.
We have then carried out our analysis on a dataset consisting on only male patients with an age within 5 and 40 years. Some further analysis were performed with further cut on eye status at scan, selecting only patients with open eye.




\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{abide/abide_ages.png}
\caption{\textbf{Age} distribution of the whole ABIDE I \& II dataset}
\label{fig:abideages}
\end{subfigure}

\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/abide_age_site.png}
\caption{Boxplot of \textbf{age} of patients per site}
\label{fig:abide_age_site}
\end{subfigure}
\caption{Histogram distribution and boxplot of AGE (jointly for Controls and ASD) per site, from ABIDE I + II}
 
\end{figure}






\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{abide/abide_fiq.png}
\caption{\textbf{FIQ} distribution of the whole ABIDE I \& II dataset}
\label{fig:abidefiq}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/fiq_site.png}
\caption{Boxplot of patient's \textbf{FIQ} per site}
\label{fig:abide_fiq_site}
\end{subfigure}
\caption{Histogram distribution and boxplot of FIQ (jointly for Controls and ASD) per site, from ABIDE I + II}
 
\end{figure}








%eye plots
\begin{figure}
\centering
\begin{subfigure}{0.7\linewidth}
\includegraphics[width=\linewidth]{abide/eye_site.png}
\caption{Eye status distribution per site of the entire ABIDE dataset}
\label{fig:abideeyesite}
\end{subfigure}
%\hspace{0.1 cm}
\begin{subfigure}{0.4\linewidth}
\includegraphics[width=\linewidth]{abide/abide_eye.png}
\caption{Overall eye status distribution on the entire ABIDE dataset}
\label{fig:abideeye}
\end{subfigure}
\caption{Histograms of patients with open (orange), closed (green) and unknown (blue) eye status in the whole ABIDE I \& II dataset}
 
\end{figure}



\chapter{Image preprocessing}\label{chap:image_preprocessing}
\section{Common preprocessing steps}
\label{sec:preprocessing_steps}

Data acquired after a MRI and fMRI session are not ready to be analyzed to perform functional connectivity \textcolor{ForestGreen}{or other kind of} measures.
In fact data are usually affected by different sources of noise and artifacts.
It is possible to distinguish between two main sources of noise: physiological noise due to normal biological functions of patients such as heartbeat and breathing, and movement of subject inside the scanner.
Additionally it is possible to identify artifacts due to inner flaws of the scanning systems such as inhomogeneities of the static magnetic field or drift of the baseline of the BOLD signal \cite{bijsterbosch2017}.


%For example, breath rate, can affect BOLD signal because of the induced local motion of the brain's vessels, or the change in blood oxygenation and pressure. The second noise source is due to movements of the patients during the scan, such as head movements or small body adjustments.

%One of the main artifact intrinsic to the signal nature is distortion and field inhomogeneities, deriving from making the scan sensitive to the BOLD signal which intrinsically is the detection of a signal loss due to field distortion.
%This could be corrected by employing small coils inside the scan to smooth magnetic field differences, this process called shimming still left some artifacts, therefore common data preprocessing pipelines use extra acquisitions to create a field map of the remaining field inhomogeneities and a shift and stretching voxel to correct for them.

%In the following lines, we list the most important causes of noise and how they are corrected during common preprocessing steps implemented in the main software of analysis.

In this section we briefly review some of the most important artifacts and the common procedures to reduce their effect on the scquired images, and other common preprocessing steps applied to images to prepare them for subsequent analysis such as functional connectivity measures.
%Subsequently we discuss the implementation of a software
\begin{itemize}
\item Head motion effect is due to the physical movement of the patient inside the scanner. It results in a misalignment from one acquired volume to the next.
To correct this effect, \textbf{motion correction} steps are performed at the beginning of image preprocessing. Motion correction works by spatially applying transformations as rotation or translation volume by volume, aiming to overlap every acquired slice to a chosen reference volume, like the first or the middle one.

\item For EPI data, \textbf{slice timing correction} is usually performed as well. It aims to correct artifacts deriving from the sequentiality of acquisition for each slice of the brain, due to which each slice is acquired at different time.
In EPI images, the entire time elapsed to acquire all the slices is called repetition-time, and it usually ranges from 1 to 3 seconds, and during this interval signal may lose its initial strength.
Slice timing correction uses interpolation in time to shift the BOLD timeseries of each voxel, in order to align them to a reference starting time.
A downside of the use of interpolation, though, is the signal smoothing it performs that can lead to a slight loss of high frequencies information.

\item  \textcolor{ForestGreen}{\textbf{Distortion correction} is usually applied to reduce distortion in EPI images caused by inhomogeneities in the magnetic field arising from flaws of the external magnetic field, or caused by different tissue within the brain which cause localized distortion.
They are usually corrected during the image acquisition procedure by means of electromagnets called \emph{shims} coils used to generate a magnetic field that opposes and cancels these inhomogeneities.
Yet, this procedure, often referred as \emph{shimming}, is not always able to cancel all the inhomogeneities, and the remaining ones are corrected during preprocessing.
The correction is possible by using a \emph{fieldmap}: a map containing magnitude information of the magnetic field across the space.
Fieldmaps sould be acquired after each scan, but in absence of them, they can be generated using tools such as SPM
\footnote{https://www.fil.ion.ucl.ac.uk/spm/toolbox/fieldmap/}
}
\item A further common step is \textbf{spatial smoothing} of both structural and functional data. It operates calculating a weighted average of each pixel over neighbored voxels. To this end, a gaussian kernel with a chosen FWHM is applied to create the weights.
Spatial smoothing is useful to avoid abrupt changes of signal between two neighbouring voxels.

\item \textbf{Band-pass temporal filtering} is commonly applied to BOLD data, aiming to reduce artifacts from hardware such as an effect called \textquotedblleft drift \textquotedblright namely the slow change of the BOLD baseline signal over time. This is accomplished by applying a high-pass filter to remove very low frequency components. This filtering operation is usually referred as a removal of linear trends, since we are removing from a signal only components with a frequency lower than the low-frequency fluctuations of BOLD signal.
At the same time, a low-pass filter is applied to remove all frequencies above a cut off frequency, it is commonly applied in processing resting state fMRI data because the physiological signal is driven by low frequencies oscillations while high frequencies are associated to noise.
\end{itemize}

All the preprocessing steps cited above are performed for each patient, and their main common objective is to enhance the signal to noise ratio of each image. However, if our task is to perform a group analysis, and compare different patient images, a further, essential step called \textbf{registration} has to be carried out.

During registration structural (T1-weighted) data are aligned over a standard coordinates system space to universally describe location of the different brain parts, to make sure that the same voxel coordinate corresponds to the same brain area for all the subjects.
Registration occurs for functional images as well: they are firstly aligned and adapted to the structural image and then registered to the same standard template.
The most common template, provided by packages like FSL is Talairach and Monreal Neurological Institute's MNI152, adopted by the International Consortium of Brain Mapping (ICBM) as the international standard, replaced the previous Talairach and Tournoux template.
Because the adoption as international standard it is also called ICBM152 but the most common name remains MNI152.

To understand how MNI152 differs from the Talairach and Tournoux we have to spend a few words about the latter.\cite{brett2002}

\noindent\fbox{%
    \parbox{\textwidth}{
    \textbf{From Talairach-Tournox to MNI152 template}\hfill

    The Teilarch and Tournoux space was published in 1988 and introduced some innovative aspects to tackle the problem of the great variability in brain anatomy between people which so far had limited the accuracy of statistical analysis.
    They introduced a common coordinate system to identify different brain locations, based on some anatomical landmarks, and introduced a spatial transformations to match different brains.
    They choose two anatomical areas as reference areas: the anterior and the posterior commissure, which are relatively invariant between different brains and conjuncted them with an axis. This axis constitutes the horizontal y axis, and the origin is placed in correspondence of the anterior commissure. From it, a perpendicular axis passing thgough the interemispheric fessure is chosen as the z axis and consequently the x axis is chosen perpendicular to y and z.
    %An horizontal plane passing through this axis was chosen such that it was perpendicular with the interemishpere axis.
    This way they created a 3D coordinate system called the Talairach coordinate system.
    Afterwards, to match different brains they described a set of spatial transformation, one for each different brain quadrant, to transform a brain in order to match the principal anatomical structures of each other.

    From this template, a first MNI template was created, called MNI305, created by manual scaling 241 brains to the Talairach template and averaging them to obtain  new template, and then transform 305 additional scans to this new template and averaging them to create a second average and final template (MNI305).

    From this template, MNI152 was finally created and published in 2001, by registering 152 T1 scans to the MNI305 template and averaging them \cite{brett2002_w} \cite{brett2002}.

    }%
}

\hfill

\noindent This template is used both for the structural and the functional registration of a MRI and fMRI scan.
%When we work with functional images though, a second registration step occurs within each patient to align EPI (functional) data to the structural registered image of that subject.

\begin{figure}
\centering
\includegraphics[width=.3\linewidth]{mri/registration_my.png}
\caption{Registration steps:
 starting from the first row there are the acquired functional and structural images and the MNI template respectively
 The second row shows the functional image registered on the structural space
 The third row shows the final images both registered to the MNI152 template
}
\label{fig:registrationchart}
\end{figure}

Looking at figure \ref{fig:registrationchart} we can see an example of functional and structural image registration to a MNI152 template: firstly the functional image is registered to the structural and next, they are both registered to MNI template.

Once the structural and functional images have been registered to a standard space, is possible to extract brain region information using an \textbf{atlas}.

Atlases are in the same space as the template image (MNI or Talairach space for example) and consist of a 3D standard brain template where different brain areas are associated to a label.
This division into areas and the subsequent labeling, is called parcellation.

There are different atlases, each one including a different parcellation of the brain, which is obtained by dividing it into N labeled ROIs
(Regions Of Interest) to focus on anatomical and/or functional regions, according to the study we are carrying out.
%We will describe some of the most important atlases in the following section (\ref{sec:cpac})
Some of the most popular atlases are:

\begin{itemize}

\item The H-O atlas was first employed in 2006 \cite{makris2006} and consists of a subcortical and a cortical atlas, with a total of 117 ROIs of which 21 subcortical and 96 cortical (48 for each emishpere). A colored representation of the two (cortical and subcortical) parcellation is shown in figure \ref{fig:hoatlas}.

\item The Automated Anatomical Labeled, an atlas created in 2002 \cite{mazoyer-2002} consisted of 90 total ROIs, 45 each emisphere. Since then, different modified and improved version were released, the last of which AAL3 consisting of 166 ROIs.

\item CC200 and CC400 are more recent atlases created by C. Craddock in 2012 \cite{craddock2012} for functional parcellation, consisting of 200 and 392 ROIs respectively \footnote{http://ccraddock.github.io/cluster\_roi/atlases.html}.

\item The Desikan atlas \cite{desikan2006} originally created in 2006 and consisting on 68 regions also underwent different transformation towards a finer parcellation of the brain.

\end{itemize}


\begin{figure}[h]
\centering
\includegraphics[width=.4\textwidth]{mri/hoatlas.png}
\caption{Horizontal section of Harvard-Oxford subcortical (sx) and cortical (dx) atlased}
\label{fig:hoatlas}
\end{figure}




\section{Preprocessing of ABIDE I \& ABIDE II dataset}\label{sec:cpac}

\paragraph{ABIDE-preprocessed initiative}\hfill

In 2013 Neuro Bureau Preprocessing Initiative took care of data preprocessing of ABIDE I dataset and shared its results making them publicly available \cite{cameron2013}.
Thanks to their work, ABIDE I data are now available both as raw data and as preprocessed data.
Four preprocessing approaches were employed each from a different team and each one publicly available \footnote{http://preprocessed-connectomes-project.org/abide/C-PAC.html} for download.
The four pipelines for image preoprocessing employed for ABIDE I are:
\begin{itemize}
\item Connectome Computation System (CCS)
\item Configurable Pipeline for the Analysis of Connectomes (C-PAC)
\item Data Preprocess Assistant for Resting-State fMRI (DPARSF)
\item Neuroimaging Analysis Kit (NIAK)
\end{itemize}
The preprocessing steps implemented by different pipelines are similar. They differ on the programming language on which they rely (Python, MATLAB..) the algorithm implementation, the order of preprossessing steps and their parameters.


\paragraph{C-PAC the Configurable Pipeline for the Analysis of Connectomes}\hfill

\textcolor{ForestGreen}{Preprocessed data are only available for the ABIDE I dataset.
In order to perform analysis on a bigger dataset comprising ABIDE II as well, we need to run a preprocessing pipeline to create a unified dataset with data belonging to ABIDE I and ABIDE II, obtained following the same preprocessing steps.}
% work only concerns ABIDE I dataset, and our task is to work with the entire ABIDE I + ABIDE II dataset, we to repeat the preprocessing procedures in order to obtain a dataset including both ABIDE I and II preprocessed with the same pipeline.

In our work, we choose to process data from ABIDE I and ABIDE II using C-PAC: a configurable, open source pipeline, based on Nipype platform.
\textcolor{ForestGreen}{We selected this software after a brief review of the most used pipeline among the four employed for ABIDE I. C-PAC and DPARFS resulted to be the most popular choices, however, we choose C-PAC since previous studies compared the different pipelines and found out that images preprocessed with C-PAC lead to a better classification control/ASD on ABIDE I dataset \cite{yang2020}.}

C-PAC was run on a Docker container installed on a personal computer with an hardware and software setup consisting on:
\begin{itemize}
\item 16-core Intel i7-5960X processor and 64 Gb RAM
\item Ubuntu 20.04 operating System
\item C-PAC 1.8.1 installed on Docker v. 20.10.11
\end{itemize}

We were allowed to run 3 patients in parallel, reserving 4 cores for each partecipant and up to 12 Gb memory to each patient, necessary to save intermediate-steps outputs.

%We choose to employ C-PAC because, basing on machine learning classification results, \cite{yang2020} C-PAC prove to be the most efficient preprocessing pipeline to preprocess ABIDE dataset.
C-PAC employs tools like AFNI\footnote{https://afni.nimh.nih.gov/}, FSL\footnote{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslOverview} and ANTS\footnote{http://stnava.github.io/ANTs/} to perform image correction of structural MRI and rs fMRI.

\textcolor{ForestGreen}{Preprocessing of ABIDE I and ABIDE II data was carried out along the lines of what was fulfilled on ABIDE I by the C-PAC team.}
%Data were preprocessed following the same steps as the pipeline employed with C-PAC to create the ABIDE-preprocessed dataset.
This pipeline includes both anatomical and functional preprocessing.
Anatomical pipeline steps consist on:
\begin{itemize}
\item Skull removal using AFNI's 3dSkullStrip
\item Tissue segmentation using FSL-FAST, to separate gray matter, white matter and CSF, using a thresholding probability map
%whose threshold values are set the same as ABIDE-preprocessed pipeline values
\item Registration to a standard template using ANTS, with a spatial resolution of 2mm
\end{itemize}
Functional pipeline consists on the following steps
\begin{itemize}
\item Slice timing correction using AFNI-3dTshift.
\item Motion estimate and correction using AFNI-3dvolreg.
\item Distortion correction using PhaseDiff and AFNI 3dQWarp.
\item Create a brain-only mask of the functional data using AFNI 3DAUTOMASK.
\textbf{ForestGreen}{
\item Band-pass filtering of (0.01 - 0.1) Hz.
\item Time Series extraction using different atlases.
}
\end{itemize}

\hfill

At the end of functional preprocessing steps, timeseries are extracted from each patient, making use of different atlases implemented in C-PAC, \textcolor{ForestGreen}{provided by different softwares as FSL for Harvard-Oxford (HO), AAL Toolbox for Automated Anatomical Labeled (AAL), pyClusterROI for CC200, CC400 etc.}

The AAL atlas employed in C-PAC is an intermediate version among the different releases of this atlas and consists on 116 ROIs\footnote{http://preprocessed-connectomes-project.org/abide/Pipelines.html}.

\textbf{red}{Va bene parlare di desikan DesikanKilliany senza dire qual è il software che lo fornisce? Non si trovano informazioni..}
C-PAC employs three different Desikan atlases, the one we found more information about is a modified intermediate version named DesikanKilliany consisting on 94 ROIs of which 32 cortical regions each side, 3 ROIs belonging to cerebellar vermis and 29 subcortical regions.

According to previous studies \cite{spera-2019} the atlas that gives best classification performances is Harvard-Oxford, so this is the atlas we choose to employ for our work.

The H-O atlas employed by C-PAC is provided by FSL library \footnote{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/}, and as includes both subcortical and cortical atlases, even though there's only 111 ROIs, namely 14 subcortical and 97 cortical.
The lacking ROI in the subcortical areas are L/R cerebral white matter, L/R cerebral cortex, L/R lateral ventricle and the brain stem \footnote{Subcortical ROIs https://neurovault.org/images/1700/ \quad Cortical ROIs https://neurovault.org/images/1705/ }.
%In this unified HO atlas (subcortical and cortical) the entire cerebral cortex area was removed because is replaced by the finer parcellation of the cortical areas from the cortical atlas.
Removing these regions from the 117 ROIs we should obtain 110 ROIs, while the atlas employed in C-PAC has 111 regions.
The extra cortical ROI in this H-O atlas is present in the 83th position and is named 3455; however there are no information abut this ROI neither on Harvard-Oxford documentation nor on C-PAC's/FSL's documentation, and because it is only made of 2 voxels it was excluded from our further analysis.
In summary we are able to obtain 110 ROIs with Harvard-Oxford atlas, and for each ROI we are able to extract a timeseries using C-PAC.


As is possible to notice from figure \ref{fig:confrontoabidepreproc} timeseries extracted after our preprocessing pipeline do not exactly match those from ABIDE preprocessed.
Even if it \textcolor{ForestGreen}{they follow a similar trend, there are some local differences between the two plots.}
This slight difference is most likely due to two main reasons: the differences in software version, and the lacking of a detailed step-by-step pipeline legend showing the value of all the sub parameters employed during the C-PAC analysis pipeline.
Abide preproccessed data were obtained using one of the first version of C-PAC; nowadays, after more than 7 years, C-PAC and the libraries it relies on were upgraded several times and this may have slightly affected the final outcome of the process.
We found though the old pipeline configuration file employed on ABIDE preprocessed \footnote{https://github.com/preprocessed-connectomes-project/abide}, but since it belongs to a previous version of C-PAC, it lacked information about lots of sub-parameters and sub-settings added in these years.
For this reason, in our pipeline configuration file, parameters in common between our file and ABIDE preprocessed' were set the same as ABIDE, and for all the others we choose to left the C-PAC's default values.

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{abide/timeseries51456.png}
\caption{\textcolor{ForestGreen}{Comparison of 4 timeseries between ABIDE-preprocessed and the ones obtained by our implementation of C-PAC. The four plots show four randomly chosen ROIs belonging to patient 51456 from ABIDE I. They corresponding to ROI: 10, 601, 1901, 4702 which are the corrisponding lables on the Harvard-Oxford atlas legend of regions: left thalamus, right inferior frontal gyrus, right supramarginal gyrus, left supracalcarine cortex.}}
\label{fig:confrontoabidepreproc}
\end{figure}



\chapter{Connectivity measures}\label{chap:connectivity_coefficients}
As described in chapter \ref{sec:preprocessing_steps} if we choose an atlas on fMRI data, for brain parcellation, we can extract timeseries of different brain areas. If we use Harvard-Oxford atlas, we are able to extract 110 timeseries for each patient.
With these data, we want to construct a correlation matrix for each patient, by pairwise comparing timeseries and extracting a coefficient representative of functional correlation between two areas.
\textcolor{ForestGreen}{We are only interested in correlation between timeseries belonging to different brain areas since correlation between a timeseries with itself would would not carry any information.
Accoridng to this, the total number of combination achievable with n timeseries is given by
}
\begin{equation}
N_{comb} = \frac{n\cdot(n-1)}{2}
\end{equation}
Therefore, employing H-O atlas with 110 ROIs we obtain 5995 combination each one expressed by a correlation coefficient.

In the following section we discuss two different approaches \textcolor{ForestGreen}{to extract a correlation coefficients by comparing two timeseries:} the first makes use of Pearson correlation analysis, a useful tool to quantify the linear correlation between two timeseries, and the second approach, based on wavelet analysis, performs a comparison between two signals in time-frequency domain.
We start discussing Pearson correlation coefficients in the following section and in the next section we describe the main traits of wavelet analysis and how we extracted a correlation coefficient from a time-frequency analysis of two timeseries.

\section{Pearson correlation and Z-Fisher transform}
Pearson correlation often simply called correlation coefficient is the measure of a linear relation lying between two sets of data $x$ and $y$, is defined as the covariance of $(x, y)$ over the product of the standard deviation of the two sets.

\begin{equation}
Corr(x, y) = r_{xy}=  \frac{Cov\left( x, y\right)}{\sigma_x \sigma_y} = \frac{E\left[ \left( x - \mu_x \right) \left( y - \mu_y \right)\right]}{\sigma_{x} \sigma_{y}}
\end{equation}
Pearson correlation coefficients have the important property to be scale and magnitude invariant, since each timepoint is shifted by the average value of the timeseries and divided by its standard deviation.
They can assume values between -1 and +1 where the extremes correspond to exact anti-correlation or correlation respectively, so that, if a linear relation lies between the two sets, a high absolute value of Pearson coefficient indicates that the two series \textcolor{ForestGreen}{are highly  (positively or negatively) correlated}.
%tend to be simultaneously greater or lower than their respective means.
\cite{baldini2021}

Given two series x and y, of length n correlation can be easily computed by
\begin{equation}
r_{xy} = \frac{\sum_{i = 1}^n \left( x_i - \mu_{x_i}\right)\left( y_i - \mu_{y_i}\right)}{\sqrt{\sum_{i = 1}^n \left( x_i -\mu_{x_i}\right)^2}\sqrt{\sum_{i = 1}^n \left( y_i -\mu_{y_i}\right)^2}}
\end{equation}

For each patient, Pearson correlation coefficient was computed for all timeseries pairs.
Before further analysis, a common way to proceed is to transform each coefficient with Fisher z-transformation.\cite{spera-2019}
The reason behind this common operation appears clear when we are dealing with highly correlated variables.
When Pearson correlation distribution results in an highly skewed distribution, Fisher's transform sought to transform it into a normal distribution whose standard error is approximately constant equal to $\sigma = \/\sqrt{N-3}$ where N is the total number of points, and it does not depends on the values of correlation.\cite{wicklin2017}

%With this property, Fisher transform is also important to test some hypotesis about correlations, we can run the test with the transformed variables which are normal distribuited with a known variance.
\textcolor{ForestGreen}{
Thus, this transformation allows us to obtain a variable which is normally distribuited even when Pearson correlation coefficients follow a bivariate normal distribution
%or they are leaning toward the extremes.
In our data, this difference between the two distributions is not pronounced, one example where this difference is evident is shown in figure \ref{fig:pearson_zscore_distribution_3}, but since we are not working with highly correlated or uncorrelated variables, there are many other examples where pearson correlations already follow a gaussian distribution and there is not much difference with z-score values distribution.
%In any case, we are dealing with a bivariate distribution, and then performing Fisher transformation is a common practice to get a dataset more normally distributed.
The Fisher transformation is defined as:
}
\begin{equation}\label{eq:fisher_transform}
z = \frac{1}{2}ln\left(\frac{1+r}{1-r}\right) = arctanh(r)
\end{equation}

At the end of this analysis, we obtain correlation matrices like the one shown in fig \ref{fig:corrmatrices}, referred to patient 20243 from ABIDE I dataset.


\begin{figure}[h!]
\centering
\includegraphics[width=.7\textwidth]{pearson/pearson_zscore_feature_3}
\caption{Distribution of feature 3: Pearson correlation values in blue and Fisher transformed values in light orange}
\label{fig:pearson_zscore_distribution_3}
\end{figure}


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{pearson/corrmatrix.png}
\caption{}
\label{ref:corrmatrix}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{pearson/zscorematrix.png}
\end{subfigure}
\caption{Correlation and z-values matrix computed from timeseries extracted using Harvard-Oxford atlas, from patient 20243 belonging to ABIDE I dataset. \textbf{Note:} The two figures are not on the same values scale because Pearson correlation coefficients have range [-1, 1], and z-scored coefficients have range ($-\infty,\infty$)}
\label{fig:corrmatrices}
\end{figure}

\section{Wavelet analysis}\label{sec:wavelet_theory}
A different approach \textcolor{ForestGreen}{measure a correlation between two timeseries is} by making use of wavelet analysis \cite{ferrante-2015} \cite{vandenberg-1999}.
Wavelet transformation is a mathematical tool for analyzing time series or images, and provides a comprehensive way for investigating the bivariate relationship between timeseries both in time (or space) and frequency domain.

To understand wavelet transformation it could be helpful to compare it with Fourier analysis.
Fourier analysis allows us to expand a periodic function f(t) into a series, an ideally infinite sum of weighted sines and cosines with different frequencies:
\begin{equation}\label{eq:fourier_series}
f(t) = a_0 + \sum_{k = 1}^\infty (a_k cos(2\pi k t/T) + b_k sin (2\pi k t /T))
\end{equation}
In equation \ref{eq:fourier_series} terms corresponding to the k-th frequency are called harmonics and they are multiples of the fundamental frequency corresponding to k = 1.

The Fourier Transform (FT) is a natural extension of the Fourier series to aperiodic functions defined over the real axis.
Aperiodic functions don't allow a discrete superposition of sines and cosines terms, and for this reason they need to be represented by a continue superposition.
Fourier transform takes a function from time or space domain and turns it into spatial or temporal frequency domain.
It is a complex function since sines and cosines terms can be represented by a complex exponential as shown below in equation \ref{eq:cft}.
\begin{equation}\label{eq:cft}
\tilde{F}(\omega) = \int\limits_{-\infty}^{+\infty} f(t)e^{-i \omega t} dx
\end{equation}
To deal with discrete signal, for example a sequence $x_n$ obtained from a discrete sampling of a continuous signal x(t), is possible to make use of the Discrete Time Fourier Transform, wich allows us to write the $\tilde F(\omega)$ as in equation \ref{eq:dft} which represent the discrete version of \ref{eq:cft}.

The Discrete Fourier Transform, is useful to analyze discrete periodic signals.
It acts on a function defined over a finite domain and returns a sequence of samples of the discrete Fourier transform we can denote as $\tilde{x_k}$

\begin{equation}\label{eq:dft}
 \tilde{F}(k) = \tilde{x}_k = \sum_{n = 0}^{N-1}x_n e^{\frac{-2\pi i k n}{N}}
\end{equation}
where N is the total number of points of the timeseries x(t).
The frequencies at which samplings are computed are $\omega_k = 2\pi k/N$.
The series we obtain from equation \ref{eq:dft}: $\tilde{x}_k$ is periodic, with a period is equal to N.

One of the main drawbacks of FT though, is the lacking of spatial information, for example for a non stationary signal, is possible that the signal contains a variable component of frequency in different space or time locations.

A time-frequency analysis provides for this lacking by computing both frequency and spatial information from a signal, allowing us to analyze non stationary signals and obtain information both on time (or space) and frequency domain.
Since we are working on signal defined over a time domain, from now on we would refer only to  \textquotedblleft time \textquotedblright  and  \textquotedblleft frequency \textquotedblright domain, but we keep in mind that all the relation are true for spatial and spatial frequency domain as well.
\textcolor{ForestGreen}{
The process through which we extract information about frequencies and time location is a convolution of the signal $x(t)$ with a function $w(t)$ as shown in equation \ref{eq:convolution_time_frequency}.
These functions are called \emph{windowing function} and are usually centered at zero with a symmetric trend that rapidly decays towards zero.
In a convolution, parameters describing frequency and time location are often simply called $a$ and $b$,
%where a is the parameters that describes frequency information and b is a time translation parameter, and acts by simply shifting the windowing function throughout the entire timeseries x(t).
\begin{equation}\label{eq:convolution_time_frequency}
\tilde{x}(a, b) = \int_{-\infty}^{\infty} x(t)w(t-b) e^{-2\pi i a t} dt
\end{equation}
The convolution term $w(t-b) e^{-2\pi i a t}$ changes according to the type of analysis we are performing.
While the time parameter $b$ acts the the same way through all the type of analysis, by simply shifting the windowing function throughout the entire timeseries x(t), it is $a$ that differentiates the most the different analysis.
}
It is the frequency parameter and the way it is introduced in a convolution determines the developing of the transform.
\textcolor{ForestGreen}{
We distinguish between two main families of time-frequency analysis, according to the convolution term employed and the effect of parameter $a$:
}
\begin{enumerate}
\item Windowed Fourier Transform (WFT): $\psi_{ab}(t) = e^{it/a}\phi(t-b)$ where $\phi(t)$ is a window function of constant width and the parameter a acts as frequency modulation (freq $\sim$ 1/a): the lower is a, the greater the number of oscillation inside the window $\phi(t)$.
\item Wavelet Transform (WT): $\psi_{a, b}(t) = \frac{1}{\sqrt{a}}\psi(\frac{t-b}{a})$ where $\psi$ is a function called \emph{mother wavelet}; a is a positive real number and defines the dimension of $\psi$: with a$>$1 we obtain a dilation and a$<$1 corresponds to a contraction. b is any real number (positives and negatives) and defines the location of the wavelet in time.
\end{enumerate}

In Wavelet Transform, the equivalent to the window function in WFT is a wavelet function $\psi$.

\paragraph{Wavelet} \hfill

A wavelet, literally \textquotedblleft small wave \textquotedblright is a wave function limited both in space and period: begins at zero, grows and decrease in a limited time period and returns to zero. With these properties, it is a function local in the temporal domain as well as in the frequency domain.

To be defined as so, a wavelet is required to satisfy two properties: have zero mean (it must be oscillating) and unitary squared norm \cite{percival-2013}.

\begin{equation}\label{eq:waveletproperties}
\int_{-\infty}^{\infty}\psi(x)dx = 0 \qquad \qquad  \int_{-\infty}^{\infty} |\psi(x)|^2 dx = 1
\end{equation}

It is defined over the space $L^2$ of Lebesgue measurable functions that are both absolutely integrable and square integrable. In this space the two properties can be satisfied.

There are different wavelets that satisfy properties \ref{eq:waveletproperties}, such as Haar (fig \ref{fig:wavelet_haar}), Meyer (fig \ref{fig:wavelet_meyer}) \cite{dabauchies-1992}, Mexican hat (fig \ref{fig:wavelet_mexicanhat}) or Morlet (fig \ref{fig:wavelet_morlet}) \footnote{https://it.mathworks.com/help/wavelet/gs/introduction-to-the-wavelet-families.html}.



\begin{figure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{wavelet/haar.png}
\caption{}
\label{fig:wavelet_haar}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{wavelet/meyer.png}
\caption{}
\label{fig:wavelet_meyer}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{wavelet/mexicanhat.png}
\caption{}
\label{fig:wavelet_mexicanhat}
\end{subfigure}
\caption{Visual comparison between three different types of mother wavelets, Haar wavelet (fig \ref{fig:wavelet_haar}) is a simple square wave function, mexican hat (fig \ref{fig:wavelet_mexicanhat}) is a wavelet belonging to gaussian function family and represents the negative normalized second derivative of a gaussian function; and Meyer wavelet (fig \ref{fig:wavelet_meyer}) a wavelet with applications in fields like adaptive filters}
\label{fig:different_wavelets}
\end{figure}



What we are going to use in our analysis is Morlet wavelet, defined by equation \ref{eq:morletwavelet} and shown in figure \ref{fig:wavelet_morlet}.
For each wavelet, there's a trade-off between time and frequency resolution: compressing the wavelet in a shorten time domain improves time resolution but at the cost of frequency resolution and vice-versa.
With this wavelet the trade-off between time and frequency resolution can be controlled by the choice of $\omega_0$.\cite{muller-2004}.
We choose to run our analysis with a value of $\omega_0 = 6$, since it provides a good balance between the two resolutions \cite{grinsted-2004} and besides, this value gives the best ratio between Fourier Period and the scale parameter a during a Transform, equal to $\lambda = 1.03$.
In this way the results in frequency domain are more interpretable since the scale parameters a used for the Transform is almost equal to the Fourier period.

%omega 0 = 6: \footnote{https://it.mathworks.com/matlabcentral/answers/824015-understanding-cwt-morlet-time-and-frequency-resolution?s_tid=srchtitle}

\begin{equation}\label{eq:morletwavelet}
\psi\left(t\right) = \pi^{-1/4}e^{iw_0 t}e^{-t^2/2}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=.4\linewidth]{wavelet/morlet}
\caption{Morlet wavelet}
\label{fig:wavelet_morlet}
\end{figure}

Two main type of analysis can be performed using wavelets: continuous wavelet transform (CWT) and discrete wavelet transform (DWT) and the CWT is what we employed for our analysis.

Continuous wavelet transform decomposes a time series in time-frequency domain by successively convolving the timeseries with several scaled and translated version of the mother wavelet $\psi$: $\psi_{a, b}\left(t\right) = \frac{1}{\sqrt{a}}\psi(\frac{t-b}{a})$.
If the timeseries is described by a function f, assumed to be real in the equation below, the convolution of f and $\psi_{a, b}\left(t\right)$ is

\begin{equation}
W_{a,b}(f) = \int_{-\infty}^{+\infty}  f(t) \cdot \psi_{a, b}^\ast \left(t\right)   dt
\end{equation}
Where the $\ast$ indicates the complex conjugate.
This integral is performed for different values of a and b.
It can be useful to visualize this integral in a dynamic way: set a value for $a$, a wavelet centered in $b$, is slided across the signal by changing the value of $b$ and for each of these values, a coefficient is extracted by integrating the product between the wavelet and the signal; in this way, coefficients are function of frequency (or scale) and time. This operation is repeated for different values of a and b, and allows us to assemble a matrix called \emph{scalogram}, which is basically a plot of these coefficients in time-frequency domain.

Formally, a wavelet transform, can be described as a mapping from $L^2(\Re) \rightarrow L^2(\Re^2)$.

To computationally implement CWT, a discretized version was implemented on MATLAB tools Wavelet toolbox, but it should not be confused with the Discrete Wavelet Transform \footnote{https://it.mathworks.com/help/wavelet/gs/continuous-and-discrete-wavelet-transforms.html}, a similar type of analysis but with some important differences.
\textcolor{red}{Metto questa parte che parla delle differenze in un box? per separarlo dal resto del testo. }
The difference between the continuous wavelet transform implemented on MATLAB and discrete wavelet transform lies in how finely stretching and shifting parameters are sampled: the CWT discretizes scale more finely than the discrete wavelet transform.

In the CWT, parameters are discretized based on a fractional power of two, by setting a = $2^{j/\nu}$  (\footnote{This way, the discretized wavelet can be written as $\frac{1}{2^{j/\nu}} \psi\left ( \frac{t-b}{2^{j/\nu}}\right )$ }) \cite{liu-1994} \cite{dabauchies-1992} where $\nu $, j are integers.
The parameter $\nu$ is often referred to as the number of “voices per octave” because increasing the scale by an octave (namely to double the frequency) requires $\nu$ intermediate steps, for example from $f = f_0 2^{\nu/\nu} \ to \ f = f_0 2^{2\nu/\nu}$ , $\nu$ steps are required.
The larger the value of $\nu$, the finer the discretization of the scale parameter.
In the DWT, the scale parameter is always discretized to integer powers of 2, $2^j$, j=1,2,3,..., so that the number of voices per octave is always 1.
Since it employs a more rough discretization, DTW is usually used for denoising and compression of signals and images.

In our analysis we are going to use DTW implemented in MATLAB with the default parameters of 12 voices per octave.

%Using CWT is possible to obtain signal information on a matrix: a time-frequency plot also called scalogram by convolving the signal, using different wavelets parameters a and b from a grid

One problem that arises from working on a timeseries, which is basically a signal with a finite support, is that when a wavelet is located at the beginning or at the end of the signal, the wavelet extends itself outside the boundary of the signal, and the convolution would require nonexistent values beyond the boundary.

To overcome this problem it would be possible to accept this data information loss and truncate the values beyond boundaries; whereas an other approach could be to artificially extend data using methods such as zero padding which assumes that the signal is zero outside its original support, or symmetrization which extend the signal symmetrically outside the boundaries or smooth padding which recovers a signal by extrapolating values from the first derivative values or from the signal itself.
Symmetrization method is the one employed for the subsequent analysis.
The confidence value obtained on the boundaries, though, is lower than other obtained for central values of time location.
Areas of the scalogram affected by these edge effects are indicated as \textquotedblleft outside the Cone of influence (COI) \textquotedblright.
\textcolor{ForestGreen}{
Figure \ref{fig:cwt_timeseries} shows the timeseries extracted from the right angular gyrus of patient 51056 from ABIDE I, and its corresponding Continuous Wavelet Transform. In CWT scalogram, the COI is marked with a dotted line.
}
%x axis corresponds to time points and on the y axis frequencies or periods (derived from the parameter a) are represented. Just as an example, we choose to display the y axis as periods.
%The COI is shown as a dotted white line and values outside the COI, where edge effect becomes effective are shown with a lighter faded.
%The color is a visual representation of the magnitude of each wavelet coefficient.

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/timeseries.png}
\caption{}
\label{fig:timeseries_cwt}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/cwt_timeseries.png}
\caption{}
\label{fig:cwt}
\end{subfigure}
\caption{Timeseries (fig \ref{fig:timeseries_cwt}) of ROI 2201: right angular gyrus of patient 51056 from ABIDE I, and its corresponding Continuous Wavelet Transform  (fig \ref{fig:cwt}).
x axis corresponds to time points and y axis the periods derived from the scale parameter $a$ of the wavelet convolving function.
The COI is shown as a dotted white line and values outside the COI, are shown with a lighter faded.
The color represents the magnitude of each wavelet coefficient.}
\label{fig:cwt_timeseries}
\end{figure}



\paragraph{Wavelet coherence} \hfill \newline
\textcolor{ForestGreen}{
Even though what we discussed so far only concerns the analysis of a single timeseries, with wavelet analysis is possible to compare time-frequency information of two signals by performing an analysis called wavelet coherence.
From two timeseries it is possible to compute the Cross Wavelet Transform (XWT) which allows to examine the cross-wavelet power and relative phases.
Using XWT is then possible to compute the Wavelet Coherence.
From this analysis we are finally able to extract a correlation coefficient, indicative of how much two signals are correlated or anti-correlated.
%Continuous wavelet transform can only be used to analyze one signal at a time; if our goal is to analyze and compare two signals using wavelet transform, the analysis to perform is called wavelet coherence.
}

Denoting as $W^{X}(a, b)$ the continuous wavelet transform of a signal $X$, the \emph{wavelet power spectrum} of a single signal X(n) is defined as:
\begin{equation}
W^{XX}(a, b) = W^X(a, b) \left[W^X(a, b)\right]^{\ast}
\end{equation}
where the $^{\ast}$ represents the conjugate transpose.
\textcolor{ForestGreen}{Similarly, the \textbf{Cross Wavelet Transform (XWT)}, sometimes also called \emph{cross wavelet spectrum}, of two time series X and Y is defined as}
\begin{equation}\label{eq:cross_wavelet_spectrum}
W^{XY}(a, b) = W^X(a, b)[W^Y(a, b)]^\ast
\end{equation}
whose module $|W^{XY}(a, b)|$ represents the amount of joint power between the two time series, and it is called \emph{cross wavelet power}.
From the cross wavelet spectrum (eq \ref{eq:cross_wavelet_spectrum}), is possible to compute the the complex argument
\begin{equation}\label{eq:wavelet_phase}
\Delta \phi (a, b) = arctan\left(\frac{Im\left[ W^{XY}(a, b)\right]}{Re\left[ W^{XY}(a, b)\right]} \right)
\end{equation}
which represents the relative phase between X and Y, for each given value of the parameters $a$ and $b$, and is defined over the interval $[-\pi, \pi]$.

The wavelet coherence $R^2(a, b)$ is finally defined as reported in equation \ref{eq:wcoherence}. This coefficient ranges in the interval (0, 1) and represents the localized correlation coefficient between X and Y in the time-frequency domain.
\begin{equation}\label{eq:wcoherence}
R^2(a, b) = \frac{|S(W^{XY}(a,b))|^2}{S(|W^X(a,b)|^2) S(|W^Y(a,b)|^2)}
\end{equation}

In equation \ref{eq:wcoherence} S is a smoothing operator, both in frequency and time, defined as \cite{torrence-1999} \cite{grinsted-2004}
\[
S = S_{scale}S_{time}(W) \quad S_{time} = W\cdot c_1 ^{\frac{-t^2}{2a^2}} \quad S_{scale}(W) = W\cdot c_2 \Pi(0.6)
\]
where $c_1, c_2$ are normalization constants, $\Pi$ is a boxcar (rectangle) function and 0.6 is an empirically determined factor for Morlet Wavelet \cite{torrence-1998}.

\textcolor{ForestGreen}{
An example of wavelet coherence scalogram is shown in figure \ref{fig:wcoherence}.
It shows two timeseries (fig \ref{fig:wcoherence_timeseries}) extracted from patient 51056, and the respective wavelet coherence scalogram (fig \ref{fig:wcoherence_scalogram}).
The color represents the magnitude of the cross-power, and phase information is represented as oriented arrows, pointing towards an imaginary 360 degrees circle, where the zero phase shift is represented by an arrow pointing right and a 180 degrees phase, by an arrow pointing left.
}


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/timeseries_for_wcoh.png}
\caption{}
\label{fig:wcoherence_timeseries}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/wcoherence.png}
\caption{}
\label{fig:wcoherence_scalogram}
\end{subfigure}
\caption{Plot of two timeseries from patient 51056 in figure \ref{fig:wcoherence_timeseries} and their relative wavelet coherence scalogram.
timeseries are extracted from ROI 57 corresponding to right angular gyrus and ROI 65 right lateral occipital cortex.
On the x axis the timepoints and on the y axis the frequencies of this time-frequency decomposition are reported respectively.
The color represents the magnitude of the cross-power coefficients and the relative phase shift is represented as arrows pointing to the right is phase shift is zero ad to the left is is 180 degrees. Starting from 0 degrees arrows rotate counterclockwise.
}
\label{fig:wcoherence}
\end{figure}

Our goal is to extract from this wavelet coherence matrix, a single value, interpretable as a correlation coefficient, just like we had with Pearson coefficients.
We can accomplish this, by extracting two types of information from this scalogram: the magnitude of the correlation and relative phase between the two signals.

We can extract the magnitude information of each entry of the scalogram, but before doing so, we need to assess the level of significance of this coherence matrix over the noise, this way we can estimate the statistical significance level of our values, and only collect the significant ones.

The theoretical procedure \cite{grinsted-2004} \cite{bernas-2018} \cite{hartmann-2014} is to generate, using Monte Carlo methods, a large (1000) samples of wavelet coherence matrices using red noise timeseries.
These red noise samples should be generated, for each time series, with the same 1-lag autoregressive coefficients (AR1 coefficient) of the two timeseries under examination.
Then, for each pair of red-noise timeseries, we should compute the wavelet coherence matrix.

However, since AR1 coefficients have little impact on the significance level \cite{grinsted-2004}, we choose to generate a single sample of 1000 pairs of red noise and for each pair we computed the wavelet coherence matrix.
For each pair of noise, the corresponding wavelet coherence matrix from equation \ref{eq:wavelet_phase} is stacked to obtain a $A\times B \times x \times 1000$ 3D noise matrix to extract the distribution of the entries.
\textcolor{ForestGreen}{
For each entry of a row matrix $A \times B$ we obtain a distribution of 1000 coefficients given by the stack of all the noise matrices.
This null distribution of wavelet coherence coefficient is used to estimate the threshold to 5\% significance level for the subsequent analysis.
We refer to the value corresponding to this $95-th$ percentile as $a_{95}$.
Collecting the value of the $95-th$ percentile for the distribution of each entry, we obtain the matrix visualized in figure \ref{fig:a95_matrix}
}


\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{wavelet/a95_matrix}
\caption{\textcolor{ForestGreen}{Noise matrix obtained by collecting the $95-th$ percentile of the distribution of each entry of the 1000 noise matrices stack. It determines the threshold to assess significance of the coefficients of the wavelet coherence matrix computed for two timeseries}}
\label{fig:a95_matrix}
\end{figure}




Once assessed the threshold for the significance level, the second step is to extract the relative phase information of each entry of the wavelet coherence matrix.
At last, combining together information on significance level and on relative phase, we can estimate the time of in-phase (or out of phase) coherence which can be seen as the percentage of time synchronicity (or anti-synchronicity) between the two timeseries. \cite{bernas-2018}
This time of in-phase coefficient is defined as:

\begin{equation} \label{eq:wavelet_inphase}
c_{ij} = \frac{100}{N}\sum_{a, b}^N I\left\{ R_{ij}^2(a,b) > a_{95}\right\}\cdot I\left\{-\frac{\pi}{4}<arg(W^{XY}(a, b))_{ij} < \frac{\pi}{4}  \right\}
\end{equation}
where the indices i and j refer to the two timeseries i and j; N is the total number of points inside the cone of influence (COI).
$I\left\{ ...\right\}$ is either 0 or 1 depending on whether the condition inside is satisfied;
$a_{95}$ is the threshold value above which the computed wavelet coherence coefficient is regarded as significative.

The time of counter-phase coefficients have a similar definition. They can be obtained as:

\begin{equation} \label{eq:wavelet_counterphase}
c_{ij} = \frac{100}{N}\sum_{a, b}^N I\left\{ R_{ij}^2(a,b) > a_{95}\right\} \cdot I\{arg(W^{XY}(a, b))_{ij} < -\frac{3\pi}{4} \ \lor \ arg (W^{XY}(a, b))_{ij} >\frac{3\pi}{4} \}
\end{equation}

which is basically the same relation as \ref{eq:wavelet_inphase} with a modification of the phase condition

\[
I\left\{-\frac{\pi}{4}<arg(W^{XY}(a, b))_{ij} < \frac{\pi}{4}  \right\} \ \longrightarrow \ I\{arg(W^{XY}(a, b))_{ij} < -\frac{3\pi}{4} \ \lor \ arg (W^{XY}(a, b))_{ij} >\frac{3\pi}{4} \}
\]

\hfill

\noindent In short with this analysis, we are extracting coefficients just considering values from the wavelet coherence scalogram with an high significance level (above 95\%) and with a small $\in [-\pi/4, \pi/4]$, or big phase shift ($< -\frac{3\pi}{4}  \lor  >\frac{3 \pi}{4}$).

Wavelet coherence maps were calculated using MATLAB's wavelet Toolbox, which employs the Morlet wavelet, and decomposes the frequency range using 12 subscales per octave and 9 octave.

%Figure \ref{fig:wcoherence} represents the wavelet coherence scalogram obtained from two ROIs of patient 51056.

From equation \ref{eq:wavelet_inphase} both coefficients in-phase and anti-phase matrix coefficients were computed.
The correlation coefficients matrix for each subject, was then used as input to the neural networks.
An example of correlation matrix is created with in-phase and out-of-phase coefficients from data of patient 51056 is shown in figure \ref{fig:win_wout}



\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{wavelet/win_corrmatrix.png}
\caption{Coefficients of In-phase percentage}
 
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{wavelet/wout_corrmatrix.png}
\caption{Coefficients of Off-phase percentage}
 
\end{subfigure}
\caption{Correlation matrix created with wavelet coefficients of in-phase and off-phase percentage. Patient 51056, ABIDE I }
\label{fig:win_wout}
\end{figure}



\newpage


\chapter{Multicenter data harmonization}\label{chap:harmonization_theory}
Since now, a big percentage of neuroimaging studies have been carried out with limited data acquired from a single center, to minimize varibility in data due to the different instrumentations employed.
In recent years, however, there is the tendency to create shared datasets, by pooling together data acquired from different centers.
A positive side of creating a large dataset by putting together data from multiple centers is the possibility to work on dataset of bigger dimension, resulting in statistically more accurate analysis.
% and facilitates the generalization and the robustness of the model.
\textcolor{ForestGreen}{
Yet, this process has some downsides since it introduces variability between data.
Data acquired from different sources are affected by the so called \emph{scanner effects}, related to differences in the acquisition system such as the scanner model or the acquisition parameters.
%As a drowback, however, it introduces variability in our analysis such as differences due to scanner models, acquisition parameters, generally known as scanner effects.
When analyzing data either with statistical analysis or with machine learning methods, a level out procedure becomes necessary.
This procedure is generally referred as \emph{harmonization}.
When working with machine learning models, for example, harmonization can be helpful to avoid the model to learn patterns related to inhomogeneity of data because of scanner effects, and improve its ability to learn pattern related to the classification task it is conducting.
}
%A to avoid the model to learn these differences deriving from
Harmonization was tested on different datasets and has proven to be an effective way to reduce scanner effects in different kind of datasets such as genes microarray data, diffusion tensor imaging or structural MRI \cite{johnson-2006} \cite{fortin-2017} \cite{lombardi2020}.

\section{ComBat harmonization and NeuroHarmonize}\label{sec:harmonizationtheory}


The state-of-art procedure used in genomic is called ComBat (named after Combating Batch effects), used for structural MRI but applicable to any kind of imaging data, is used to mitigate scanner effects.
It is based on a previous method initially proposed in 2007 \cite{johnson-2006}, for gene expression studies to compute batch effect corrections, later implemented by Fortin et al \cite{fortin-2018} for the harmonization of cortical MRI volumes, and finally in its current improved version, developed by Pomponio et al. \cite{pomponio-2019} in 2019 and available as a free Python package called Neuroharmonize \footnote{https://github.com/rpomponio/neuroHarmonize}.
In this paragraph we describe how ComBat technique works and after that, we discuss how it was modified and improved in the implementation made by Pomponio et al.

ComBat technique belongs to the family of Location and Scale adjustment methods, but it represents an improved version of them since it models site-specific scaling factors and uses empirical Bayes estimate to improve the estimation of the site-related parameters for small-sized datasets \cite{fortin-2018}.
ComBat aims to reduce inter-site variance while preserving biological variability such as differences due to sex, age, FIQ (Full intellective quotient), ICV (Intra Cranical Volume) and so on.
The model assumes that a feature can be modeled as a linear combination of the biological variables plus the site effect, and the errors introduced with site effect can be modeled as both a multiplicative and an additive term. This error can be standardized by adjusting the mean and the variance across the batches.

\textcolor{ForestGreen}{
To clarify how ComBat harmonization works, we can suppose to work with a dataset comprising different data, where each data consists on F features.
This dataset is made of data acquired with K different scanners, where each scanner is used to obtain data from more than one subjects.
We denote with $y_{ijf}$ be the numeric value of the feature $f$ for the patient $i$ acquired with the scan (or equivalently, from the site) $j$
so that $i = 1, 2, ..., K$ indexes the scanner, $j = 1, 2, ..., N_i$ indexes the subject acquired with scanner $i$, for a total of $N_i$ subjects acquired for that scanner. $f$ ranges from 1 to $F$ being F the total number of features.
As a premise, ComBat assumes that the value of each feature: $y$, can be written as depending on different parameters
}
\begin{equation}
y_{ijf} = \alpha_f + x_{ij}^T \beta _f + \gamma_{if} + \delta_{if} \epsilon_{ijf}
\end{equation}
where:
\begin{itemize}
\item $\alpha_f$ is the mean value for the feature f,
\item $x_{ij}$ is the entry of the matrix X created with the covariates of interest such as age, sex,
\item $\beta_f$ is the vector of regression coefficients corresponding to $X$ for the feature $f$,
\item $\gamma_{if}$ and $\delta_{if}$ represent the additive and multiplicative terms for site-$i$ effects related to feature $f$ respectively,
\item $\epsilon_{ijf}$ is a residual term which is assumed to follow a normal distribution with zero mean and variance $\sigma_f^2$.
\end{itemize}

The final location-and-scale-adjusted data $y^{\ast}_{ijf}$ are given by
\begin{equation}\label{eq:harmonized_data}
y^{\ast}_{ijf} = \frac{y_{ijf} - \hat \alpha_f - X\cdot \hat \beta_f - \hat \gamma_{if}}{\hat \delta_{if}} + \hat \alpha_f + X \cdot \hat \beta_{f}
\end{equation}
where $\hat \alpha_f , \hat \beta_f , \hat \gamma_{if} ,  \hat \delta_{if}$ are estimators of the corresponding parameters, based on the model.

%Specifically it is assumed that that the site-effect parameters follow the normal distribution and the inverse gamma distribution respectively
%\begin{equation}
%\gamma_{if} \sim N(Y_i, \tau^2_i) \qquad \delta^2_{if} \sim \frac{\beta^{\alpha}}{\Gamma (\alpha)}(1/x)^{\alpha +1}\cdot e^{-b/x}
%\end{equation}

The process that ComBat employs to estimate feature-dependent parameters, and to adjust data for batch effect can be summarized in 3 steps, at the end of which we obtain the results shown in equation \ref{eq:harmonized_data}

\begin{enumerate}
\item \textbf{Standardization}: Data are standardized feature-wise, so that every feature has similar overall mean and variance.
least square regression, is then performed to determine parameters $\hat \alpha_f, \hat \beta_f, \hat \gamma_{jf}$ and subsequently, $\hat \sigma^2_f = \frac{1}{n} \sum_{it} \left ( y_{ijf} - \hat \alpha_f - X \hat \beta_f - \hat \gamma_if \right )^2$ being n the total number of patients.


The standardized data are then calculated by equation \ref{eq:harmonization_std_data}

\begin{equation}\label{eq:harmonization_std_data}
Z_{ijf} = \frac{y_{ijf}-\hat \alpha_f - X \hat \beta_f}{\hat \sigma_f}
\end{equation}


\item \textbf{Empirical Batch parameters estimate}:
%Site effect parameters estimate using parametric empirical priors:
We assume that standardized data follow a normal distribution $Z_{ijf} \sim N(\gamma_{if}, \delta^2_{if})$ and we seek for a proper estimation of parameters $\gamma_{if}, \delta^2_{if}$.
\textcolor{ForestGreen}{
One of the disadvantages of using a simple location and scale batch adjustment is that it requires a large batch size for the implementation because is not robust to outliers in small sample sizes.
ComBat uses empirical Bayes estimates to provide a more robust adjustment for the parameters $\hat \gamma_{if} \ \hat \delta^2_{if}$, making the model able to deal with small-sized dataset as well.
}
It is also assumed that these site-effect parameters follow the normal distribution and the inverse gamma distribution respectively
\[
\gamma_{if} \sim N(\eta_i, \tau^2_i)  \qquad \delta^2_{if} \sim \text{Inverse Gamma}(\lambda_i, \theta_i) =  \frac{\theta_i^{\lambda_i}}{\Gamma (\lambda_i)}(1/x)^{\lambda_i +1}\cdot e^{-\theta_i/x}
\]

And these hyperparameters $\eta_i, \tau^2_i, \lambda_i, \theta_i$ are empirically estimated from standardized data  $Z_{ijf}$ by using the method of moments.
%Breve box to exmplain method of moments?? No.
We then obtain improved estimation of parameters $\gamma^\ast_{if} \ \text{and} \ \delta^\ast_{if}$ and we use them for the third and last step, where we adjust our data using them.

\item \textbf{Adjust the data}: After the site-effect parameters have been calculated, we are finally able to adjust our initial data using the relation

\begin{equation}
y^{ComBat}_{ijf} = \frac{\hat \sigma_f}{\delta^\ast_{jf}}(Z_{ijf} - \gamma^\ast_{jf}) + \hat \alpha_f + X \hat \beta_f
\end{equation}
which is just an equivalent way to write equation \ref{eq:harmonized_data}, using parameters estimated in the previous passage.
\end{enumerate}

\textcolor{ForestGreen}{
This is the main idea behind ComBat technique, but, as we said before, the state-of-art implementation of this technique by Pomponio et al. \cite{pomponio-2019} is an improved version of it.
}
It differ in the modeling of the biologically covariates, which in the formulas above are expressed by the terms $\hat \alpha_f + X \hat \beta_f$ which is a simple linear model.
Pomponio substituted this linear model with a Generalized Additive Model (GAM). In this model covariates such as sex, age, FIQ, are represented by terms $x_{ij}, \ z_{ij}, \ w_{ij}$ which allow a better parametric modeling to deal with non-linear trends such as the trend of cortical thickness in relation to age.
This way, the terms $\hat \alpha_f + X \hat \beta_f$ in equation \ref{eq:harmonized_data} are substituted by a linear combination of function F of these covariates
\begin{equation}
\hat \alpha_f + X \hat \beta_f \ \longrightarrow \ F_f(x_{ij}, z_{ij}, w_{if}...) = a_f + g_f(x_{ij}) + h_f(z_{ij}) + p_f(w_{ij}) + ...
\end{equation}
\textcolor{ForestGreen}{
Where functions $g_f, h_f, p_f$ can be either linear or non-linear functions of our covariates, according to how we want to model these covariates.
In the implementation of Neuroharmonize, if we specify a covariate as non-linear, it will be treated using thin plate regression splines to find the right smoothing function (find the right basis expansion).
This procedure of including non-linear terms though, brings with it the downside of a huge increasing in computational costs.
}




\chapter{Domain-adversarial Neural Networks}\label{chap:domain_adversarial_theory}

In this section we discuss a different and innovative approach to deal with multi-center datasets.
So far we presented one important harmonization procedure that aims to remove site-related features in an analytical way; an other possible approach is to make use of a deep neural network specially designed to perform a classification unbiased by site-related features.
The construction of this network gets its main idea on the network proposed by Ganin, Ustinova et al \cite{ganin2016}.
\textcolor{ForestGreen}{
Their work addresses the issue of working with two different datasets.
They call them \emph{source} and \emph{target} dataset, where each dataset contains data following a distribution which is different between the two datasets.
Data are in a total amount of $N$, of which $n$ samples belong to the source dataset and $N-n$ samples to the target.
Both source and target data belong to different classes and the task of their work is perform a classification over these classes.
However, all the data belonging to the source dataset are associated to a label specifying the respective class, while data on the target dataset are unlabeled.
%Moreover, source data are associated to a label while target data are unlabeled.
Their goal is to train a network using both the source and the target dataset.
The network must be able to learn to learn distinctive patterns between classes, from data belonging to the source dataset.
To this end they can use only data from the source dataset because it is the only one labeled.
At the same time, the network is trained to learn distinctive characteristics between data from the source and the target domain.
Combining these two information (of classes learned from the source domain and of source/domain differences), they manage to create a network able to learn important features for classification as well as to generalize this information form one domain to another.
%Their goal is to create a network able to learn relevant features for the classification task, from the source dataset (since is the only one labeled). and learn as well some distinctive feature related to the two domains. This result is achivable by constructing a model able to generalize well from one domain to another.
}
%the inner representation of the network contains no discriminative information about the origin of the input.

We illustrate the main aspects of this problem using a shallow neural network with a single hidden layer consisting of D nodes.
We suppose that the network takes as input an m-dimensional features vector.
The hidden layer can be represented as a function $G_f : \Re^m \rightarrow \Re^D$ with a weight parameters matrix $\mathbf{W}$ and bias vector $b$. For brevity's sake we can denote the parameters $\mathbf{W}$ and $b$ as $\theta_f$.
Given an input vector $x \in \Re^m$ the hidden layer acts like
\begin{equation}
G_f(x; \mathbf{W}, \mathbf{b}) =G_f(x; \theta_f)  = f(\mathbf{W}\cdot x + b)
\end{equation}
%where $\textbf{W}$ and $\textbf{b}$ are the matrix and vector parameters to be optimized.
Where $f$ is some activation function that we can represent as a sigmoid function.

Similarly the output will be a function $G_y:R^D \rightarrow \Re^Y$ where Y is the total number of classes of our data. This layer can be written as
\begin{equation}
G_y(G_f(x; \theta_f); V, c) = f'(V\cdot G_f(x) + c)
\end{equation}
Here, too we can denote parameters $V$ and $c$ with a single notation $\theta_y$.

A usual train carried on only on the (labeled) source domain, will therefore bring to the minimization of the loss function associated with the output, dependent on parameters $\theta_f, \theta_y$
\begin{equation}
\underset{\theta_f, \theta_y}{min} [ \frac{1}{n}\sum_{i = 1}^n L_y^i(Gy(G_f(x_i; \theta_f); \theta_y), y_i )]
\end{equation}

Training procedure and minimization of this loss function can be performed only on the source dataset consisting on n samples, since target data are unlabeled.
At this point, to tackle the problem of domain independence the idea introduced by Ganin et al. is to consider the hidden layer as an internal representation of data, and use its information to create a domain regressor.

A domain regressor can be implemented as a layer $G_d$, completely similar to the output layer $G_y$, depending on parameters $U, d$ which we will denote as $\theta_d$. It takes as input the hidden layer $G_f$, and after being activated by a sigmoid function, returns an output.
We indicate the loss function associated to this output as $L_d$, and the loss previously introducted for label classification as $L_y$.

The complete optimization function can be now written as
\begin{equation}
E(\theta_f, \theta_y, \theta_d) = \frac{1}{n} \sum_{i = 1}^n L_y^i - \lambda (\frac{1}{n} \sum_{i = 1}^n L_d^i + \frac{1}{N-n} \sum_{i = n+1}^N L_d^i)
\end{equation}
\textcolor{ForestGreen}{
Following this strategy, the optimization of the function $E(\theta_f, \theta_y, \theta_d)$ involves a minimization with respect to parameters $\theta_f$ and $\theta_y$, and a maximization with respect to $\theta_d$. More precisely they seek for a saddle point given by
}
\begin{equation}
\begin{split}
\hat \theta_f, \hat \theta_y = \underset{\theta_f, \theta_y}{argmin} E(\theta_f, \theta_y, \theta_d) \\
\hat \theta_d = \underset{\theta_d}{argmax} E(\theta_f, \theta_y, \theta_d)
\end{split}
\end{equation}

This task was accomplished by embedding the domain regressor $G_d$ into the neural network consisting on $G_f$ and $G_y$.
The resulting neural network will include two branches: one for label classification and the other with the domain regressor.
They linked these parts using a \emph{gradient reversal layer} and exploited a classic stochastic gradient descent procedure to update weights.
A gradient reversal layer allow to train the network in an adversarial way.
It is placed at the top of the domain regressor branch, as we can see from figure \ref{fig:dann_original} directly taken from their paper.
Let us point out that so far, we discussed the structure of this network by using a shallow neural network consisting just on a single hidden layer, but this architecture can be generalised by adding additional layers for each branch of the network, as shown in figure \ref{fig:dann_original}.


\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{ml/models/dann}
\caption{A schematic representation of a Domain Adversarial Neural Network, from the presentation article \cite{ganin2016}. The network mainly consists of three parts: a feature extractor, which takes inputs and creates an internal representation of data. From it the network is splitted into two branches: a label classificator $G_y$ with associated loss $L_y$ and a domain regressor/classifier denoted as $G_d$ with associated loss $L_d$. This latter branch is linked to the feature extractor branch by making use of the gradient reversal layer.
}\label{fig:dann_original}
\end{figure}


\textcolor{ForestGreen}{
During the training, of this network, the forward propagation is not influenced by this gradient reversal layer, however, during backpropagation, this layer acts by multiplying the gradient by -1 before passing it to the preceding layer.
In this way, the partial derivatives of the domain loss $L_d$ with respect to the parameters $\theta_f$ get a minus sign.
The whole updating procedure for parameters $\theta_f, \theta_y, \theta_d$ can then be described by equations \ref{eq:adversarial_updating_parameters}.
}
\begin{equation}\label{eq:adversarial_updating_parameters}
\begin{split}
\theta_f \rightarrow \theta_f - \frac{\partial L_y}{\partial \theta_f} +  \lambda \frac{\partial L_d}{\partial \theta_f}\\
\theta_y \rightarrow \theta_y -  \frac{\partial L_y}{\partial \theta_y} \\
\theta_d \rightarrow \theta_d - \lambda \frac{\partial L_d}{\partial \theta_d}
\end{split}
\end{equation}

During training, classification branch and domain regressor compete against each other in an adversarial way for the optimization of the parameters. For this reason this network is called Domain-Adversarial Neural Network (DANN).



\chapter{Machine learning model explanation with SHAP}\label{chap:shap}


\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{shap/shap_presentation}
\caption{\textcolor{ForestGreen}{Graphic representation of how a deep model works, it typically acts as a black box, taking some data as input, such as age, sex, blood pressure, and uses them to produce an output which differs form a reference value. With SHAP it is possible to explain the contribution of each input and determine which one influenced the output the most. Credits: https://shap.readthedocs.io}}
\label{fig:shap_waterfall_example}
\end{figure}

SHAP (SHapley Additive exPlanation) is a technique based on game theory that provides a method for machine learning model explanability.
It is a powerful tool to get rid of the \say{black box} idea of a machine learning model and try to understand its deep mechanism and the reasons why a model returns a certain output, related to an input sample like a vector of features or an image.
As it is shown in figure \ref{fig:shap_waterfall_example}, just as a visual example took from the SHAP documentation \footnote{https://shap.readthedocs.io/en/latest/}, a common machine learning model acts like a black box that returns an output value after some non-linear unknown calculations over some input values. With an explanatory model, we are able to quantify the contribution of each feature of our input data and assess what and why are the most important features and how much they contributed to the final outcome.

The idea behind SHAP is to use Shapley values (see chapter \ref{chap:shapley_values}) to explain every single feature contribution in our machine learning model, treating the learning process as a cooperative game.
To apply the concept of Shapley values to a machine learning model we just adapt some terms we used for game theory: the payout of a coalition becomes the model prediction and the players are the features in our input data.
For a single feature, its Shapley value is defined as the average marginal contribution of that feature across all possible coalitions.

There are two main classes of explainatory algorithms called local and global methods
%Shap can be regarded as both a local and global explainatory method, and before it, several algorithms belonging to these categories were implemented as an attempt to create an explainatory technique for machine learning models.
In general, the objective of a local explainatory model can be defined as the attempt to explain an output f(x) after a single instance x that in our case can be identified as a vector of features.
Local methods differ from global methods, because the latter provide a global explanation of the model, across all the instances, they are able to attribute an importance to each feature to determine which one contributes the most to the output of the model.

One of the key point of SHAP is its flexibility, being both a local and global explainatory model, this way we are able to explain a single instance as well as an entire dataset and extract the most important features.


\textcolor{ForestGreen}{
As a local model, SHAP shares with other algorithms some common traits.
These algorithms typically employ a simplified function $g$ to explain a model $f$.
Given a single input vector $x \in \Re^F$ they introduce a simplified vector $x'\in \Re^{F'}$ = $\left\{ 0, 1 \right\}^{F'}$. The simplified space $F'$ is not necessarily equal to the input space $F$. A map function $h_x$, unique for each vector $x$, maps the simplified vector $x'$ to the original vector $x$
$x'$ is a binary vector consisting only on 0 and 1, if an entry is zero, it means that the corresponding features of $x$ will be withheld from the subsequent analysis, while 1 means that they are included.
They seek to find the best approximation of $g$ in the locality of $x'$. In order to do this, they create a perturbed dataset consisting on different vectors $z'$ obtained by random sampling from the nonzero elements of $x'$ \cite{ribeiro-2016}.
Local methods try to obtain a model $g$ that faithfully approximate $f$, and that, if  $g(x') \approx f(x) \text{if} x = h_x(x')$
This simplified model $g'$ should also be a linear combination of binary variables of $x'$ \cite{lundberg-2017}:
}
\begin{equation}
\label{eq:shap_g}
g(x') = \phi_0 + \sum_{i = 0}^{F'} \phi_i x_i'
\end{equation}
Several methods before SHAP were created that satisfy this condition such as LIME \cite{ribeiro-2016} or DeepLIFT \cite{shrikumar-2017} but they lacked additional properties that SHAP has:

\begin{enumerate}
\item \textbf{Local accuracy}:
\begin{equation}
g(x') =\phi_0 + \sum_i^F \phi_i x_i' \mathbf{=} f(x) \quad \text{when} \quad x = h_x(x')
\end{equation} and not just an approximation $f(x) \approx g(x')$. Here coefficients $\phi_i$ represent the impact of the associated feature to the model's output, and $\phi_0$ represents the coefficient corresponding to a vector x' with all entries equal to zeros.
\item \textbf{Missingness}:
\begin{equation}
\text{if} \ x_i' = 0 \Longrightarrow \ \text{then} \ \phi_i = 0
\end{equation}
Practically if a feature is equal to zero, it has no attributed impact on the model outcome
\item \textbf{Consistency}: if we have two different models $g'$ and g. Denoting with $z' / i$ the setting where $z_i' = 0$
\begin{equation}
\text{if} \ g'(z') - g'(z' / i) \ge g(z') - g(z' / i) \forall z' \in \{0, 1\}^F \Longrightarrow \ \text{then} \ \phi_i(f', x) \ge \phi_i(f, x)
\end{equation}
This property states that if the contribution of a feature $z'_i$ increases, regardless all the other features in a model, its associated importance value increases or at least remain the same, but does not decrease.
\end{enumerate}
%The most important properties of this models is called local accuracy and states that g and f must give the same result when passing all the features: g has to match f when x = $h_x(x')$.
%We attribute a contribution $\phi_i$ to each simplified vector $z_i'$

In 1975 it was demonstrated \cite{young1985} that the only coefficients satisfying all of the above properties are Shapley values, and, as a consequence, methods which employ different coefficients violate one of these properties, usually local accuracy and/or consistency.
For this reason SHAP employs Shapley values and takes advantage of pre-existing algorithms (some of which were cited before: LIME and DeepLIFT) that used different coefficients.
It readapts and enhances them providing an unified approach to assess feature importance for different models without any violation of the axioms above.

To compute the Shapley value for a feature we have to evaluate the model over all the possible subsets S that we can create with our data S$\subseteq F \setminus \left\{i\right\}$.
This approach, in a problem with an high number of features would result in a huge computational work.
To bypass this problem, we can choose not to calculate the exact Shapley value but just an approximation of it, depending on the model we are working on.
Some algorithms such as TreeSHAP, are able to compute the exact Shapley coefficients.
Others just compute an aproximate Shapley value.
Algorithms we are going to focus on are KernelSHAP which is a model agnostic explanation method, and DeepSHAP which is optimized to work faster on deep models by making use of the knowledge of the structure of a neural network.
Each one of these algorithms is built upon previous algorithms like LIME or DeepLIFT.
In the following section we are going to explain some important features from LIME that were employed in KernelSHAP, and from DeepLIFT employed for DeepSHAP.

\section{LIME and KernelSHAP}\label{sec:lime}\hfill

LIME (Local Interpretable Model-agnostic Explanations) is a local, model-agnostic, interpretability model, made to explain the prediction of any classifier in a faithful, but only local way.
This means that it can accurately explain a single prediction but it is not the most suitable to generalize to many of them \cite{ribeiro-2016}.

Since KernelSHAP is built upon LIME algorithm, we briefly discuss an important strategy implemented by LIME to avoid the computation of all the permutations.

Starting from a single instance  $x \in \Re^n$, LIME creates a perturbed dataset $X$ and evaluates the model over this dataset.
Perturbed dataset is created by making use of several binary vectors containing just zeros and ones.
In fact, it randomly creates different binary vectors of the same lenght of $x$, but randomly set 0 and 1 as its entries.
From this binary dataset it is possible to come back to the space of features value, recalling that \textquotedblleft 1 \textquotedblright means that we are including the corresponding feature from x, while \textquotedblleft 0 \textquotedblright is associated to the missingness of the corresponding feature.
LIME replaces missing features in different ways according to the type of data it is dealing with (images, text data, tabular data).
In the case of tabular data, LIME repleaces missing values by randomly sampling a value from the distribution of that feature from the training data  \cite{ferrando2018}.

KernelSHAP works in a similar way, but introduces a background dataset from which missing values are randomly picked and replaced in the perturbed vector.

LIME uses the synthetic dataset just created, and the model is evaluated on each vector.
These outputs are subsequently used to accomplish a minimization task to finally find the importance coefficients.
Minimization is performed in a regression, but before it, each output value is weighted according to the distance of the synthetic vector with the original vector x by using coefficients $\pi_x$ defined as
\begin{equation}
\pi_x = \frac{p-1}{{p \choose |z'|} \cdot |z'| (p-|z'|)}
\end{equation}
Where we indicated as p the total number of features in the original feature vector x anc with $|z'|$ the number of element in that subset, and cosequently ($p-|z'|$) is the number of features not included in the subset.

To perform a fit, the loss function to minimize is in the form $L(f, g, \pi_x) = \sum_{z\in Z} [f(h(z))-g(z')]^2\ \pi_x$
where $g(z')$ is the simplified function expressed in equation \ref{eq:shap_g}, given by a combination of coefficients $\phi_i$ that are the parameters of the fit.

LIME is regarded as local method, because it explains the prediction of a black box model by making use of a local model: in this case a regression, which is an interpretable model computed in the neighborhood of the instance we want to explain.
KernelSHAP implementation was strongly influenced by the LIME algorithm but,thanks to the introduction of Shapley values, it also comes with theoretical guarantees about consistency and local accuracy, from game theory.
%In a nutshell, the main steps of KernelSHAP algorithm are
%\begin{enumerate}
%\item Given a feature vector $\mathbf{x} = [x_1, x2, ..., x_p]$, where $x_1, .., x_p$ are the features (numbers), a model f and a background set $X_{bckg} = \{ %\mathbf{x1}, ... , \mathbf{xn}\}$, we want to explain \textbf{x}
%\item replace a subset of entries of x with values from the background dataset:
%\item create n different copies of the original input fector x, all slightly different from each other because of these replacements
%\item Evaluate the model on each copy
%\item Use these outputs to fit simple model and find shapley coefficients
%\end{enumerate}

% shap
%deepLIFT \cite{shrikumar-2017}

As mentioned above, KernelSHAP is a model-agnostic algorithm, in the sense that be easily used with every kind of model, but SHAP includes some model-specifics algorithms that make use of a previous general knowledge of a model structure to optimize the explainer performances and speed up the process. DeepSHAP is one of these model-specific algorithms and it is what we are going to use in our analysis.

\section{DeepLIFT and DeepSHAP}
%deeplift github: \footnote{https://github.com/kundajelab/deeplift}
DeepSHAP alghorithm works with some similar implementation of KernelSHAP, making use of a background dataset to simulate missing values, but it is also optimized to perform better on deep models by taking advantage of the idea behind the DeepLIFT algorithm.
It can be considered as an enhanced version of DeepLIFT, thanks to the introduction of the Shapley values, rather than \textcolor{ForestGreen}{the coefficients employed in DeepLIFT called \emph{DeepLIFT multipliers} \cite{shrikumar-2017}.
}
The strategy implemented by DeepLIFT to explain features is to compute the difference between the output of a deep learning model obtained with the \say{true}  data and a reference value computed as the output of our model with some reference data.
The choice of the reference depends on what kind of data we are working on: images or genomic data, for example.
For genomic the most common way to produce reference output in DeepLIFT is to shuffle some training data, evaluate the model on them and average across all the scores.
With the implementation of DeepSHAP, this reference value is chosen from the backgound dataset and represents an uninformative value for a feature.

As mentioned before, deepLIFT is also shaped to perform on deep learning algorithm computing its feature importance values called DeepLIFT multiplier during the process of backpropagation.
DeepLIFT attributes to each feature $x_i$ a value $C_{\Delta x_i, \Delta y}$, that represents the effect of that input being set to a reference value rather than its actual value.

In a few words the idea behind DeepLIFT (and DeepSHAP) can be summarised as: let t represent the output of some inner neurons and let $x_1, x_2, ... , x_n$ be some preceding neurons necessary to compute t, if we label $t_0$ the reference output, we can compute the value $\Delta t = t-t_0$ and use them to define the DeepLIFT coefficients $C_{\Delta x_i \Delta t}$ associated with $\Delta x_i$ which is the difference between the true feature and the reference value for that feature.
DeepLIFT coefficients are chosen to follow the property called \emph{summation to delta} property
\[
\sum_{i = 1} ^n C_{\Delta x_i \Delta t} = \Delta t
\]

For a given input neuron x with difference from reference $\Delta x$ and a target neuron t, is defined the \emph{DeepLIFT multiplier}
\[
m_{\Delta x \Delta t} = \frac{C_{\Delta x \Delta t}}{\Delta x}
\]
which represents the contribution of $\Delta x$ to $\Delta t$. Once computed the multipliers of each neuron in a layer it is possible to compute the multiplier of any target neuron during the backpropagation.
For a more detailed explanation of how DeepLIFT works we refer to its presentation article \cite{shrikumar-2017}.

DeepSHAP exploits the same principles as DeepLIFT, and combines Shapley values for smaller components of a network to compute values for the whole network. It accomplish this by recursively passing Deep LIFT multipliers now defined in terms of Shapley values during backpropagation.
\textcolor{ForestGreen}{To not weight down this discussion, we explain the main idea of how this procedure works by making use of a simplified neural network.
%we follow the notation on article \cite{chen2019}, simplifying it for a better understanding
We suppose to have a single input vector $x$ consisting only on two features: $x = (x_1, x_2)$ and a single background vector $b = (b_{x_1}, b_{x_2})$ .
}
We denote as $f_{x_i}$ the actual value of the feature $x_i$ and as $b_{x_i}$ the value of the entry from the backgound vector corresponding to $x_i$.
We also indicate as $h_i$ the output of a neuron after the input $x$, and with $b_{h_i}$ the output of the neuron when the input is $b$.
Giving a look at figure \ref{fig:deepshap_linear} we can write formulas for forward propagation.

\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{shap/deepshap_linear}
\caption{Representation of a linear network and important quantities involved during forward propagation.}
\label{fig:deepshap_linear}
\end{figure}


\[
h_1 = w_{1, 1}x_1 + w_{2, 1}x_2 \qquad y = w_1 h_1 + w_2 h_2
\]
Giving a look at the model, we can retrieve the Shapley values for each node by summing all the contribution from each path, where the path contribution to a Shapley value for $x_i$ is defined as the product of the weights along the path and the difference between $x_i$ and the background value $b_{x_i}$.
In practice, if we focus on the colored blue line, we have that the contribution from this path to $\phi(h_1)$ is given by
\[
\phi(h_1) = w_1 (f_{h_1} - b_{h_1})
\]
with this information, we can finally compute the contribution to the Shapley value of $x_1$ coming from this path
\begin{equation}
\phi(x_1) = (f_{x_1} - b_{x_1}) w_{1, 1} w_1 = (f_{x_1} - b_{x_1}) w_{1, 1} \frac{\phi(h_1)}{f_{h_1} - b_{h_1}}
\end{equation}
adding all contribution from all the possible paths.
\textcolor{ForestGreen}{
In figure \ref{fig:deepshap_linear} are represented for a greater clarity only two paths, deriving from the two input features $x_1 text{and} x_2$, however, a real neural network consists of much more input features and consequently many more patterns will contribute.
Considering all the possible patterns, it is possible to compute the final Shapley value for a feature $x_i$ in terms of the attributions of all the intermediary nodes of the network.
}


\section{Shap values as feature importance}

To express the significance of a feature, we can rely on the relation between feature importance in a qualitative way and Shapley value: features with high absolute Shapley value are important.
The absolute importance value for a feature f is calculated as the mean of the magnitude of all the Shapley value for that feature, so the mean is performed across all the n samples we used to calculate these values
\begin{equation}\label{eq:shap_magnitude}
I_f = \frac{1}{n} \sum_{i = 1}^n \|\phi_i^{(f)}\|
\end{equation}

SHAP is implemented as a free Python package with MIT license, developed and maintained by Scott Lundberg \footnote{https://github.com/slundberg/shap}.
This package includes different classes such as KernelExplainer, which is the class name of KernelSHAP mentioned in section \ref{sec:lime}, TreeExplainer tuned to perform rapidly on tree and forest-like models, linearExplainer which deals with linear model and is able to compute the exact shapley values and not just an approximation, and the class we are using: DeepExplainer, suitable to explain deep learning models and to compute an approximate value of Shapley values.


\newpage





\addcontentsline{toc}{chapter}{IMPLEMENTATION \& RESULTS}
\chapter*{IMPLEMENTATION \& RESULTS}
\chapter{Analysis workflow}\label{chap:analysis_workflow}
Up to this point, we have been discussing what type of data we used in our work and how we obtained them.
In this chapter we specify what kind of analysis we carried out using these data.

We mainly performed classification of control subjects vs ASD patients using a deep neural network (DNN).
We compared different harmonization pipelines implemented during the classification process.
%We also analyzed the differences between classification performances of a DNN and a coventional random forest classifier.

To this end, before running any classification procedure, in chapter \ref{chap:harmonization_results} we show some results obtained through the process of harmonization explained in chapter \ref{chap:harmonization_theory}.
%tackled the problem of data harmonization.
%To this end in chapter \ref{chap:harmonization_results} we explain how we implemented the procedure of data harmonization and we show some results obtained by applying it on our data obtained from the analysis discussed in chapter \ref{chap:connectivity_coefficients} which we summarize here for convenience:
%\begin{itemize}
%\item Data obtained by Fisher-transforming Pearson correlation coefficients extracted for each pair of timeseries of each patient
%\item Data obtained from wavelet analysis by computing time of in-phase or out-of-phase percentage coefficients for each pair of timeseries of each patient
%\end{itemize}
We report these results to allow a visual feedback and understand how harmonization modify data and how effective is this procedure in eliminating site-related information from our data.

In chapter \ref{chap:deep_models} we present the structure of the deep neural network employed for classification and how we reached this structures.

In chapter \ref{chap:results_deeplearning} we present results obtained from classification of data using deep neural networks. We follow different classification and harmonization pipelines and we compare the performances between a DNN and a random forest classifier.
We seek to find the best strategy to obtain the best separability between controls and ASD data.

Finally in chapter \ref{chap:shap_results} we apply techniques of explainable AI to extract information about what feature are relevant in the discrimination between controls and ASD.

\hfill
%To this end, classification was carried out following different pipelines: we compared performances between a random forest classifier, a deep neural network and a domain-adversarial neural network. We also discuss the effects of data harmonization during the classification procedure and we discuss the best strategy to implement harmonization.

We start now examining in details what correlation coefficients we employed and the classification and harmonization pipelines we carried out.



\section{Coefficients selection}\label{sec:coefficient_selection}

All the different classification and harmonization pipelines explained in this chapter are carried out using different correlation coefficients, created following the analysis explained in chapter \ref{chap:connectivity_coefficients}, or obtained through a combination of them. 
In details, we used the following coefficients as input to DNNs and ML classifiers.

\begin{itemize}
\item \textbf{Pearson} correlation coefficients, Fisher transformed according to equation \ref{eq:fisher_transform} 
\end{itemize}


\noindent Four different coefficients computed using wavelet analysis:


\begin{itemize}
\item \textbf{w\_in}: in-phase wavelet coefficients, computed using equation \ref{eq:wavelet_inphase}.
%with results reported in table \ref{tab:classification_win}
\item \textbf{w\_out}: counter-phase wavelet coefficients, computed using equation \ref{eq:wavelet_counterphase}
%with results reported in appendix in table \ref{tab:classification_wout}
\item \textbf{w\_stack} = w\_in $\oplus$ w\_out: in-phase and counter-phase coefficient stacked together in a single array of dimension 11990, namely twice as long as the array of w\_in or w\_out singularly taken (with length equal to 5995). 
%Results reported on table \ref{tab:classification_win+wout}
\item \textbf{w\_diff} = w\_in - w\_out: coefficients resulting from the elementwise subtraction of w\_in and w\_out.
This is an attempt to create a single coefficient with similar properties as Pearson correlation coefficients.
w\_diff have range [-1, +1] where w\_diff = -1 is obtained when the two signals are completely anti-correlated which means w\_in = 0 and w\_out = +1 and w\_diff = +1 is obtained with perfect in-phase correlation namely w\_in = 1 and w\_out = 0. 
%reported in appendix, in table \ref{tab:classification_win-wout} 
\end{itemize}


\section{Classification and harmonization pipelines}


%To assess the performances of a classification algorithm, we implemented a k-fold cross validation scheme to report our results.
To assess the performances of a machine learning model we implemented a k-fold cross validation scheme in order to train and test the model using each time a different train and test dataset.
We evaluated and reported model performances as the mean of the results and the standard deviation of the AUC scores for each fold.
Since we are not working with a huge amount of data ($< 1600$ samples), we performed a 5-fold CV.
With a 10 k-fold CV, the reduced number of data in each test set would have lead to an higher variability of the results.
For this reason we preferred to hold a greater number of data in test, at the cost of reducing the number of data available for train.
This choice reduces the variability among them and consequently reduces the error of the model score.
K-fold CV was implemented using the stratifiedKFold class provided by scikit-learn library for Python\footnote{https://scikit-learn.org/stable/}.




\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{ml/flowcharts/flowchart_analysis}
\caption{Flowchart of the harmonization and classification pipelines carried out in this work.
}
\label{fig:workflowanalysis}
\end{figure}



A flowchart of the different harmonization pipelines used to perform classification with a machine learning or deep learning model is shown in figure \ref{fig:workflowanalysis}.
Each block indicates a process or a result.
\begin{enumerate}
\item Coefficient selection: select whether to use Pearson-based correlation coefficients, or wavelet-based correlation coefficients among the ones listed in section \ref{sec:coefficient_selection};
\item Dataset selection: select the desired attributes of patients in order to thin out the dataset. For example we could choose to use the whole ABIDE I + II dataset or just ABIDE I, limit analysis to a certain age range, sex, or limit the eye status at scan to open or closed eyes. In this regard, our choices are explained in section \ref{sec:dataset_selection};
\item Collect the raw data left after the previous steps;
\item Classification of controls/ASD can be fulfilled by a deep neural network, a random forest classifier, or the domain-adversarial neural network;
\item Based on how we implemented harmonization during the classification procedure, we can distinguish four pipelines: 1) with the harmonization of data implemented inside the k-fold CV,  2) with the harmonization implemented before the k-fold CV, 3) without implementing harmonization 4) with harmonization included within the DNN structure and learning process by using the domain-adversarial neural network.
\item For each pipeline, the final classification score is reported as the mean and standard deviation across all the partitions created by the k-fold CV.
\end{enumerate}

In details the four harmonization and classification pipelines we followed are:
\begin{enumerate}
\item Classification of control subjects vs ASD patients with harmonization of data implemented inside the k-fold CV procedure, following the flowchart shown in figure \ref{fig:harmon_kfold_flowchart}. 
For each fold, data are splitted into a train and a test subset. 
The harmonization model is created on control data of the train dataset, and applied to the ASD data of train and to the entire test dataset. 
Harmonized data of control and ASD belonging to train, are merged again to obtain a harmonized train dataset. 
Classification is performed on the new harmonized dataset created in this way. The training of the model is accomplished on the harmonized train dataset and then the model is evaluated on the harmonized test dataset. 
This entire process is repeated for each fold with different train/test partitions;

\item Classification controls/ASD using dataset harmonized before the k-fold CV procedure, created following the pipeline shown in figure \ref{fig:harmon_upstream_flowchart}. We refer to this procedure as upstream harmonization.
From the entire dataset controls data are collected, and the harmonization model is created using their information. 
The harmonization model is then applied to the all the controls and the ASD data, and the harmonized dataset is created. 
Using this dataset, a k-fold CV is performed: the dataset is split into train and test subsets and on them, classification is carried through;
\item Classification controls/ASD using raw (not harmonized) data;
\item Classification controls/ASD using the domain adversarial neural network, giving as input raw data. Using this network, an inner harmonization is implemented through an adversarial learning process.
\end{enumerate}


We believe that implementing the harmonization procedure inside a k-fold CV process, is the best way to keep train and test data apart and avoid data leakage between the two subsets.
Data leakage is in fact a common source of error and may bring to an overestimation of the model performances.
When we harmonize the entire dataset upstream, namely before the splitting into a train and test, the harmonization model executes a fit using all the data, so each data is modified according to information obtained from every other data.
In this way, once the harmonization is done, within the data belonging to the train set there is already information about the data belonging to the test. 
This would likely lead to a misleading increase of model performances.

Furthermore, some articles where harmonization of data is performed, evaluate model performances using a leave-one-site-out Cross Validation \cite{spera-2019} \cite{ingalhalikar-2021}.
In our case this procedure is not feasible, if we want to perform harmonization inside the CV procedure. 
To compute the harmonization model, we need information about control data form each site. 
If a site is only on the test dataset, we would not have information to harmonize its data since we are using a model created with only information obtained from the train dataset. 
%if we compute the harmonization model on the train dataset we would not have information about the site belonging to the test dataset and we could not harmonize those values.


\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{ml/flowcharts/harmon_kfold}
\caption{Flowchart for the implementation of harmonization inside a k-fold CV: }
\label{fig:harmon_kfold_flowchart}
\end{figure}



\begin{figure}[h!]
\centering
\includegraphics[width=0.45\linewidth]{ml/flowcharts/harmon_upstream}
\caption{Flowchart of the upstream implementation of harmonization, before the k-fold split}
\label{fig:harmon_upstream_flowchart}
\end{figure}


\section{Dataset selection} \label{sec:dataset_selection}

With reference to the  \say{Dataset seleciton} in figure \ref{fig:workflowanalysis} we discuss now what selection criteria were adopted for the cohorts of subjects.

First of all, we excluded from the analysis data from sites NYU\_2 and KUL\_3 because as noticed in chapter \ref{chap:dataset} they did not provide control subjects data but only ASD patients, and without control data, we are not able to perform a proper harmonization on these sites.

We choose to run analysis considering only male subjects, aged between 5 and 40 years old.
This allows to reduce variability due to sex

The first collection of data we used to run classification on, is the whole dataset consisting of ABIDE I and II.
With the cuts on covariates cited above, we obtained a dataset consisting on 1470 subjects.
This dataset can be divided to analyze classification performances separately on ABIDE I and ABIDE II, maintaining the same constraints on ages and sex.

This choice was made because some works dealing with controls/ASD classification on the ABIDE dataset, are carried out only on ABIDE I, or, more precisely, on ABIDE I preprocessed (see section \ref{sec:cpac}) dataset \cite{spera-2019}. 
Thus for an immediate comparison we chose to run classification on the two disjoint datasets as well.

To seek for a more homogeneous dataset, we can choose to set constrains on the eye status at scan.
The objective is to select only patients who kept open eyes throughout the entire scan session.
This, as mentioned in chapter \ref{sec:intro_ml} helps removing data potentially altered by sleep.
This constraint was applied on the dataset consisting on ABIDE I + II and separately on ABIDE I and ABIDE II.
In short, considering these constraints, we obtain 6 different datasets, with different number of patients each, summarized in table \ref{tab:controlASD_per_subset}.




\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrr}\toprule
Dataset &Constraints &Tot &Controls &ASD \\\midrule
AB I+II &eye = all, sex = M, age = (5, 40) &1470 &737 &733 \\
AB I& eye = all, sex = M, age = (5, 40)&841 &426 &415 \\
AB II&eye = all, sex = M, age = (5, 40) &629 &311 &318 \\
AB I + II &eye = open, sex = M, age = (5, 40) &1026 &514 &512 \\
AB I& eye = open, sex = M, age = (5, 40) &568 &281 &287 \\
AB II &eye = open, sex = M, age = (5, 40) &458 &233 &225 \\
\bottomrule
\end{tabular}
\caption{Total number of data and control/ASD amount for each subset and thresholds. }
\label{tab:controlASD_per_subset}
\end{table}


\section{Dimensionality reduction of data}

An important issue related to data we are working with concerns their dimensionality.
We are dealing with data lying in an high dimensional space but we lack an apropriate number of data to make acccurate predictions on them: this problem is usually referred as the \say{curse of dimensionality}
%\footnote{https://en.wikipedia.org/wiki/Curse\_of\_dimensionality\#Machine_Learning} .
In our work, dimensionality is a relevant issue since we are dealing with 5995 features and less than 1500 patients and, for this reason, we run our analysis in a condition of permantent overfitting.
One important method to tackle this aspect is to perform a Principal Components Analysis (PCA), as explained in section \ref{sec:pca_theory}, hence reducing the dimensionality of the problem.

%non avrebbe senso calcolare la PCA solo sui controlli perchè pca cerca la direzione di massima varianza, dove i dati variano maggiornente, e quidni dove si ha una migliore distinguibilità bewteen controls and ASDs. NOn avrebbe senso farla solo sui controlli.
%Il numero di PC può essere più grande del test dataset, ma non sarà mai più grande del numero di feature in esso (per come è definito il massimo numero di pc), quindi una volta estratte le PC può essere applicato a qualsiasi test dataset.

When applying PCA to a dataset we seek to explain as much variance as possible, without losing relevant information.
For this reason we start our PCA analysis extracting a number of principal components that explain more than 90$\%$ of variance, then we gradually reduce this number by halving it.

As mentioned in section \ref{sec:pca_theory}, the maximum number of PCs  that is possible to extract is limited by the number of samples in the dataset and by the number of features of each sample.
In our case the number of samples $n\_samples$ is always lower than the number of features $n\_features$, since we have less than 1500 samples and 5995 features for each sample.
Specifically, the number of principal components will be limited by the number of data in the train dataset.
We use just the train dataset to calculate principal components because this is the correct way to proceed to avoid data leakage between train and test dataset.
The search for principal components is then accomplished only on the train dataset and subsequently the same transformation is applied to the test dataset.
In accordance to what we made for harmonization, we calculated the principal components just on the training set, thus keeping train and test set apart.
For this reason we had to implement it inside the k-fold CV procedure.


To avoid repeating all the analysis mentioned earlier with the additional implementation of PCA, we chose to implement it only with in the pipeline that gave the best classification results and using just two dataset: ABIDE I + II and ABIDE I with patients with open eyes.
%For each one of the analysis above, PCA was implemented to restrain the effect of overfitting.
From the initial input data consisting of 5995 features, we performed classification using different values of principal components.
Results are shown in section \ref{sec:results_pca}



\chapter{Harmonization - results}\label{chap:harmonization_results}

% data do not present a clear age-trend  (and FIQ?)
%scrivere che quando faccio l'harmonization non uso nessuna covariata specificandola come non lineare perchè non ci sono ragioni per farlo
As described in chapter \ref{chap:harmonization_theory}, ComBat-based harmonization procedure simultaneously models and estimates biological and non biological terms, from data and algebraically removes the estimated additive and multiplicative site-effect terms.

Harmonization procedure was tested on the dataset created by using patients from ABIDE I + ABIDE II after the cuts on sex and age discussed in section \ref{chap:analysis_workflow}.
From these patients we tested harmonization using Pearson correlation coefficients and wavelet coefficients of in phase time percentage (w\_in).
We run this analysis to have a quantitative and a visual representation of how features are modified after being processed with an harmonization pipeline.

To harmonize data we used NeuroHarmonize package\footnote{https://github.com/rpomponio/neuroHarmonize} for Pyhton, developed by Pomponio et al. \cite{pomponio-2019} which implemented and added some features to the previous NeuroComBat Python package\footnote{https://github.com/Jfortin1/ComBatHarmonization}.

To use NeuroHarmonize package we need to input the feature data to harmonize and the information about sites as well as information about all the covariates we want to preserve the effect of.
We could also specify whether or not to include non-linear terms for some covariates.

For our data, biological information were provided by a csv file on the ABIDE website. 
In this file, for each patient, medical and biological data are collected in order to provide as much information as possible for each patient.
%A regression model is thus created with parameters estimated with the procedure explained in section \ref{sec:harmonizationtheory}. This model can be applied to the same input data we used to generate it, or to new, unseen data acquired from the same sites we used to create the model.

We tested harmonization procedure on 1470 male patients coming from ABIDE I and ABIDE II of which 737 are controls and 733 cases, choosing as covariates to preserve the age, and the FIQ (Full Intelligence Quotient) to maintain important possible biolgical trends in the data and avoid overcorrections.

The central idea is to create the model as explained on chapter \ref{chap:harmonization_theory} on control subjects only, to operate without the influence of information related to ASD patients whose feature may follow a different distribution compared to control.
Once created the model on control subjects, it is applied to both control and ASD subjects. 
Following this procedure site-related information are eliminated from our data.

To test the effectiveness of this harmonization procedure, we performed a binary classification of data: site vs site, for each pair of sites.
Classification is carried out using a random forest classifier and we tested its performances for site classification before and after the harmonization of data.
In this analysis, we splitted the model into a train and a test subset: using only control subjects from the train dataset, we create the harmonization model as explained before.

To actually harmonize data, the model is applied to all the data: controls and ASD of the train dataset and controls and ASD of the test.
%We remind that this approach, allows train and test dataset to remain independent, because harmonization model is only created on train subjects and once learnt the parameters, is applied to the test dataset. Thus, data we use to train the random forest, are kept separated from the test set, and there's no information leakage of the test data into the training data.
Thus, two random forest classifiers are trained: one using raw data and the second using harmonized data.


As mentioned in the previous paragraph, two sites (\texttt{NYU\_2} and \texttt{KUL\_3}) provided only ASD patients and, since we can not perform a proper harmonization with them, we excluded these sites from our analysis.

Figures \ref{fig:heatmap_harmonization} and \ref{fig:heatmap_harmonization_w} show the results of classification site vs site using the two correlation coefficients described above: Pearson and wavelet-based correlations.
Sites along the x and y axes are ordered by increasing average age of patients and scores are reported in terms of AUC.

Specifically, fig \ref{fig:heatmap_harmonization_noharmon} and \ref{fig:heatmap_harmonization_noharmon_w} show the AUC score of the model trained with the raw, not harmonized dataset, while in figures \ref{fig:heatmap_harmonization_harmon} and \ref{fig:heatmap_harmonization_harmon_w} the results obtained with harmonized data are shown.



% HEATMAPS
\begin{figure}
\centering
\begin{subfigure}[b]{0.7\textwidth}
   \includegraphics[width=1\linewidth]{harmonization/pearson/matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with raw data}
   \label{fig:heatmap_harmonization_noharmon}
\end{subfigure}

\begin{subfigure}[b]{0.7\textwidth}
   \includegraphics[width=1\linewidth]{harmonization/pearson/harmon_matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with harmonized data}
   \label{fig:heatmap_harmonization_harmon}
\end{subfigure}

\caption{Comparison of two heatmaps with AUC score of binary classification site vs site with \textbf{Pearson}-based correlation coefficients of raw (fig \ref{fig:heatmap_harmonization_noharmon}) and harmonized data (fig \ref{fig:heatmap_harmonization_harmon}) classified using a Random Forest Classifier
}
\label{fig:heatmap_harmonization}
\end{figure}



\begin{figure}
\centering
\begin{subfigure}[b]{0.70\textwidth}
   \includegraphics[width=1\linewidth]{harmonization/wavelets_2/matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with raw data}
   \label{fig:heatmap_harmonization_noharmon_w}
\end{subfigure}

\begin{subfigure}[b]{0.70\textwidth}
   \includegraphics[width=1\linewidth]{harmonization/wavelets_2/harmon_matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with harmonized data}
   \label{fig:heatmap_harmonization_harmon_w}
\end{subfigure}

\caption{Comparison of two heatmaps with AUC score of binary classification site vs site of \textbf{wavelet}-based correlation coefficients of raw (fig \ref{fig:heatmap_harmonization_noharmon_w}) and harmonized data (fig \ref{fig:heatmap_harmonization_harmon_w}) classified using a Random Forest Classifier
}
\label{fig:heatmap_harmonization_w}
\end{figure}





To have a visual representation of how much each feature is modified in respect of its sourced site, in fig \ref{fig:features_raw-harmo} two features among the 5995 are shown as a function of sites: \emph{feature 324} and \emph{feature 2800}. This figures show features from Pearson coefficients while in figure \ref{fig:wavelet_features_raw-harmo} are shown the same two features obtained with wavelet coefficients of in time percentage.
Feature 324 represents the correlation between right and left inferior frontal gyrus, and feature 2800 the correlation between left precuneous cortex and the left inferior frontal gyrus. There is not a particular reason for choosing these two feature, they are just randomly drawn from the 5995 possible correlation coefficients.
Their values are plotted as a box along the y axis and sites are indicated along the x-axis, it appears clear how the mean value of each feature for each site is shifted and features are stretched or shrinked to make them follow a more uniform distribution.

\begin{figure}
\centering
\begin{subfigure}[b]{1.\textwidth}
   \includegraphics[width=1\linewidth]{harmonization/pearson/Feature324raw-harmo}
   \caption{}
   \label{fig:feature324}
\end{subfigure}
\begin{subfigure}[b]{1.\textwidth}
   \includegraphics[width=1\linewidth]{harmonization/pearson/Feature2800raw-harmo}
   \caption{}
   \label{fig:feature2800}
\end{subfigure}
\caption{Pearson-based connectivity coefficients per site for features 324 (fig \ref{fig:feature324}) and 2800 (fig \ref{fig:feature2800}) before and after harmonization}
\label{fig:features_raw-harmo}
\end{figure}



 \begin{figure}
 \centering
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{harmonization/wavelets_2/Feature324raw-harmo}
    \caption{}
    \label{fig:wavelet_feature324}
 \end{subfigure}
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{harmonization/wavelets_2/Feature2800raw-harmo}
    \caption{}
    \label{fig:wavelet_feature2800}
 \end{subfigure}
 \caption{wavelet-based connectivity coefficients of in-phase percentage, for features 324 (fig \ref{fig:wavelet_feature324}) and 2800 (fig \ref{fig:wavelet_feature2800}) before and after harmonization}
 \label{fig:wavelet_features_raw-harmo}
 \end{figure}



 If we split these plots into control and ASD subjects to see the effect of harmonization separately on these subsets, we obtain figure \ref{fig:features_control-ASD_raw-harmo} which shows feature 324 computed using Pearson coefficients and figure \ref{fig:feature324-control-ASD_w} with wavelet coefficients.

 \begin{figure}
 \centering
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{harmonization/pearson/Feature324control-ASD_raw-harmo}
    \caption{Feature 324 with Pearson correlation coefficients}
    \label{fig:feature324-control-ASD}
 \end{subfigure}
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{harmonization/wavelets_2/Feature324control-ASD_raw-harmo}
    \caption{Feature 324 with wavelet-based correlation coefficients}
    \label{fig:feature324-control-ASD_w}
 \end{subfigure}
 \caption{Pearson correlation and wavelet coefficients for features 324 before and after harmonization with a separated plot for controls and ASD}
 \label{fig:features_control-ASD_raw-harmo}
 \end{figure}




\section{Comments on harmonization}

%C'è da dire che anche dopo l'armonizzazione, il sito SU_2 rimane distinguibile, anche se dalle feature sembra essere stato allineato con gli altri. Quindi stessa storia potrebbe essere avvenuta per i wavelet.

Figure \ref{fig:heatmap_harmonization} \ref{fig:heatmap_harmonization_w} show a binary classification performed with a random forest, for site classification of Pearson-based and wavelet-based correlation coefficients respectively.
Before harmonization we notice (fig \ref{fig:heatmap_harmonization_noharmon} and \ref{fig:heatmap_harmonization_noharmon_w}) that the AUC value is mainly near to 1, which stands for exact ability to classify site\_a vs site\_b.
With Pearson-based coefficients this value decreases when the classification is performed using harmonized data (figure \ref{fig:heatmap_harmonization_harmon}).
This stands as a confirm that the harmonization procedure removes (or, at least, reduces) site-related features making them less recognizable among data.
This same effectiveness, though, is not visible with wavelet-based coefficients.
It appears clear from figure \ref{fig:heatmap_harmonization_harmon_w}) that sites still remain distinguishable even after harmonization.
Actually, they even becomes more distinguishable. To understand why this happens we can take a look at figure \ref{fig:wavelet_features_raw-harmo} and confront it with figure \ref{fig:features_raw-harmo} we can notice the presence of a greater number of outliers between data.
It is likely that those values are decisive and for the discrimination between two sites.
The discriminability of sites becomes higher because after harmonization,  for each site, a feature is scaled to follow a more regular distribution centered at its mean value, but the outliers still are outliers even after the harmonization as it is possible to see from the plots.
This leads to a greater effect of these outliers to the outcome of the classification.


%This may be due to the loss of some information linked to the site througout all the process of extracting a correlation coefficient starting from a time-frequency analysis of two timeseries.
%Accordind to this hypotesis, if we lost site information, the particular trend of a feature we see for example in figure \ref{fig:wavelet_features_raw} is just a casual trend and taking into account all of the features, there is not a specific bias connected to a site.
%This way, during procedure of harmonization, we are creating a bias towards each site and for this reason after harmonization, sites remain distinguishable, or actually, they become even more recognisable
%If we look at the boxplot of the mean value of a feature computed with wavelet-based correlation across sites (figure \ref{fig:wavelet_features_raw-harmo}) we note that there is a certain trend related to sites, that disappears after harmonization.
%It is possible either that we obtain this trend just for this choice of feature or that even if each feature underwent a level out procedure, they still remain recognizable across sites because of different outliers.

\hfill

Both for wavelet-based coefficients in figure \ref{fig:wavelet_features_raw-harmo} and for Pearson-based coefficients in figure \ref{fig:features_raw-harmo} we can notice a disordered trend of features of raw data (plots on the left side).
This trend is mitigated by the effect of harmonization, on the right side of figures  \ref{fig:features_raw-harmo} and \ref{fig:wavelet_features_raw-harmo}.

%Excluding just for a moment the idea that this trend is related to a site effect, it would be possible to think that it is due to the presence of an unbalanced number of control/ASD data across sites.
%This unbalanced distribution would reflect on the mean value of a feature per site, perhaps site with a greater number of controls exhibit a greater or lower mean value of the shown feature.
\textcolor{ForestGreen}{
In figure \ref{fig:feature324-control-ASD} and \ref{fig:feature324-control-ASD_w} it is possible to notice how harmonization affects controls and ASD data separately.
It appears that it is more effective in removing site-effects from controls than from ASD data.
This result is a consequence of the different distribution followed by controls and ASD, so that, parameters for harmonization extracted from control distribution may differ from the necessary ones to properly harmonize ASD data. The main reason is likely due to little statistic in both cases.
In a dataset of bigger dimensions, the theoretical trend of harmonized ASD data should be as smoothed as the one of harmonized control data.
}
%However, this hypotesis is disproved by the plots in figures \ref{fig:feature324-control-ASD} and \ref{fig:feature324-control-ASD_w}.
%It appears clear that the disordered trend manifests both for controls and ASD data separately.

%Thich is not due to the presence of an unbalanced Controls/ASD number of patients per site, because from figure \ref{fig:features_control-ASD_raw-harmo} we observe as well, a messy trend both for Controls and ASD.
We can therefore conclude that in our data there is a bias linked to the acquisition site.
After harmonization procedure both Pearson-based coefficients and wavelet-based coefficients assume a more regular trend.
This is a visual confirm that harmonization effectively removes site-related, but if for Pearson coefficients this is effective to remove inter-site variability, for wavelet-based coefficients this method is not enough to remove site-related effect.

%This bias is more evident with Pearson-based correlation coefficients (fig \ref{fig:wavelet_features_raw}) than with wavelet-based coefficients (fig \ref{fig:features_raw-harmo}) that appear to assume a more uniform trend.



\newpage
\chapter{Deep neural models} \label{chap:deep_models}

\section{Deep neural network}

In this work we performed a classification task, using a shallow neural network built using Keras library\footnote{https://keras.io/} for Python.
Since we are working under a permanent overfitting condition, we tried to build a network with as less capacity as possible, but still preserving good classification performances.
To this end, we used Pearson coefficients o perform a control/ASD classification using the dataset ABIDE I + ABIDE II.
We performed the optimization of our network with a shallow network comprising only 3 layers.
By gradually adding neurons, we searched for their minimum number to employ in each layer.
We started with a trivial network made by just 3 neurons in the first layer, 2 in the second and at last a single-output layer.
Even if this was just an attempt to put a lower limit to the number of neurons, we noted that even with this configuration, the network tends to overfit our data.
This is clear if we take a look at the learning curves in figure \ref{fig:learningcurve} which shows the training and validation AUC curves for three simple models: the first one (fig \ref{fig:auc_no3-2-1}) is the trivial structure just mentioned and shows the classical trend of an overfitting condition.
This trend is characterized by an increasing AUC curve on the training set over epochs, while the validation AUC score, after a few epochs, settles on a value and does not increases. 
We continued adding neurons to each layer and tried different configurations. 
With a configuration of 8-8-1 (fig \ref{fig:auc_no8-8-1}) the performances were slightly higher, so we set the second layer to 8 neurons and tried changing the number of the neurons in the first layer.
Performances seemed to increase as the number of neurons increased, as is possible to see in fig \ref{fig:auc_no64-8-1} with a network consisting of 64-8-1 neurons that reached a validation AUC value of $\sim 70\%$. 
Adding more neurons network reached a stable performance value.
Different attempts and the related classification scores are reported in table \ref{tab:different_model_structures} in Appendix.
We decided to pick the configuration comprising the minimum number of neurons, beyond which performances were stable, and the addition of more neurons to create a more complex network was unjustified.

\begin{figure}
\centering
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no_321}
   \caption{Train and validation AUC curve for a network with structure 3-2-1.}
   \label{fig:auc_no3-2-1}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no_881}
   \caption{Train and validation AUC curve for a network with structure 8-8-1.}
   \label{fig:auc_no8-8-1}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no_6481}
   \caption{Train and validation AUC curve for a network with structure 64-8-1.}
   \label{fig:auc_no64-8-1}
\end{subfigure}
\caption{AUC learning curve corresponding to deep neural networks with different structures trained on the entire non-harmonized dataset consisting on ABIDE I + ABIDE II.}
\label{fig:learningcurve}
\end{figure}


In our work we employed the neural network schematized in figure \ref{fig:model_structure}.
This network consists of 2 hidden layers made up of 264, and 8 neurons respectively. 
Both layers are activated by a ReLU function and separated by a Batch-Normalization and a Dropout layer with a dropout chance of 30\%.
After the 8-neurons layer we added a second Batch-Normalization layer, before the output layer.
At the end we put a single output layer with a single neuron for the classification output.
This last layer is activated by a sigmoid function, which outputs a real number between 0 and 1.



\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{ml/models/model_structure}
\caption{Schematic structure of the deep neural network employed in this work for control subjects vs ASD patients classification. It follows a structure 264-8-1.
Each box contains the layer name, layer task (Input, Dense, BatchNormalization etc..), activation function, input shape (number of neurons) and output shape.}
\label{fig:model_structure}
\end{figure}


To create the network we set the subsequent hyperparameters after a grid search on learning rate and number of epochs, setting a validation set of 25\% of train data, and studying the evolution of the learning curves, to find the minimal parameters configuration which allow the best classification scores.
We trained the model using binary cross-entropy loss function and optimizing parameters using Adam optimizer with a learning rate of $10^{-4}$.
Adam optimizer was chosen over Stochastic Gradient Descent (SGD) optimizer since with Adam we empirically achieved similar classification performances with a lower error than with SGD.
%Model's classification performances were assessed with a 5 k-fold Cross validation, collecting the AUC score for each fold and then computing mean and std deviation of these results.

%Adversarial model:
\newpage
\section{Domain-adversarial neural network}\hfill

As mentioned in chapter \ref{chap:analysis_workflow} a different approach to classification was fulfilled by using a domain-adversarial neural network (DANN).
A domain-adversarial (or site-adversarial) network is a model able to learn from data both class and site information, and avoid to make class predictions affected by a bias due to sites.

%The main idea behind this network comes from a revisited and customized version of the model described in chapter \ref{chap:domain_adversarial_theory}, but before building this network we tried a slight different approach.

%LOSSES COMBINATION
\paragraph{Adversarial model with losses composition}\hfill

\noindent Our first attempt was to implement a model able to predict category label without any influence of site information consisting on two different branches, one for the output of control/ASD prediction, and one for the output of the predicted site.
With these two outputs, a loss is created by combining two different loss functions, one for the class and the other for site: the idea is similar to that proposed in the article \cite{guan2021}.
This combined loss is in the form:
\begin{equation}
L = L_1 - \lambda L_2
\end{equation}
Where $L_1$  is in our case the loss for the binary control/ASD classification, that we want to minimize (binary crossentropy), while $L_2$ is the loss for site classification: a categorical crossentropy we aim to maximize in order to avoid to learn any information related to sites.
$\lambda $ is a parameter empirically set, to control the contribution of the sites loss $L_2$ to the overall loss.
This way, the minimum of the total loss is reached with parameters that minimize the category classification loss and maximize the site classification loss.
We empirically noticed that performances of this type of model are strictly linked to the value of the parameter $\lambda$.
If a certain value gives an optimal performance on a certain dataset, on another dataset, for example one created with different constraints on patients, the best score is obtained for a slightly different value of $\lambda$.

%DOMAIN-ADVERSARIAL
\paragraph{Adversarial model with gradient reversal layer}\hfill

\noindent Our second approach was the creation of a domain-adversarial neural network following the idea described in chapter \ref{chap:domain_adversarial_theory}.
Even though this network is based on the same principles of the neural network with combination of two losses (minimize the error on classification of controls/ASD, and maximizing the one on site classification), it accomplishes its task in a different way.
The model is trained to learn site-related patterns from data, and use these information in an adversarial way during the extraction of relevant feature for the classification controls/ASD.
With respect to the model described in chapter \ref{chap:domain_adversarial_theory}, our model has a substantial difference in the structure of the site-classifier branch.
The difference is due to the number of sites our data belong to: we deal with 36 sites when using the dataset of ABIDE I + II, while the domain-adversarial network of chapter \ref{chap:domain_adversarial_theory} only concerns 2 domains.

%whose inner weights are updated moving towards a minimization of the gradient for classification, and in the opposite direction for site classification, as explained in section

To construct our domain-adversarial neural network, we employed the same number of layers and neurons as we used to build the deep neural network implemented in the previous paragraph.
Then we embedded in this structure the domain-adversarial branch.
We can describe the structure of this model as consisting on three parts: a feature extractor branch takes input data and creates an inner representation of them within its structure; at this point the network is forked into two branches: a label classificator branch for control/ASD cassification and the site classification one.
At the top of the site-classification branch, the gradient reversal layer was placed.

%with a composition of the two losses, the structure of this model can be divided into three main components, two of which are exactly the same of the deep neural model explained at the beginning of this section.
In details the first branch namely the feature extractor, comprises the input layer, and the two hidden layer consisting on 264 and 8 neurons respectively.
Between the two hidden layers we put a batch-normalization and a 0.3 dropout layer, as we did in the DNN.
From this point on the network is splitted into two branches: the label classifier, similarily to the DNN consists of an output layer with a single neuron activated by a sigmoid function, for the output of control/ASD classification.
The other branch, the site classifier consists of a gradient reversal layer at the top of it, followed by a layer with 16 neurons, a batch-normalization layer and the last layer: a multi-output layer with N neurons, being N the total number of sites that input data belong to.
The effect of the gradient reversal layer can be regularized with a factor to weight the contribution of the site loss on the feature extractor parameters update during backpropagation.
We empirically set this value to 0.3.
The structure of this network is illustrated in figure \ref{fig:adv_model_structure}

The final layer of the site-classification branch is activated by a softmax function, and the loss function employed is the categorical crossentropy, commonly used for multi-class classification tasks as explained in section \ref{sec:loss_functions}.


To work with this loss function it is necessary to perfrom a \textbf{one-hot encoding} of the sites because functions like these can not deal with string variables such as site names.
For this reason, firstly we have to assign a number to each site name like: site\_a = 0, site\_b = 1 and so on for all the M sites.
However, if we stop to this point and use integer numbers $\{0, ..., M-1\}$ to compute the loss, our classification model would end up considering sites with higher number as  \say{larger} than others.
This would result in a non optimal way to compute an error since, for example, the error between site 0 and site M-1 would be greater than the error between 0 and 1.
This should be avoided in our case since all the sites are equally important. For this reason working with increasing site label values is not the best way to encode site information. % not-optimal results.
One hot encoding provides a solution to this problem.
This common approach converts each value to a vector of length M, containing all zeros but in the entry corresponding to the number of the site it is encoding, where it puts 1.
For example if a datapoint belongs to a site m, its corresponding one-hot vector will be defined as $y_{im} = \begin{cases} 1  \text{ if } y_i = m \\ 0 \text{ otherwise }\end{cases}$
With this transformation, each site label becomes a binary vector, with just a single 1 places in correspondence to that specific site.


\hfill

We tested and compared the two different adversarial models (the model with loss combination and the one with gradient reversal) and we noticed that they had more or less similar performances. However, we choose to carry out all the subsequent analysis with this second model, for a main reason:
%Firstly, its performances are more stable with respect to the value of the regularizer parameter $\lambda$: just like in the model with loss compositions, the gradient reversal layer contains a parameter $\lambda$ that controls the weights updating during the backpropagation.
%However, results obtained with this model are more stable in respect to the the value of this parameter.
the implementation of a network with a gradient reversal layer allows to create a site-classifier branch able to recognize sites, since the inversion of gradient only takes place at the top of this branch. This allows an update of weights related to this branch aimed to reduce the error on this branch for site classification and encourages it to learn site-related patterns. These information are then used in an adversarial way during backpropagation by updating only parameters related to the feature extractor branch in an opposite direction, in order to maximize the loss, as shown in equations \ref{eq:adversarial_updating_parameters}.
On the other hand, the creation of a network with combination of two losses would not allow this difference in parameters update, since the minus sign is related to the loss itself.
This leads to an updating of both site-classifier branch and feature-extractor made in order to maximize the site loss.
In this way, the site-classifier branch is discouraged to learn site related pattern, which is not what we aimed to create with this model.

In addition, there are studies \cite{kamath2019}, that compared the performances obtained with different implementation of adversarial network, and, even if they work with different data (text data), they find the DANN the best performing implementation.
Since this network appears to be more promising than the simple loss combination networks, we choose to employ this network in the following analysis.




\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{ml/models/adv_model_structure}
\caption{Structure of the domain-adversarial neural network with two outputs: a single-neuron output for binary classification controls/ASD and a multi-class classification for site classification.
Each box contains the layer name, and the related task (Input, Dense, BatchNormalization etc..), activation function, input shape (number of neurons) and output shape.
}
\label{fig:adv_model_structure}
\end{figure}


\newpage
\chapter{Results of classification} \label{chap:results_deeplearning}
In the following sections we report results of classification of control subjects vs ASD patients.
Classification is pursued by following different harmonization pipelines as explained in chapter \ref{chap:analysis_workflow}.
We mainly employed a deep neural network for classification of raw and harmonized data, and we compared results with those obtained with the adversarial neural network presented in chapter \ref{chap:deep_models}.

From now on, in all the tables where results are reported, we will denote with the following labels different harmonization pipelines:

\begin{itemize}
 \item \textbf{Harmon. k-fold} the pipeline where harmonization is implemented inside the k-fold CV and harmonization model is created only on control train data, and then applied to the rest of the dataset. The process is outlined in figure \ref{fig:harmon_kfold_flowchart}. 
 \item \textbf{Harmon. upstream} the pipeline where harmonization is implemented upstream, namely before the partition of the dataset, for each k-fold, into a train and a test subset. With this pipeline, harmonization model is created using data of all controls as outlined in figure \ref{fig:harmon_upstream_flowchart}. 
 \item \textbf{No harmon.} the pipeline where classification is carried out using raw data and no harmonization is applied. 
 \item \textbf{Adversarial} the pipeline where we use the domain-adversarial neural network giving as input raw data.
\end{itemize}

Furthermore, when we list in tables the different dataset partitions, we will denote as ABIDE I+II / ABIDE I / ABIDE II datasets created with patients belonging to the two different release of ABIDE dataset, as described in chapter \ref{chap:dataset}.
The cuts on covariates such as sex and ages as discussed in chapter \ref{chap:analysis_workflow}.
When from these subsets we select only patients who kept their eyes open, expressly specify it as \textbf{eye = open}.

We eventually compared performances of the deep neural network, with a random forest classifier.
We limited these comparison only on two dataset partition, following the three main harmonization pipelines of Harmon. k-fold, Harmon. upstream and No harmon.  


\section{Results with Pearson coefficients}\label{sec:pearson_results}
The first classification analysis was made on Pearson correlation coefficients. 
%To recall the different analysis pipeline we can take a look at flowchart \ref{fig:workflowanalysis}, and in this analysis \textquotedblleft Coefficients selection \textquotedblright  translates as we are selecting coefficients based on Pearson correlation.

Results of classification are reported in terms of mean AUC across all the 5 folds.
We listed in table \ref{tab:classification_pearson} results obtained with our DNN model following different harmonization implementation and with the domain-adversarial network.

In table \ref{tab:comparison_dnn_rf_pearson} we compared some of these results with a Random Forest classifier to check whether or not is appropriate to use a deep neural model instead of a conventional machine learning algorithm.
Comparison with random forest were performed on the whole dataset ABIDE I+II, and on ABIDE I with open eyes, since on this dataset we obtained higher classification performances. 
Results of DNN and RF concern the three pipelines: hamon k-fold , harmon. upstream, and no harmon.








%If the table is too wide, replace \begin{table}[!htp]...\end{table} with
%\begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]...\end{threeparttable}\end{adjustwidth}
\begin{table}[!htp]
\centering
\begin{tabular}{|lcccc|}
\hline
Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adversarial \\
\hline
AB I+II &71$\pm$1 &74$\pm$2 &73$\pm$3 &70$\pm$3 \\
AB I &71$\pm$3 &74$\pm$2 &72$\pm$3 &71$\pm$4 \\
AB 2 &63 $\pm$5 &68$\pm$ 6 &64$\pm$ 4 &66 $\pm$ 4 \\
AB I+II, eye = open &72$\pm$3 &75$\pm$4 &72$\pm$3 &71$\pm$3 \\
AB I, eye = open &73$\pm$ 1 &76$\pm$2 &72$\pm$1 &72$\pm$4 \\
AB II, eye = open  &66$\pm$3 &70$\pm$6 &69$\pm$6 &68$\pm$6 \\
\hline
\end{tabular}
\caption{AUC score obtained with the deep neural network, using Pearson-related coefficients and following different harmonization procedures.}
\label{tab:classification_pearson}
\end{table}





\begin{table}[h]
        \centering
        \scalebox{1.3}{
        \begin{tabular}{|ccccc|}
        \hline
        Harmon. pipeline & \multicolumn{2}{|c|}{AB I + II} & \multicolumn{2}{|c|}{AB I eye = open}\\
        \hline 
        &DNN &RF  &DNN &RF \\
        Harmon. k-fold &71$\pm$1 &66+2  &73$\pm$ 1 &70$\pm$ 3 \\
        Harmon. upstream &74$\pm$2 &72+1 & 76$\pm$2 &71$\pm$ 2 \\
        No harmon. &73$\pm$3 &65+1 &72$\pm$1 &68$\pm$ 3 \\
        \hline
       \end{tabular}
       }
    \caption{Comparison between AUC scores of a DNN and a RF classifier, using Pearson-based correlation coefficients, for dataset: ABIDE I + II and ABIDE I with open eyes}
    \label{tab:comparison_dnn_rf_pearson}
\end{table}


\subsection*{Discussion}
From table \ref{tab:classification_pearson} it is possible to notice that using the whole dataset does not lead to an improvement of results with respect of the use of ABIDE I dataset only.
On average, the best classification performances are obtained using ABIDE I data with only open eyes.
Results obtained with ABIDE II dataset are significantly lower this can be an effect of a greater variability of data in this dataset.
Even if almost all the AUC scores are compatible with each other,
with the pipeline of upstream harmonization, the mean AUC value  is systematically higher than the other three pipelines.

Table \ref{tab:comparison_dnn_rf_pearson} shows the comparison between a random forest and a DNN. It appears clear the advantage in using the DNN since the mean AUC value is significantly higher than the one achieved with a random forest.
Results increase when we limit the analysis to a more homogeneous dataset collecting only patients with open eyes belonging to ABIDE I.
Random forest performances as well are boosted when following the pipeline of upstream harmonization.
Since this is a common trend even with different coefficients, for a proper discussion we refer to section \ref{sec:classification_discussion}.


\newpage

\section{Results with wavelet coefficients}\label{sec:wavelet_results}
Following the same workflow as with Pearson-based coefficients, we run the classification and harmonization pipelines using wavelet-based coefficients.
We used different wavelet coefficients as described in section \ref{sec:coefficient_selection}.
Results are listed in table \ref{tab:classification_wavelet}.

  
%If we refer to the flowchart \ref{fig:workflowanalysis}, now with  \say{Coefficients selection} we denote one of the four different types of coefficients created using wavelet analysis.



\begin{table}[h]
%\centering
\begin{tabular}{clcccc}
\hline
Coefficients & Dataset &Harmon k-fold &Harmon. upstream &No harmon. &Adversarial \\
\hline
\multirow{6}{*}{w\_in}
&AB I+II &65$\pm$ 2 &74$\pm$ 2 &66$\pm$ 2 &67 $\pm$2 \\
&AB I &67$\pm$ 3 &73 $\pm$2 &68$\pm$ 3 &68$\pm$ 2 \\
&AB II &62 $\pm$4 &68$\pm$ 4 &63$\pm$ 4 &62$\pm$ 4 \\
&AB I+II, eye=open &65 $\pm$4 &71 $\pm$3 &66 $\pm$3 &66$\pm$ 3 \\
&AB I, eye=open &69$\pm$ 4 &74$\pm$ 6 &67$\pm$ 3 &67 $\pm$4 \\
&AB II, eye=open &66 $\pm$2 &69 $\pm$3 &67$\pm$ 2 &68 $\pm$2 \\
\hline
\multirow{6}{*}{w\_out}&
AB I+II &60$\pm$1 &71$\pm$3 &63$\pm$2 &60$\pm$1 \\
&AB I &61$\pm$2 &69$\pm$3 &62$\pm$2 &61$\pm$2 \\
&AB II &56$\pm$3 &69$\pm$4 &60$\pm$4 &59$\pm$4 \\
&AB I+II, eye=open &63$\pm$2 &68$\pm$3 &64$\pm$2 &63$\pm$3 \\
&AB I, eye=open &63$\pm$5 &74$\pm$3 &71$\pm$4 &72$\pm$2 \\
&AB II, eye=open &63$\pm$5 &70$\pm$3 &63$\pm$3 &62$\pm$3 \\
\hline 

\multirow{6}{*}{w\_stack}
&AB I+II &65$\pm$ 2 &71$\pm$ 2 &62$\pm$ 2 &64$\pm$ 1 \\
&AB I &66$\pm$ 3 &73 $\pm$3 &65$\pm$ 3 &66$\pm$ 2 \\
&AB II &62 $\pm$3 &65 $\pm$3 &61 $\pm$4 &60 $\pm$2 \\
&AB I+II, eye=open &64$\pm$ 4 &70 $\pm$3 &66$\pm$ 3 &65 $\pm$3 \\
&AB I, eye=open &64 $\pm$5 &68 $\pm$5 &64 $\pm$3 &64 $\pm$4 \\
&AB II, eye=open &66 $\pm$2 &68$\pm$ 3 &66$\pm$ 4 &65 $\pm$5 \\
\hline

\multirow{6}{*}{w\_diff}
&AB I+II &67$\pm$ 3 &70$\pm$ 3 &66 $\pm$3 &66 $\pm$3 \\
&AB I &70$\pm$ 3 &72 $\pm$2 &68 $\pm$3 &68$\pm$ 3 \\
&AB II &61$\pm$ 4 &65$\pm$ 4 &64 $\pm$5 &65 $\pm$4 \\
&AB I+II, eye=open &70$\pm$ 1 &73$\pm$ 2 &71 $\pm$1 &68 $\pm$2 \\
&AB I, eye=open &71$\pm$ 4 &74 $\pm$3 &71 $\pm$3 &68 $\pm$2 \\
&AB II, eye=open &62 $\pm$4 &66 $\pm$3 &68 $\pm$3 &64$\pm$ 5 \\
\hline
\end{tabular}
\caption{AUC score obtained with the deep neural network, using the 4 different wavelet-based coefficients. For each partition of the main dataset (AB I+II) results are listed following all the harmonization pipelines described in chapter \ref{chap:analysis_workflow} }
\label{tab:classification_wavelet}
\end{table}

As we did with Pearson-related coefficients, we compared results obtained using a DNN with results obtained with a random forest. %We run this comparison only on w\_in coefficients just to assess whether or not a conventional classifier results in similar performances as the DNN. results are 
They are listed in table \ref{tab:comparison_dnn_rf_win}.


\begin{table}[h!]
        \centering
        \scalebox{1.3}{
        \begin{tabular}{|ccccc|}
        \hline
        Harmon. pipeline & \multicolumn{2}{|c|}{AB I + II} & \multicolumn{2}{|c|}{AB I eye = open}\\
        \hline 
        &DNN &RF  &DNN &RF \\
        Harmon. k-fold & 65$\pm$2 & 62$\pm$5 & 69$\pm$4 & 62$\pm$5 \\
        Harmon. upstream & 74$\pm$ 2 &74$\pm$3 & 74$\pm$ 6 &69$\pm$3 \\
        No harmon. & 66$\pm$2 & 59$\pm$3 & 67$\pm$3 & 61$\pm$5 \\
        \hline
       \end{tabular}
       }
   \caption{Comparison between AUC scores of a DNN and a RF classifier, using w\_in coefficients, for dataset: ABIDE I+II and ABIDE I with open eyes}
    \label{tab:comparison_dnn_rf_win}
\end{table}

\subsection*{Discussion}
Looking at table \ref{tab:classification_wavelet} we can notice that some coefficients are more significant than other in classification between controls and ASD.
w\_in outperform w\_out and stacking together these two coefficients to obtain w\_stack does not bring to an improvement in the average classification performances.
The best way to combine information from w\_in and w\_out seems to be the elementwise subtraction in order to create w\_diff.

w\_diff is an effective way to create a coefficients that carry information in a similar way as Pearson coefficient do.
They compress the information held by w\_in and w\_out into a single coefficient without the further increasing in dimensionality as we obtain with the creation of w\_stack. 

The best classification performances are obtained using the dataset ABIDE I with only open eyes. While using just ABIDE II dataset leads to the lowest classification scores.
As we noticed from the results obtained with Pearson coefficients, the pipeline of upstream harmonization leads to an increase of classification performances.
With the other three pipelines: harmon. k-fold, harmon. upstream and no harmon. we achieve on average similar performances.

\newpage


\section{Results with PCA}\label{sec:results_pca}

As mentioned in section \ref{chap:analysis_workflow} we chose to run PCA analysis using coefficients that gave the best classification results in the previous analysis.
Comparing average scores obtained with Pearson-based and with wavelet-based coefficients, we can notice that with Pearson we achieved a better separability between controls/ASD. 
Furthermore, to not weight down this table, we limited our dataset choice to the whole ABIDE I+II and ABIDE I with open eyes.
The latter is chosen because on it, we obtained slightly hiher AUC scores.

We started our analysis choosing the proper number of PC to explain $> 90\%$ of variance and then gradually reduced this number as reported in table \ref{tab:pca_explained}.

\begin{table}[h]
\centering
\begin{tabular}{ |l|c c| }
\hline
\multicolumn{3}{|c|}{PCA explained variance [\%]} \\
 \hline
  N PC & ABIDE I + II  & AB I eye=open \\
  \hline
 800 & 93 & -- \\
 400 & 78 & 98 \\
 200 & 62 & 77 \\
 100 &48 & 58 \\
 50 & 36 & 41 \\
 20 & 24 & 26\\
 \hline
\end{tabular}
\caption{Explained variance [\%] for different numbers of principal components, and for the whole dataset of ABIDE I+II and ita subset of ABIDE I with eye open.}
\label{tab:pca_explained}
\end{table}

As explained in section \ref{sec:pca_theory} the number of pc it is possible to extract is limited by the dataset size.
It must be $N\_pc \leq min\{n\_samples, n\_features\}$.
For this reason the first row of table \ref{tab:pca_explained}
lacks an entry for the dataset ABIDE I open eyes.
Using this dataset, we have in fact, an amout of training data (from which PC are extracted) lower than 800 samples.

We followed the same pipelines as we did for Pearson and wavelet coefficients, following the four different harmonization procedures.
Results obtained from this analysis are reported in table \ref{tab:classification_pearson_pca} and shown in figure \ref{fig:classification_pearson_pca}.

\begin{table}[!htp]\centering
\begin{tabular}{lrrrrrr}\toprule
PC &Dataset &Harmon. K-fold &Harmon. upstream &No harmon. &Adversarial \\\midrule
800 &AB I+II &69$\pm$2 &73$\pm$3 &71$\pm$1 &71$\pm$2 \\
\hline
\multirow{2}{*}{400} &AB I+II &67$\pm$3 &73$\pm$5 &72$\pm$3 &73$\pm$3 \\
&AB I, eye=open &71$\pm$3 &72$\pm$3 &71$\pm$3 &72$\pm$4 \\
\hline
\multirow{2}{*}{200} &AB I+II &68$\pm$3 &74$\pm$2 &72$\pm$2 &71$\pm$2 \\
&AB I, eye=open &69$\pm$3 &73$\pm$2 &72$\pm$4 &73$\pm$3 \\
\hline
\multirow{2}{*}{100} &AB I+II &67 $\pm$2 &72$\pm$2 &69$\pm$1 &70$\pm$2 \\
&AB I, eye=open &70$\pm$2 &73$\pm$2 &71$\pm$4 &72$\pm$2 \\
\hline
\multirow{2}{*}{50} &AB I+II &66$\pm$4 &70$\pm$6 &68$\pm$3 &69$\pm$2 \\
&AB I, eye=open &70$\pm$4 &72$\pm$3 &69$\pm$3 &72$\pm$3 \\
\hline
\multirow{2}{*}{20} &AB I+II &65$\pm$5 &67$\pm$4 &64$\pm$5 &66$\pm$1 \\
&AB I, eye=open &68$\pm$6 &71$\pm$2 &71$\pm$6 &71$\pm$3 \\
\bottomrule
\end{tabular}
\caption{AUC score obtained with the deep neural networks, using a decreasing number of principal components, following different harmonization procedures}
\label{tab:classification_pearson_pca}
\end{table}





\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_vs_pc_all}
   \caption{Classification on the entire dataset ABIDE I+II}
    
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_vs_pc_ab1}
   \caption{Classification on dataset ABIDE I - only open eyes}
    
\end{subfigure}
\caption{AUC scores obtained from control/ASD classification of data using decreasing numbers of principal components for Pearson-based coefficients.
Results obtained with different analysis are represented with different colors: AUC scores with data harmonized inside k.fold are marked \textbf{blue}, AUC score with data harmonized upstream \textbf{orange}, AUC scores with raw data (not harmonized) \textbf{green} and AUC scores classification with the adversarial network \textbf{red}.
}
\label{fig:classification_pearson_pca}
\end{figure}


\subsection*{Discussion}
As it is possible to notice from table \ref{tab:classification_pearson_pca}, reducing the dimensionality of the problem using PCA is an effective way to keep relevant information from data and discard redundant or uninformative data.
Comparing these results with those listed in table \ref{tab:classification_pearson} we can notice that, expecially with an higher number of principal components, classification performances are not affected by this reduction of dimensionality.
The higher is the percentage of variance explained with PCA the higher classification performances are achieved for the both dataset ABIDE I + II and ABIDE I with open eyes.

At a first glance we can notice from figure \ref{fig:classification_pearson_pca} that reducing the number of PCs leads to a gradual reduction of classification performances.
This trend is more evident using the whole datataset, while with ABIDE I only open eyes dataset, we obtain a flatter performances curve.
A reduction from 5995 to 800 or 400 feature is a substantial reduction since we are lowering our dimensionality by an order of magnitude still preserving an high classification AUC comparable to that obtained without PCA.
In this analysis as well, following pipeline of upstream harmization, leads to an improvement of performances the suspected reason for this common trend are explained in section \ref{sec:classification_discussion}.

%data leakage due to the prior harmonization of dataset plays an important role as it leads to a rise in classification performances for any choice of number of PCs.



\section{Discussion on classification results}\label{sec:classification_discussion}

\hfill 

\paragraph{Comparison between Pearson and wavelets}
\hfill

\noindent From results reported in section \ref{sec:pearson_results} and \ref{sec:wavelet_results} we can observe that average scores obtained with Pearson correlation coefficients are systematically higher than wavelet-based ones.
We can therefore assert that working with wavelet-based correlation coefficient obtained through processes described in section \ref{sec:wavelet_theory} does not bring many advantages to this type of analysis.
A possible reason for this outcome is the loss of information that occurs during the extraction of these coefficients.
To this regard an important role is played by the introduction of redundancy and randomness into this analysis. 
Starting from two timeseries wavelet transform computes a 2D matrix for each timeseries and uses them to calculate the cross-power spectrum as explained in the dedicated paragraphes of section \ref{sec:wavelet_theory}.
Doing so, we create a redundant representation of timeseries data moving from one dimension (timeseries) to two-dimensional data (cross-power spectrum matrix).
We furthermore compare the cross-power spectrum obtained from the wavelet transform of each timeseries pair, with a red-noise cross-power spectrum to assess the significance of our data.
In the end we shrink again our dimensions to reduce to a single data point representing a correlation.

Since differences of traits between controls and ASD are not very strong, this process can cause the loss of this weak information, which does not happen with a more direct and linear correlation calculus like with Pearson coefficients.

This lower scores is found both on wavelet coefficients of in-phase and counter-phase time percentage (w\_in and w\_out), and not even the combination of this two in a double-sized array.
The best way to combine wavelet information, however, seems to be the creation of what we called w\_diff, obtained by subtracting, for each subject, coefficients of counter phase w\_out from coefficients of in-phase w\_in.
%This process allows the creation of a coefficient with similar properties as Pearson ones, containing all information belonging to w\_in and w\_out, but without the further increasing in dimensionality as we have by stacking together w\_in and w\_out.
However, average scores obtained through all the pipelines and over all the dataset is lower that the ones obtaines with Pearson.

The use of wavelet-based coefficients still represent an important and different way to extract a correlation coefficient between two signals.
However, for this work, with our classification task, these coefficients are not the most suitable.


\paragraph{Effects of a smaller and more regular dataset}\hfill

To discuss the benefits of reducing the dataset size to run analysis on a more homogeneous cohort of subjects, we focus on table \ref{tab:classification_pearson} and \ref{tab:classification_wavelet} 

We can state that the eye status of patients during the scan has a non-negligible impact on the outcome of classification, since limiting the dataset only on subjects with open eyes systematically improves classification scores of more than 1 \% for every dataset (ABIDE I+II, ABIDE I or ABIDE II).
As introduced in chapter \ref{chap:autism} there can be differences in brain functional areas between patients with open eyes and patients with closed eyes.
This may happen because of the activation of different cortex and gyri areas, expecially those related to the processing of visual stimuli. 
An other variability introduced by patients with eyes closed, is that during the scan, some of them may fall asleep, hence heavily modifying the functional connectivity network of their brain.
For this reason, removing these sources of variability could bring to a cleaner distinction of relevant patterns between controls and ASDs.

We can also notice that results on the dataset made by ABIDE I + II are mostly driven to the data belonging to ABIDE I.
This is true either if we restrict our analysis on open eyes patients only or we do not.
In fact, putting together the two dataset does not lead to a great improvement of scores than limiting just on ABIDE I.
It is possible that this trend is due to two main factors: the greater number of ABIDE I patient with respect to those in ABIDE II left after all the cuts on covariates, and the presence of a greater biological variability of subjects in ABIDE II dataset.
%in ABIDE I of a great site: NYU containing a great percentage of patient which are also balanced between controls and ASD.
%This could monopolise the results since a great percentage of train and test data would belong to this center leading to a lower variability due to site-related features as demonstrated in studies like \cite{spera-2019}.

\paragraph{Comparison between a deep neural network and a random forest classifier}

\hfill

\noindent It is always a good practice to compare results obtained with a complex model, with the ones obtained with a simpler machine learning classifier.
Instead of immediately opting for a complex model, simpler methods should be tried to establish a baseline and to allow a meaningful comparison.

The principle of Occam's razor, when applied to machine learning, demands that if two model perform quite the same, the simpler of the two should be picked \cite{domingos-1999}.
This is true especially for our data, since we start from a situation of overfitting and there is no need to choose a complex model a priori if this choice is not supported by data.
This is the same principle that lead us during the choice of the best structure and the best number of layers and neurons of the deep neural network.

Comparing the results obtained by the deep model with those obtained by a random forest, (table \ref{tab:comparison_dnn_rf_pearson} and \ref{tab:comparison_dnn_rf_win}) it is possible to deduce that a deep model is more adapt to find characteristic patterns between controls and ASD than a simpler random forest classifier.
Scores obtained with a deep model are systematically much higher than those obtained with a random forest classifier.
The only exception is with data classified with the upstream harmonization pipeline.
However, as explained in the paragraph below, this procedure leads to biased data and as a consequence results obtained with this pipeline are not reliable.
We are still going to use Random Forest classifier in some of the remaining anslysis just as a comparison, even though we assert that a deep model is more suitable to understand differences between the two classes of interest.

\paragraph{Effect of different harmonization pipelines} 

\hfill 

\noindent An other common trend that stands out from all the analysis is the systematic improvement of AUC score when we use data harmonized with the pipeline of upstream harmonization (explained in section \ref{chap:analysis_workflow} and outlined on flowchart in figure \ref{fig:harmon_upstream_flowchart}).
This improvement disappears when we implement harmonization inside the k-fold cross validation procedure (sketched in flowchart \ref{fig:harmon_kfold_flowchart}).
These two implementations differ on the order by which harmonization is implemented: with upstream harmonization we harmonize the whole dataset and later we split it into train and test to run the cross validation procedure.
Conversely, with harmonization implemented inside the cross validation procedure, we run harmonization only after the division of the dataset into a train and test subsets.
We suspect that these higher results are driven by a misleading way to proceed.
Classification results are affected by a bias due to data leakage which occur if we implement harmonization upstream.
It is also possible that some good results on other similar studies, obtained with harmonization implemented in this same way such as \cite{ingalhalikar-2021} may be due to a similar data leakage as we have in this analysis.

As a general definition, we have data leakage when information outside the training set is used to create and train models. This additional information allows the model to know something more about the data that otherwise it would have not known.
This leads to an improvement of the results and an overestimation of its performances.

In this case we have data leakage when we harmonize the entire dataset before splitting it into train and test, becaus, when we use train data to train the model, they already contain information about test data.
In fact, harmonized train and test data have been created using covariates, features and other information belonging to the whole dataset.
We can then assert that the right way to implement harmonization is inside the k-fold cross validation. 
In a more general case, if we are not running a cross validation procedure, it is important to implement harmonization of data after the splitting of the dataset into a train and a test subset.
In this way, we can create the harmonization model only on control subjects belonging to train dataset.
Thus, when the model is trained with this dataset, it is not biased by external information from the test data.
We can then harmonize data belonging to test by applying the harmonization model with parameters estimated using only the train dataset.

%For this reason, from now on, when we refer to \textquotedblleft harmonized data \textquotedblright, we are referring to the procedure according to which data are harmonized inside the k-fold CV.

\hfill

Comparing results of harmonized data with results on raw data we don't notice any particular improvements. 
As we seen in chapter \ref{chap:harmonization_results} harmonization is an effective strategy to reduce site-related features, but it has no particular effect on classification controls/ASDs.

This is likely due to the weak information contained within our data. 
Distinction between healthy and ASD subjects based only on functional connectivity data is not a straigthforward task, and it is possible that these results are the best it is possible to achieve with these data. 
%Removing site-related information from our data does not bring relevant improvements because this may be all the information we can retrieve from these data. 
%With harmonization we can just clean data but we are not able to bring out new information from them.
However harmonization procedure can become a useful tool to remove some noise from data and obtain a cleaner assessment of what feature are the most discriminative between controls and ASD. This theme is entirely addressed in chapter \ref{chap:shap_results} talking about feature importance.


\paragraph{Results with adversarial}

\hfill

\noindent Looking at tables \ref{tab:classification_pearson} and \ref{tab:classification_wavelet} we notice that the adversarial model does not bring significative advantages in classification scores. 
AUC score obtained with this network are on average the same as obtained with other harmonization pipelines (except for the upstream harmonization pipeline that as we stated before, leads to biased results).
However, a possible drawback of using this type of network with these data, is the possible confusion introduced by dealing with a huge number of sites.
So far, in different papers \cite{ganin2016}, \cite{kamath2019} or \cite{guan2021},
adversarial networks are employed with only two domains, which can be considered as the equivalent of our sites. 
In our data we deal with 36 sites when we consider ABIDE I+II, or, best case scenario, about 15 sites when we reduce to ABIDE I or II with just open eyes.
This way even if the site-predictor branch is encouraged to learn site-distinctive traits, it is not always able to distinguish between 36 sites, so this can cause confusion during backpropagation on the update of weights related to the feature extractor branch. 

For this reason it is possible that when dealing with more than two domains (sites) this implementation of an adversarial network does not bring advantages in classification problems.



%Looking at tables \ref{tab:classification_pearson} we notice that the adversarial model perform slightly worse working on Pearson-based coefficients while for wavelet coefficients (tables \ref{tab:classification_wavelet} we obtain on average the same values of AUC from DNN and adversarial network.

%A reason for this is the possible confusion in weight updates introduced by the adversarial network that sometimes may generate noise between weights of the feature extraction branch. 
%So far, in different papers \cite{ganin2016}, \cite{kamath2019} or \cite{guan2021},
%(where, however adversarial is introduced just by combining losses), 
%adversarial networks are employed with only two domains, which can be considered as the equivalent of our sites. 
%In our data we deal with 36 sites when we consider ABIDE I+II, or, best case scenario, about 15 sites when we reduce to ABIDE I or II with just open eyes.
%This way even if the site-predictor branch is encouraged to learn site-distinctive traits, it is not always able to distinguish between 36 sites, so this can cause confusion during backpropagation on weights of the feature extractor branch. 
%This noise introduced by this process may be the cause of this slight worsening of classification performances.
%For this reason it is possible that when dealing with more than two domains (sites) this implementation of an adversarial network is not the most suitable way to proceed to remove site-related patterns from data.


%A possible reason, in light of the results obtained with harmonization of wavelet coefficients which is not able to make sites-related feature unrecognisable, is that when computing wavelet coefficients, linear site-dependent relationship is lost and maybe a non linear relation is created.
%Same thing does not happen with pearson coefficients where linear relations remain. harmonization is able to remove linear relations on pearson but not on wavelets, and a classifier is more likely to get these non-linear relations with site and perform better on wavelets.


\paragraph{Effect of dimensionality reduction}

\hfill

\noindent The same alertness we paid for the implementation of harmonization was applied when extracting PCA. 
As mentioned in the overview chapter \ref{chap:analysis_workflow}, it is possible to make the mistake of computing principal components on the whole dataset, before its separation into train and test sets.
For this reason we implemented PCA extracting PCs from the training dataset and applying the transformation to both the train and test datasets.

An other aspect that leaps out comparing results with PCA with results without it, is a light improvement of performances of the adversarial learning with respect to the other harmonization pipelines.
An explanation to this is the reduced source of noise and, consequently, of error deriving from dimensionality reduction, that leads to a reduced variability in site-related features.
This makes the confounding feature more recognizable, since they are no longer affected by noise. 
Reducing the dimensions of data then, could lead to a smoother weight updates that does not affect classification weights as much as it does with the whole features data.

What arises from this analysis, is that PCA represents an important strategy to tackle the problem of dimensionality.
It is plausible that among all the 5995 features, a big percentage of them are uninformative for control/ASD classification and they just increase dimensions of data without any benefit, or worse, just adding noise.
This is linked to a biological reason: we can in fact imagine that there are some brain areas that just don't significantly change their activity between healthy and ASD people.
Following this explaination, all the coefficients representing a link between them are just similar between the two groups and don't bring any benefit in this discrimination.
For this reason, reducing the source of noise due to this data-points leads to the creation of an equally informative dataset, but with a reduced source of error.

On the other hand, the reduction of dimensionality with PCA has a non-negligible downside in this work.
By reducing dimensions of data, we lose any information about the feature and the biological connection they represent.
Each array containing 5995 feature is projected into a subspace of 800 or lower dimension. 
This leads to the loss of biological information of each feature which is not possible to retrieve if we aim to study the altered connection identified by a machine learning model, as we did in the following chapter.  
For this reason, to extract relevant features, linked to relevant altered functional connection, we need to work on the higher dimensional space of the whole set of features.

\newpage


\chapter{SHAP - Implementation and results}\label{chap:shap_results}

In this chapter we present and discuss the results obtained by implementing a feature explainatory model using the DeepSHAP algorithm described in chapter \ref{chap:shap}.
We also compared them with a more conventional random forest feature importance assessment.

Here we present a brief summary of all the analysis we performed using feature importance information.

We choose to run this analysis only on Pearson correlation coefficients since they gave the best classification results, which means achieving the best separability between controls subjects and ASD patients.
We used the whole dataset consisting of ABIDE I + ABIDE II patients with the same characteristics as we choose for classification: only males and with an age range of 5-40.
In addition we put in appendix some results obtained on the dataset ABIDE I only open eye.

 
In this section we are looking for what are the most important features that guide the prediction of a machine learning model.
These features are representative of altered functional connections between brain areas.
From the study of altered connections we are able to identify biological areas relevant for the distinction between healthy and ASD subjects.

%We are creating a gerarchy of features from the most to the less important in order to analyze if
What we are looking for in this section is whether there are some features whose contribution was greater than others during the distinction between controls and ASD data.
We check if these features are recurrent, regardless the type of analysis we are carrying out, or the machine learning classifier we use.
%for the most important features computed on the entire test dataset and even if for each test data they contributed poorly, there is a certain gerarchy of importance between them.

As a first, introductory analysis, in section \ref{sec:feature_importance_dnn} we start analyzing the most important feature that lead the classification of the DNNs models, using DeepSHAP.
We extract the most important features for each of the harmonization pipeline discussed in section \ref{chap:analysis_workflow}.
Our goal is to check whether the presence of harmonization, implemented by different strategies, significantly alters the contribution of features in the output of the model.
In section \ref{sec:feature_importance_rf}, following the same harmonization pipelines, we extract important features from a random forest classifier. 
For this simpler machine learning model: a random forest classifier, feature extraction is implemented by the use of the feature\_importances\_ method provided by sklearn.

%In these analysis our goal is to understand if a different machine learning algorithm gives results relying on different features than a deep network model.
%We also check the impact of harmonization just as we did with the deep models
%We start asking what are the most important feature for each of the analysis we discussed in section \ref{chap:analysis_workflow} both with the DNNs and with the Random Forest classifier.
This is, however, just a first, preliminary inspection of important features, and for this reason we limit our focus to the first 20 important feature for each pipeline.
We choose to limit to this number because we can easily visualize them and have a visual comparison to identify differences across the different analysis.

A more meaningful analysis, is carried out afterwards.
We select just 5 pipelines among all the one considered earlier, because we regard at these 5 as the most significant ways to proceed.
In section \ref{sec:feature_importance_5} we will explain more in details  what these analysis are and why we excluded the others and we will present results on common important features between these 5 analysis.
From them we examine the biological meaning of these feature and the brain areas they involve, to create an histogram of the most involved areas in discrimination of healthy and ASD subjects.
%if these most important features are the same across the different pipelines we followed or if each pipeline  more specifically we want to check if before and after the harmonization procedure, the most important features still remain the same or if they change depending on the kind of analysis we are carrying out.
%We compared important features extracted with SHAP (related to the DNNs) with those important for a Random Forest classifier to evaluate if there are feature that stand out regardless the algorithm used to compute them since we used SHAP to explain feature from the DNN and the feature\_importance class provided by scikit learn to assess feature in a random forest classifier.

\hfill

\paragraph{Operate with the DeepSHAP algorithm}\hfill

\noindent To instantiate the desired class of SHAP called \emph{DeepExplainer}, we require the trained deep model and a fraction of the training data to use as background.
We can choose as many background data as we want, keeping in mind that the bigger is this background subset $N_{bkg}$, the more accurate is the computing of Shapley values, but the more computationally expansive this process will be.
In particular using a big amount of background data, would occupy a vast amount of RAM memory and eventually run out of it.
However, since the error we make on Shapley values linked to this choice is $\sim 1/\sqrt{N_{bkg}}$, a background dataset consisting of 100 samples is already a good compromise to have a relatively small error.
We choose though, a background size of 500 samples for the entire dataset and a size of 100 samples when working on ABIDE I only open eyes, since we had a reduced number of train data to collect background data from.

Once the explainer class is created, we can input as many test dataset as we want to explain, and for each one of them, the explainer outputs an array containing a Shapley coefficient for all the n\_features of each test data.
Once inputted a batch of test data, we obtain a matrix of the same size of the test dataset we used, shaped n\_samples x n\_features, containing all the Shapley values for each feature of each test data.
We choose to input all the test datasets to obtain a more statistically accurate estimate of feature importance.
Since we are presenting our results following a k-fold cross validation scheme, we implemented this procedure inside the k-fold CV.
To do so and get a final and general result, we computed the Shapley values of the test set of each fold and collected them. At the end of the k-fold CV, we obtain a matrix containing Shapley values for each sample of the whole dataset.
%for all the test samples of each k-fold and we collect them to use later.
%At the end of the CV, we have a matrix of Shapley values and the related dataset of test data they are computed on.
We can thus proceed to visualize the results.

\paragraph{Different types of SHAP plots} \hfill

\noindent For each test data we can visualize the contribution of each feature on pushing or pulling the predicted output from the baseline output of our model.
%but to produce this kind of plot we need to save the test datasets as well.
An example is shown in figure \ref{fig:shap_waterfall}.
This type of plot, called \emph{waterfall plot}, shows the result of the most important features on a single test data.
We have the baseline value of the model in the lower right corner and on the top left the output of the model with this test subject.
We recall that the baseline value (or reference value) is the average output of the model, computed using the background dataset chosen between train data.
When a test sample is given as input to the model, a prediction is made.
During the prediction, each feature of this sample acts by pushing or pulling the predicted value towards greater values (positive contribution) or lower ones (negative contribution) with respect the baseline value.
The contribution of each feature of this sample, to the output, can be visualized with this plot.
Positive Shapley values, representing positive contribution to our output are colored red and negative ones are blue.
Even if this is just an example taken from a single test data, it is representative of all the analysis where there is not a feature whose contribution is way greater than the others.
In fact each feature makes a small contribution to the final outcome, but the sum of them push the model towards an output.


\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{shap/waterfall_plot}
\caption{Example of a waterfall plot obtained with a single test data. For each row is displayed the feature name and value and its contribution to the final output: negative contribution are colored blue and positive ones red. It is also marked the reference output in the bottom right corner, and the output of this instance on the top left.
}\label{fig:shap_waterfall}
\end{figure}


\hfill

If we want look at the overall result on all the test data, it would be inconvenient to create a plot like \ref{fig:shap_waterfall} for each data. For this reason we have to look at a summary plot called \emph{force plot}.

The force plot of feature importance can involve either the module of Shapley values or the Shapley values themselves (positive and negative), an example of the latter is figure \ref{fig:shap_dot_kfold}: this plot shows for each instance (for each sample of the test data)
%\footnote{https://shap.readthedocs.io/en/latest/example\_notebooks/tabular\_examples/tree\_based\_models/Scatter\%20Density\%20vs.\%20Violin\%20Plot\%20Comparison.html},
\footnote{https://shap.readthedocs.io}
the contribution of one particular feature to the output computed with that test data.
In other words, for each test data, we have an array of Shapley values: one for each feature, and these values (for each feature) are represented by a dot, placed to the right or to the left of the thin black line corresponding to a value of zero. 
A zero value indicates that for a certain test data, that corresponding feature did not contributed to the output.
This position in respect to the black vertical line, indicates the sign of that contribution: on the right we found positive coefficients that gave a positive contribution pushing the output towards values greater than the reference output, and on the left are placed those that had a negative contribution.
Just to be clear, positive and negative adjectives are to be intended in a mathematical sense, as greater or smaller than zero, furthermore, pushing the output towards greater values (closer to 1 than to 0), means that the input data is being classified as ASD if ASD is associated to a label 1.
This process of placing Shapley values for each feature in a plot, is repeated for each test data to obtain a scatter plot of Shapley values, where the related features are ordered by importance.
Each spot is also colored red or blue. 
The color indicates, for each sample, the magnitude of the feature calculated with respect to the mean value of the feature across all the test dataset. 
With this additional information we are able to understand if a great or a small value of a feature pushes the model towards one output or towards the opposite (0 or 1).
As a practical example to understand the concept of colors, with our data, they are useful if we ask \say{is a strong or a weak correlation between two areas, that contributed the most to the prediction of my model towards high values (and then more close to the label associated with ASD)?}.


% as a single colored spot, the color indicated the strength of the contribution of that feature to the final outcome, and
However, for our data, this kind of plot, is not the most suitable for readability because of the large amount of features and the small contribution of each feature to the final outcome, in respect to other features.
This results in a smoothed scatter plot for each feature, containing many but unnecessary details which make this kind of plot not the most clear to assess how much a feature is important in an absolute sense for a model.

Alternatively, we can visualize the total amount of the contribution of each feature, by considering the absolute value of the Shapley values. In figure \ref{fig:shap_bar_kfold} a bar plot representing of all the first 20 important features is reported.
Each bar represents a feature importance, computed from equation \ref{eq:shap_magnitude}.

For a better readability, we prefer reporting the results in terms of absolute Shapley value, where we just consider their module, regardless its positive or negative contribution to the output.
This is a clearer way just to understand the absolute contribution of a single feature to a given analysis.

From this plot, we already have a more immediate understanding of what the most important features are.
As we noticed from the example in figure \ref{fig:shap_waterfall}, there is not a standing out feature, or group of features that have a contribution way greater than the others, but in fact feature importance has a smoothed descending trend.
%An example to visualize this trend is shown in figures representing the first 20 most important features extracted from DNN trained on data harmonized inside k-fold CV.

\begin{figure}[h!]
\centering
\begin{subfigure}[t]{.45\linewidth}
   \fbox{\includegraphics[width=1\linewidth]{shap/ab_all/shap_dot_kfold}}
   \caption{Plot of dotted Shapley values}
   \label{fig:shap_dot_kfold}
\end{subfigure}
\hspace{3mm}
\begin{subfigure}[t]{.439\linewidth}
  \fbox{ \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_kfold}}
   \caption{Bar plot of Shapley values}
   \label{fig:shap_bar_kfold}
\end{subfigure}
\caption{Two different but closely related force plots of Shapley values regarding the first 20 most important features obtained from data where harmonization procedure is implemented inside the k-fold CV scheme.
In plot \ref{fig:shap_dot_kfold}, for each feature each instance (each test sample) is represented by a dot, the position indicates a positive or negative contribution to the output, while the color represents the magnitude of the feature, compared with the average value of that feature across all the test dataset.
In figure \ref{fig:shap_bar_kfold} for each feature is represented the mean absolute value of all the Shapley values computed for each instance.
}
\label{fig:shap_features_kfold_both}
\end{figure}




\newpage
\section{Plots of the most relevant features for DNNs and random forest}
\subsection{Feature importance for DNNs}\label{sec:feature_importance_dnn} 

In figure \ref{fig:shap_abide_all} we show the first twenty most important features for the DNNs, obtained from each harmonization pipeline described in section \ref{chap:analysis_workflow}.
From this plot, we can notice that some features seem to be persistent through all the four pipelines.


We find that: feature 592 and 1521 appear, even if in different order of importance, in all our four analysis.
%(We replicated the plot in figure \ref{fig:shap_bar_kfold} putting it again in figure \ref{fig:shap_bar_kfold_} for a more immediate visual comparison of all the twenty important features for each analysis).
If we consider only common features between harmonization in k-fold, harmonization upstream and raw data, two more features appear: 1513 and 3334.
We run this latter analysis excluding the adversarial network to allow comparison with results of the next section \ref{sec:feature_importance_rf}, extracted from random forest.

Since each feature is representative of a correlation between two brain areas, we report here the corresponding ares involved:
\begin{itemize}
\item Feature 592: correlation between Right Middle Temporal Gyrus (anterior division) and Left Superior Temporal Gyrus (anterior division)
\item Feature 1521: correlation between Left Angular Gyrus  and Right Middle Temporal Gyrus (posterior division)
\item Feature 1513: correlation between Left Angular Gyrus  and Right Temporal Pole
\item Feature 3334: correlation between Right Parahippocampal Gyrus (posterior division) and Right Accumbens

\end{itemize}


\begin{figure}[h!]
\centering
\begin{subfigure}[c]{.45\linewidth}
  \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_kfold}
   \caption{Impo}
   \label{fig:shap_bar_kfold_}
\end{subfigure}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_upstream}
   \caption{Important features extracted from harmonized data, with harmonization procedure implemented \textbf{upstream}, before the k-fold}
     \label{fig:shap_abide_all_up}
\end{subfigure}
\hspace{3mm}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_no}
   \caption{Important features extracted from \textbf{raw}, not-harmonized data}
     \label{fig:shap_abide_all_no}
\end{subfigure}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_adv}
   \caption{Important features extracted from raw data using the \textbf{Adversarial} neural network}
   \label{fig:shap_abide_all_adv}
\end{subfigure}
\caption{First twenty most important features plotted with a bar plot of mean absolute Shapley values. Extracted from each of the four harmonization pipeline with DNN and adversarial network.
Figure \ref{fig:shap_bar_kfold_} shows the important features extracted from harmonized data, with harmonization procedure implemented inside \textbf{k-fold} CV.
Figure \ref{fig:shap_abide_all_up}shows the important features extracted from harmonized data, with harmonization procedure implemented \textbf{upstream}, before the k-fold.
Figure \ref{fig:shap_abide_all_no} shows important features extracted from \textbf{raw}, not-harmonized data.
Figure \ref{fig:shap_abide_all_adv} shows important features extracted from raw data using the \textbf{Adversarial} neural network
}
\label{fig:shap_abide_all} 

\end{figure}





\subsection{Feature importance for random forest}\label{sec:feature_importance_rf}
%they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree \footnote{https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html}
In this section we repeat the same analysis we obtained with a DNN in figure \ref{fig:shap_abide_all}, with a random forest classifier.
We plot the first 20 most important features obtained from the three main pipelines of harmonization in k-fold, harmonization upstream, and no harmonization.
Features are extracted using the function \emph{feature\_importance\_} provided by scikit-learn library.
Results are shown in figure \ref{fig:shap_abide_rf_all}. 


\begin{figure}
\centering
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/rf_feature_importance_kfold}
   \caption{Important features extracted from harmonized data with harmonization implemented inside \textbf{k-fold} CV.}
    
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/rf_feature_importance_upstream}
   \caption{Important features extracted from harmonized data with harmonization procedure implemented \textbf{upstream}, before the k-fold.}
    
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/rf_feature_importance_no}
   \caption{Important features extracted from raw data.}
\end{subfigure}
\caption{Feature importance obtained with a Random Forest classifier trained on the entire (ABIDE I + ABIDE II) dataset.}
\label{fig:shap_abide_rf_all}
\end{figure}


From the three plots we notice that 7 different features are common to the three analysis: feature 710, 1127, 1703, 748, 2735, 1400 and 2811.
As we did for DNNs we report the brain areas involved in these correlations:
\begin{itemize}
\item Feature 710: correlation between Right Middle Temporal Gyrus (temporooccipital part) and Right Thalamus
\item Feature 1127: correlation between Left Postcentral Gyrus  and Right Postcentral Gyrus
\item Feature 1703: correlation between Right Lateral Occipital Cortex (inferior division) and Right Supramarginal Gyrus (anterior division)
\item Feature 748: correlation between Left Middle Temporal Gyrus (temporooccipital part) and Right Thalamus
\item Feature 2735: correlation between Right Precuneous Cortex  and Right Middle Temporal Gyrus (anterior division)
\item Feature 1400: correlation between Left Supramarginal Gyrus (posterior division) and Right Inferior Frontal Gyrus (pars triangularis)
\item Feature 2811: correlation between Left Precuneous Cortex  and Right Middle Temporal Gyrus (posterior division)
\end{itemize}


\subsection{Comments on feature importance}
Comparing results obtained using different classifiers: a DNN and a random forest classifier, we notice from plots \ref{fig:shap_abide_all} and \ref{fig:shap_abide_rf_all} that there are no common features among the first 20. 
This is most likely due to the difference in the inner algorithm that guides the decisional process. 
A DNN and a random forest have very different modes of operation. 
This can lead the two of them to find different patterns within data whose results do not (or just partially) overlap.
We are prone to consider the DNN most reliable than the random forest because of the higher classification performance achieved.
However, as we can notice from plots, random forest feature importance is able to determine some feature that stand out over the others, resulting in an abrupt descending trend of importance.
The same characteristic is not visible with features extracted from the DNNs except for the pipeline of harmonization inside k-fold where it seems that some feature have a greater contribution than others and the descending in feature importance is less smooth.
A possible reason for this trend, in accordance of what we assumed during the discussion of classification results, is that the harmonization procedure, even if does not bring up new information, helps in data cleaning, and remove some noise due to site-distinctive patterns.
Harmonization thus helps to obtain a better and a more defined assessment of what the most important features are, in discrimination of controls/ASD.

Even if there are no overlapping features between analysis with DNN and with random forest, if we take a look at the lists of the brain areas involved in these correlations, we can notice that certain brain areas, such as the temporal gyri, are in common between these two results.
This can suggest that even if the feature itself does not determine in the same way the output of these two models, maybe some brain areas present in a feature, are involved more than others.
The following analysis aims to study this aspect and determine what are the most important brain areas that allow a discrimination between healthy and ASD subjects.

\newpage
\section{Cross analysis of important features}\label{sec:feature_importance_5}

So far, we discussed results linked to images just to have a visual comparison of feature importance between DNN and random forest.
Now we go deeper to fulfill a more meaningful analysis of what are the important features and what they represent.

Since, as explained in section \ref{sec:classification_discussion}, the pipeline of upstream harmonization is a misleading way to proceed because it creates a bias in results, from now on, we exclude it from our analysis.
In this section, when we refer to harmonized data, we are referring to data with harmonization procedure implemented inside k-fold.

To assess what are the most recurrent features between these different types of analysis, we choose to limit our analysis to the 1\% important features among 5995.
To this end we collected the first 60 most important features and we limited our analysis to just the 5 classification procedures listed below:
%These are the most significant analysis and from them, we can extract non-biased results.


\begin{enumerate}
\item Classification of harmonized data using the DNN \label{proc:dnn_kfold}
\item Classification of harmonized data using the random forest classifier \label{proc:rf_kfold}
\item Classification of raw data using the DNN \label{proc:dnn_no}
\item Classification of raw data using the random forest classifier \label{proc:rf_no}
\item Classification using raw data with the adversarial neural network \label{proc:adv}
\end{enumerate}



Firstly we checked if and how many common features we find among the 60 most important features between these five classification.
However, there are no common features between all these 5 pipelines taking into account only the first 60.
%If we exclude, though, from our analysis the adversarial learning classification we find that features 1513, 3058, 1067 are common between the remaining classification methods.
%They represent:
%\begin{itemize}
%\item Feature 1513: correlation between Left Angular Gyrus  and Right Temporal Pole
%\item Feature 3058: correlation between Right Frontal Orbital Cortex  and Left Angular Gyrus
%\item Feature 1067: correlation between Right Postcentral Gyrus  and Right Superior Temporal Gyrus (posterior division)
%\end{itemize}

Now, we carry out the analysis on subgroups of classification methods and we report the number of common important features among the 60 most relevant one as a number and as percentage.
This kind of analysis is useful for example to assess whether the harmonization procedure significantly changes important features.
It is also aimed to quantify the common features between a DNN and a random forest classifier.

\begin{itemize}
\item Focusing just on \textbf{DNN}, we compared pipelines \ref{proc:dnn_kfold} and \ref{proc:dnn_no} finding that 22 features (37 \%) out of 60 are common between harmonized and raw data.

\item Focusing just on \textbf{Random Forest} classifier, we compared pipelines \ref{proc:rf_kfold} and \ref{proc:rf_no} finding that 33 features out of 60 are common (55\%) between harmonized and raw data.

\item Comparing results on  \textbf{harmonized data} obtained with \textbf{DNN} (pipeline \ref{proc:rf_kfold}) and \textbf{random forest} (\ref{proc:dnn_kfold}) we obtain 13 common features (22 \%)

\item Comparing results on \textbf{raw data} obtained with \textbf{DNN} (pipeline \ref{proc:dnn_no}) and \textbf{random forest} (\ref{proc:rf_no}) we obtain 7 common features  (12\%)

\item Comparing results of the \textbf{Adversarial network} (\ref{proc:adv}) with the \textbf{harmonized data} with DNN (\ref{proc:dnn_kfold}) we obtain 21 common features (35\%)

\item Comparing results of the \textbf{Adversarial network} (\ref{proc:adv}) with DNN classification of \textbf{raw data} (\ref{proc:dnn_no}) we obtain 28 common features (47\%)
\end{itemize}



\section{Brain areas related to important features}

As mentioned before, each feature is representative of a correlation between two brain areas, and, since for brain parcellation we used the Harvard-Oxford atlas with 110 ROIs, each brain area is involved in 109 features.
In this section we investigate what are the most recurrent brain areas in healthy/ASD discrimination.


For this purpose, we plot an histogram to represent the number of occurrences of each brain area among the first 60 features for each classification procedure, and subsequently we create the histogram of the overall important brain areas pooling all of these results into a single histogram.

Figure \ref{fig:histograms_60} shows for each pipeline, what are the brain areas recurrent among the 60 most important feature.
In figure \ref{fig:important_areas_features_all} the overall histogram comprising all the results is shown.


\begin{figure}
\centering
\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_no}
   \caption{Important areas from Raw data classified with DNN}
    
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_adv}
   \caption{Important areas from Raw data classified with Adversarial network}
    
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_kfold}
   \caption{Important areas from harmonized data classified with DNN}
    
\end{subfigure}
\end{figure}
\begin{figure}\ContinuedFloat
%\medskip
\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_rf_kfold}
   \caption{Important areas from harmonized data classified with Random Forest}
    
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_rf_no}
   \caption{Important areas from Raw data classified with Random Forest}
    
\end{subfigure}
\caption{Histograms of important brain areas extracted from the first 60 important features of different classification procedures}
\label{fig:histograms_60}
\end{figure}


It is immediate to notice that there are recurrent brain areas across all the analysis.
In figure \ref{fig:important_areas_features_all} the overall histogram of the most recurrent brain areas between these common feature is reported.

\begin{figure}[h]
\centering
  \includegraphics[angle = 270, origin = c, width=0.75\linewidth]{shap/ab_all/important_areas_features_all}
\caption{Histogram of the most important brain areas putting together results obtained from all the five analysis listed above.}
\label{fig:important_areas_features_all}
\end{figure}

\section{Discussion on common features and important brain areas}

\paragraph{Common important features}

\hfill

\noindent By checking the common features between different machine learning model and harmonization procedure, we can have a better comparison on how much impact these factor have on the assessment of feature importance.

Looking at the results listed in section \ref{sec:feature_importance_5} we can assert that DNN and random forest definitely have different ways to use features to make prediction and to assess their importance.
We find that just a low percentage (12\% and 22\%) of features are common between a DNN and a random forest for both raw and harmonized data.
This has a non-negligible impact when choosing the best classificator for any classification task.
An important aspect to keep in mind is that there is not \say{the best} classifier in absolute, but each classifier is able to find different patterns among data.
So the best classifier depends on the specific dataset and the classification task.

We can also state that harmonization procedure changes, especially in a DNN, the most important features.
As we pointed out when discussing the plots of feature importance, harmonization is able to remove noise from data and reveals in a cleaner way what features are really important for the network, without the contribution of site-related noise.
It is possible that the number of common feature in a random forest between harmonized and non harmonized data is bigger because random forest is a simpler algorithm and it is no able to find complex relations between data.
In fact a DNN is able to learn more complex pattern, and in doing so, it is more sensitive to noise and to the discovery of more subtle pattern that can drive to this difference in feature ranking.
This finer searching for patterns, though, is what lead a DNN to achieve better classification performances with respect to a random forest.
Thus, harmonization changes the most important features, but making it more reliable, since features are less affected by noise and results are more meaningful.

This change, resulting from a better definition of feature due to harmonization, also appears when comparing results with the adversarial network.
In fact we observe that the number of common feature between adversarial network and raw data is similar to the number between harmonized and raw data determined using the DNN.
The reason for this lies in the mechanism of the adversarial network: it results in a reduction of some site-related noise thanks to the adversarial branch, but at the same time, the introduction of some confusion due to the flawed definition of the correct site.

Between raw data and adversarial network a similarity of feature importance $\approx 47\%$ that is greater than what we obtained with raw data and harmonized data $\approx 37\%$.
%Even if between raw data and adversarial network we should have obtained a result similar to raw data and harmonized data ($\approx 37\%$), we obtain a greater similarity of feature importance ($\approx 47\%$).
This means that with the adversarial network data are modified and harmonized according to some inner processes, which occur in a different way than the analytical harmonization.

\paragraph{Important areas} \hfill

From the analysis of important brain areas, especially looking at figure \ref{fig:important_areas_features_all} we can look at $\approx 30$ among 110 areas that contributed the most in discrimination between controls and ASD.
We compared the brain areas extracted from these analysis with the brain areas obtained by Saponaro et al. \cite{saponaro2022}.
They studied with a random forest classifier the most discriminative features extracted from structural images of patients from the ABIDE dataset.
For parcellation they used the atlas Desikan-Killiany-Tourville implemented in Freesurfer \footnote{https://surfer.nmr.mgh.harvard.edu/} consisting on 62 total ROIs.
This number differs from the number of ROIs implemented in Harvard-Oxford, equal to 110.
For this reason for some feature, we can compare the two studies only identifying the main lobes involved in them both.
In the following lines, we present the brain areas found in common between our analysis and Saponaro's paper. 
We also provide a brief explanation about the role of different brain regions and how they could be linked to the development of ASD.
Information about brain areas are taken from the Wikipedia page of the areas we focus on.
The scope of this short investigation is not to achieve a medical accurate analysis but just to give some glimpses of the function of anatomical regions found in common from the structural and the functional analysis.
Areas concerning the right superior temporal, right middle temporal and right inferior temporal gyri, are among the most interesting regions.
They belong to the temporal lobe, which is involved in processes like language comprehension and emotion recognition.
In particular the r. superior temporal gyrus has been identified as a crucial area for social cognition.

Left angular and left supermarginal gyri belong to the inferior parietal lobule.
This lobule is generally involved in perception of emotions and interpretation of sensory information, however, these two structures are particularly involved in language processes and mathematical operations.
Regions like the right orbitofrontal and the right accumbens are both involved in rewards related to decision making.
The right nucleus accumbens also concerns themes like motivation, aversion and the feeling of pleasure subsequent to a reward.
The left lingual gyrus is related to the processing of visual letters, and plays and important role on the analysis of logical conditions as well.
For other regions like the precuneous cortex is still quite unclear what processes they are mainly involved in.
All these regions, and many others, such as the thalamus, which is associated to processes like the regulation of consciousness and the processing of sensory signals, are involved in so many processes that reducing them to a singular function linked to ASD is too simplistic.


%It is however recognizable a link between dissimilarity in structural, anatomical region and altered functional connectivity involving those regions.

\addcontentsline{toc}{chapter}{Conclusions}
\chapter{Conclusions and future applications}

%open-access
\section*{Recap}

In this thesis we focused on the study of functional brain connectivity using machine learning and deep learning techniques.
We applied our analysis to the study of Autism Spectrum Disorders (ASD) using data extracted from functional MRI scans.
ASD is a challenging issue because of its social impact on the quality of life of people affected by it and since so far, no univocal biomarkers were identified to its diagnose.
In fact, studies on this subject still report discordant results due to the broad range of disorders ASD manifests in.

In this work we tackled this problem by using resting-state functional MRI (rs-fMRI) data provided by the ABIDE initiative.
We dealt with rs-fMRI raw data from the preprocessing of images to the extraction of a coefficient to measure the functional connectivity.
Raw images were preprocessed using the open-source software C-PAC, specifically created for the preprocessing of rs fMRI data.
C-PAC allows to perform the main preprocessing steps and extract timeseries from different brain areas.
Brain areas are delineated by a parcellation of the brain defined by an atlas.
From the analysis of timeseries extracted from different areas, it is possible to obtain a measure of correlations and create a functional connectivity (FC) matrix for each subject.
To report a measure of the correlation between different brain areas we compared two different approaches.
One is the computation of the linear correlation between timeseries using Pearson correlation. 
The second derives from a time-frequency analysis of signals using wavelet transforms.

%provided by the open-access dataset ABIDE.
Once these two families of coefficients have been obtained for each subject, we tackled the problem of data harmonization.

Raw data are provided by ABIDE which is a multicenter dataset created by pooling together data acquired by different American and European centers over the years.
Dealing with multicenter data can be misleading because of the possible presence of the so-called \emph{batch effect}. 
Data affected by batch effect exhibit characteristic features or distributions related to the acquisition center/procedures.
For this reason, before working with multicenter data to perform statistical analysis, they should be modified using an harmonization procedure.
%The ABIDE dataset provides structural and functional MRI scans of subjects acquired from different American and European centers.
We addressed this issue by employing two different harmonization techniques.
One is analytical, based on ComBat technique implemented with the open source Python package NeuroHarmonize.  
The other was implemented inside a deep neural network through an originally developed adversarial learning model.

Machine learning was employed to perform classification of control subjects vs ASD patients. 
We compared classification performances using Pearson and wavelet coefficients. 
We sought to find which one allowed the best discrimination between controls and ASD, and found out that Pearson coefficients performed better than wavelet ones.

We also assessed the best way to implement harmonization during the training of a machine learning algorithm.

An other issue that arises working on this data is the problem of dimensionality.
We deal with a dataset consisting on less than 1500 datapoints and each one consisting on 5995 features.
Using a dataset like this, machine learning models work in a condition of overfitting. 
We implemented a Principal Component Analysis (PCA) to reduce the number of features for each subject.
PCA allows to effectively reduce the dimensionality of the problem of one order of magnitude without affecting classification performances.


Furthermore, we covered the topic of ML explainability by determining the features that contributed the most to the prediction of a deep neural network.
Since each feature is representative of a correlation between two brain areas, we look for altered connection between controls and ASD, and from a study of these connections, we assessed the brain areas mainly involved.
To this end we implemented SHAP, an algorithm based on game theory to assess the contribution of a feature in the prediction of a ML model.
The algorithm we used is called DeepSHAP and it is specifically designed for deep neural networks. 
It allows to determine an importance coefficient for each feature of each subject.
By collecting coefficients over all the subjects assigned to a test set, we were are able to determine the features, and consequently the altered functional connections that influenced the most the controls vs ASD classification.
We compared the results obtained with SHAP, with the results of the feature importance extracted from a conventional random forest classifier.
Being a random forest a more easily interpretable classifier, we aimed to discover whether or not important features are the same in the deep neural network and in the random forest approaches.

The identification of these altered connections allowed us to assess which brain areas are more involved in the ASD condition. 

\section*{Future developments}

Even if this work employed ASD data, its pipeline are generals and can potentially be applied to all the mental disease and disorders where an altered functional connectivity is suspected.
The use of explainable AI methods represents an effective tool to interpret machine learning model predictions and allows humans to trust more ML methods.
Using explainable AI, it is possible to identify the main brain areas involved in functional connectivity alterations. 
Using these information, we could tackle the problem of dimensionality during classification.
An idea to tackle the problem of dimensionality, could be to focus only on those areas and calculate only the connectivity related to them and use it as input to the ML model.
This would remove a great amount of noise or uninformative data and help a ML model in making more more decisive predictions.

With regard to ASD data, the heterogeneity of conditions this disorder manifests into, makes it hard to define and subsequently, to identify univocal biomarkers.

During the years, diagnostic parameters change, including the broadening of the range of conditions associated to this disorder.

This is one of the main aspects that restrain from achieving high classification performances.
The other aspect involves the dimension of the dataset.
The unprecedented project carried out by the ABIDE initiative for the investigation of ASD is of great importance, but it cannot be considered done yet.
The collection of an increasing amount of data is a crucial point to increase the accuracy of statistical and machine learning models.
%Because of this diversified aspects that characterize ASD, classification performances are still low.
So far, it is not possible to state that the investigation methods employed in this thesis can also represent a diagnostic tool.
Also, all the studies carried out until now, are mainly focused on males because available data are in a greater number. 
In the future, the expansion of the ABIDE dataset may lead to achieving greater classification performances both on males and females subjects.

In conclusion, a promising approach may be the combination of different investigation tools such as fMRI and structural MRI data.
Hopefully, in a not-too-far future, the combination of the two of them, could bring to the development of a non-invasive diagnostic tool for challenging issues such as ASD, schizophrenia and other mental disorders whose etiology still remain unclear. 

%The etiology of these disorder still remain unclear, however, the employment of machine learning and explainable AI techniques, together with the creation of a huge collection of data, could bring to an accurate diagnose. 
%A diagnosis made in the early years with the subsequent creation of an early treatment plan, would finally lead to an improvement in the quality of life of all the children who develop this disorders.

 

\newpage

\addcontentsline{toc}{chapter}{APPENDIX}
\chapter*{Appendix}
\appendix


\section{Other results}


\begin{table}[!htp]\centering
\begin{tabular}{lccc}
\toprule
Structure &k-fold &upstream &no harmonization \\
\midrule
3-2-1 &60$\pm$2 &63$\pm$3 &60$\pm$4 \\
8-8-1 &64$\pm$1 &65$\pm$4 &64$\pm$5 \\
64-8-1 &69$\pm$1 &72$\pm$2 &68$\pm$1 \\
64-32-8-1 &68$\pm$1 &72$\pm$2 &69$\pm$1 \\
128-8-1 &70$\pm$2 &71$\pm$3 &69$\pm$2 \\
128-64-1 &70$\pm$1 &72$\pm$3 &69$\pm$2 \\
%\cellcolor[HTML]{ffff00}264-8-1 &\cellcolor[HTML]{ffff00}70$\pm$2 &\cellcolor[HTML]{ffff00}73$\pm$3 &\cellcolor[HTML]{ffff00}70$\pm$2 \\
\textcolor{ForestGreen}{264-8-1}&\textcolor{ForestGreen}{70$\pm$2} & \textcolor{ForestGreen}{73$\pm$3} & \textcolor{ForestGreen}{70$\pm$2} \\
512-8-1 &71$\pm$2 &73$\pm$2 &70$\pm$2 \\
1024-8-1 &70$\pm$1 &74$\pm$2 &71$\pm$2 \\
1024-32-1 &70$\pm$2 &74$\pm$2 &71$\pm$3 \\
\bottomrule
\end{tabular}
\caption{Different model structures and related performances for the three main analysis we carried out during this entire work. Colored the structure we choose to employ through all our analysis}\label{tab:different_model_structures}
\end{table}

\section{Feature importance results on ABIDE I open eye dataset}

The same tests we did to assess feature importance on ABIDE I + II dataset, we carried out on ABIDE I with only patient with open eye.
In figure \ref{fig:shap_abide_all_ab1} we report the results obtained from SHAP, related to the first twenty most relevant features.
We can notice at a first glance that there are less feature common to all the four analysis:
There are no common features among them that we can find across all these four analysis, if we limit our study on the first twenty.
We have to search among the first thirty features to find something in common, which is still a good procedure since we are dealing with 5995 features and the first 30 are just the 0.5\% of them.

We find that, among the first 30 features, only feature  762 is common to the four analysis, while if we exclude the adversarial network we add feature 1417 and 748




\begin{itemize}
\item Feature 762: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Left Middle Frontal Gyrus
\item Feature 1417: correlation between Left Supramarginal Gyrus ( posterior division) and Left Middle Temporal Gyrus ( temporooccipital part)
\item Feature 748: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus
\end{itemize}



\begin{figure}[h!]
\centering
\begin{subfigure}[c]{.45\linewidth}
  \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_kfold}
   \caption{Bar plot of important features extracted from harmonized data, with harmonization procedure implemented inside \textbf{k-fold} CV}
    
\end{subfigure}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_upstream}
   \caption{Bar plot of important features extracted from harmonized data, with harmonization procedure implemented \textbf{upstream}, before the k-fold}
    
\end{subfigure}
\hspace{3mm}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_no}
   \caption{Bar plot of important features extracted from \textbf{raw}, not-harmonized data}
    
\end{subfigure}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_adv}
   \caption{Bar plot of important features extracted from raw data using the \textbf{Adversarial} neural network,}
    
\end{subfigure}
\caption{Results obtained on ABIDE I only open eye dataset. First twenty most important features plotted with a bar plot of mean absolute shap values. Extracted from each of the four analysis we run with DNNs and Adversarial network. }
\label{fig:shap_abide_all_ab1}
\end{figure}





With a random forest we obtain more coherence between important features: searching among the first 20 features, we find that
are common to the 3 pipelines: 2690, 1127, 2313, 2314, 748, 2712, 2735, 2582, 1400



\begin{itemize}
\item Feature 2690: correlation between Left Cingulate Gyrus ( posterior division) and Right Frontal Medial Cortex (nan)
\item Feature 1127: correlation between Left Postcentral Gyrus (nan) and Right Postcentral Gyrus (nan)
\item Feature 2313: correlation between Right Paracingulate Gyrus (nan) and Left Middle Temporal Gyrus ( anterior division)
\item Feature 2314: correlation between Right Paracingulate Gyrus (nan) and Right Middle Temporal Gyrus ( posterior division)
\item Feature 748: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus (nan)
\item Feature 2712: correlation between Right Precuneous Cortex (nan) and Right Hippocampus (nan)
\item Feature 2735: correlation between Right Precuneous Cortex (nan) and Right Middle Temporal Gyrus ( anterior division)
\item Feature 2582: correlation between Right Cingulate Gyrus ( posterior division) and Right Precentral Gyrus (nan)
\item Feature 1400: correlation between Left Supramarginal Gyrus ( posterior division) and Right Inferior Frontal Gyrus ( pars triangularis)
\end{itemize}


\begin{figure}
\centering
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/rf_feature_importance_no}
   \caption{Non-harmonized}
    
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/rf_feature_importance_upstream}
   \caption{upstream}
    
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/rf_feature_importance_kfold}
   \caption{kfold}
    
\end{subfigure}
\caption{Feature importance obtained from a Random Forest classifier: results on ABIDE I only open eyes.}
\label{fig:rf_feature_importance_ab1}
\end{figure}



The same cross analysis we did on the entire dataset:

\begin{itemize}
\item Focusing on random forest classifier, we compared important features between \ref{proc:rf_kfold} and \ref{proc:rf_no} and found that 34 features out of 60 are common (57\%).
\item Focusing on DNN we compared pipelines \ref{proc:dnn_kfold} and \ref{proc:dnn_no} finding that 19 features (32 \%) out of 60 are common.
\item Comparing the same analysis pipeline with DNN and Random Forest we find that with harmonized data: procedure \ref{proc:rf_kfold} and \ref{proc:dnn_kfold} there are 11 common features (18 \%)
\item Comparing results on raw data obtained with DNN \ref{proc:dnn_no} and Random Forest \ref{proc:rf_no} we obtain 13 common features  (22\%)
\item Comparing the adversarial results \ref{proc:adv} with the harmonized data with DNN \ref{proc:dnn_kfold} we obtain 19 common features (32\%)
\item Comparing the adversarial results \ref{proc:adv} with DNN classification of raw data \ref{proc:dnn_no} we obtain 23 common features (38\%)
\end{itemize}


\begin{figure}
\centering
\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab1_openeye/important_areas_features_no}
   \caption{Important areas from Raw data classified with DNN}
    
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab1_openeye/important_areas_features_adv}
   \caption{Important areas from Raw data classified with Adversarial network}
    
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab1_openeye/important_areas_features_kfold}
   \caption{Important areas from harmonized data classified with DNN}
    
\end{subfigure}
\end{figure}
\begin{figure}\ContinuedFloat
%\medskip
\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab1_openeye/important_areas_features_rf_kfold}
   \caption{Important areas from harmonized data classified with Random Forest}
    
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab1_openeye/important_areas_features_rf_no}
   \caption{Important areas from Raw data classified with Random Forest}
    
\end{subfigure}
\caption{Histograms of important brain areas extracted from the first 60 important features of different classification procedures}
\label{fig:histograms_60_ab1_openeye}
\end{figure}


\begin{figure}[h]
\centering
  \includegraphics[angle = 270, origin = c, width=0.75\linewidth]{shap/ab1_openeye/important_areas_features_all}
\caption{Histogram of the most important brain areas putting together results obtained from all the five analysis listed above.}
\label{fig:important_areas_features_all_ab1_openeye}
\end{figure}

\newpage

%\printbibliography
\bibliographystyle{unsrt}
\bibliography{library.bib}



\end{document}
