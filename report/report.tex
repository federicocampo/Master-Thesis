\documentclass[10pt]{report}
%\documentclass[12pt]{article}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\pagestyle{plain}
\linespread{1}


%\usepackage[margin=1.5cm]{geometry}

\usepackage{graphicx}
\usepackage[font=small,font = sl, labelfont=bf]{caption}
%\usepackage{epstopdf}
\usepackage{float}
%To create colored tables
\usepackage[table,x11names]{xcolor}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{dirtytalk}
\usepackage{mathtools}
\usepackage{hyperref}

\usepackage{booktabs}
\usepackage{makecell}
%To create unnumbered section
\usepackage{blindtext}

% Packager for Spread-LaTeX
\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{soul}% for underlines
%\usepackage[table]{xcolor} % for cell colors
\usepackage{changepage,threeparttable} % for wide tables

\usepackage{comment}
%To write mathematical letters such as R, C etc..
\usepackage{amsfonts}

\usepackage{amsmath}
%To create a list of item in the same row
\usepackage[inline]{enumitem}

%To create a "note" space
\newlist{notes}{enumerate}{1}
\setlist[notes]{label=Note: ,leftmargin=*}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%\newcommand{\virgolette}[1]{``#1''}

\title{Titolo Tesi}
\author{Federico}

\graphicspath{{/home/federico/magistrale/Imagess/}}


\urlstyle{same}
%\hypersetup{
%colorlinks=true,
%linkcolor=black,
%citecolor=blue,
%}



\begin{document}

\begin{titlepage}
\begin{figure}[t]
\centering
\includegraphics[scale=0.55]{cherubino_pant541}
\end{figure}

\begin{center}
	\textbf{University of Pisa \\ Department of Physics E. FERMI\\ Master's degree in physics\\}
	\vspace{20mm}
    {\LARGE{\bf Functional connectivity measures using machine learning techniques}}
\end{center}

\vspace{36mm}
\begin{minipage}[t]{0.47\textwidth}
	{\large{\bf Supervisors:\\ Prof. Alessandra Retico\\ Prof. Piernicola Oliva}}
\end{minipage}\hfill\begin{minipage}[t]{0.47\textwidth}\raggedleft
	{\large{\bf Candidate: \\ Federico Campo}}
\end{minipage}

\vspace{60mm}
\hrulefill
\\\centering{\large{\bf Academic year: 2021/2022}}

\end{titlepage}


\tableofcontents

\chapter*{Abstract}
Autism is a neurodevelopmental disorder manifesting from the early ages and involving behavioral and cognitive impairments. So far, no univocal biomarker have been detected to identify this disorder, and the most accurate diagnostic tool remains a behavioural and attitudinal test.
In this work we are focusing on brain functional connectivity data to study the difference between patients with Autism Spectrum Disorder (ASDs) and healthy patients (referred as Controls). To tackle this challenge, we are going to employ machine learning and deep learning models.
Our base data are Magnetic Resonance scans provided by the ABIDE dataset consisting on structrural and functional scans of approximately 2200 patients collected from different medical centers.
Among these patient's scan, an half belong to normal patients and the other half to people with diagnosed Autism.
Starting from these data the workflow of this thesis can be summarised in the following steps:

\begin{itemize}
\item MRI and fMRI data preprocessing and normalization to a common template, and extraction of temporal series of different brain areas for each patient from the fMRI scans.
\item Creation of a connectivity map for each patient. This connectivity matrix is created computing a correlation coefficient for every pair of timeseries of a single patient. Correlation coefficients were computed using two different aproaches: a linear correlation of the two timeseries known as Pearson correlation, and a correlation based on time-frequency analysis of a timeseries pairs by means of Wavelet Transform.
\item Implementation of an harmonization procedure to correct for potential biases and distinctive traits of data, linked to a certain acquisition scan procedures employed in a medical centre which can differ between different centres.
\item Study and classification of connectivity matrices with deep learning methods. The two different connectivity matrices described above are compared to asses which one is able to yield the best separability between Controls and ASDs.
Classification of raw data and harmonized data implemented with different procedures are compared, and in addition an harmonization implemented inside a deep learning model through an adversarial learning technique was tested.
\item Deep learning model is compared to a Random Forest classifier, a standard machine learning model, to determine if a deep model brings a benefit or is excessive for this kind of analysis.
\item After we determined the correct harmonization procedure, we implemented an interpretation method called SHAP to explain which features contributed the most to the final outcome of machine learning classification, and from them we determined the most relevant brain areas that allow a discerning between controls and ASDs
\end{itemize}

\chapter*{Organization of contents}

\textbf{Background:} In this background chapter we are going to explain some useful theory that lays the groundwork to all the thesis work.
In chapter \ref{chap:autism} we outline the main problems associated with this disorder, from the biological development to the social isseues it generates. We briefly review some studies carried on so far to study this challenging issue.
Chapter \ref{chap:fmri} provides an overview of the physical and biological principles underlying fMRI diagnostic tool. This is the means through which data we are going to employ are collected.
Chapter \ref{chap:machine_learning} provides a sufficiently detailed explaination of machine learning and deep learning models diving into the mathematical formulation of important comcepts to properly get the idea of the structure of a machine learning models, the learning procedure, and the important metrics for a proper evalutation of model's performances. Ultimately, we describe a common way to reduce data dimensionality keeping as much information as possible and reducing the number of uninformative or reduntant features from data.
Chapter \ref{chap:shap-theory} describes the theory behind an important strategy to inspect a deep model and understand the processes that brought it to a certain output given some input data.

\textbf{MATERIALS \& METHODS:} This section describes the instruments we used to run the analysis: dataset, preprocessing steps, and data preparation employed to obtain correlation matrix.
Chapter \ref{chap:dataset} describes and analyse ABIDE dataset: a collection of more than 2000 scans of healthy and ASD patients, collected from different medical centers.
Chapter \ref{chap:image_preprocessing} provides an overviwe of the most important preprocessing steps to normalize and align all these scan to a common template to run meaningful statistical analysis and to exctract brain timeseries form different brain areas aligned to a common coordinate system. We describe CPAC: the software employed to this image preprocessing and how we used it to conduct our processing.
Chapter \ref{chap:connectivity_coefficients} describes the two methods we used to calculate connectivity coefficients and create a functional connectivity matrix for each patient. We describe the computation of correlation using Pearson correlation, and illustrate the formalism of Wavelet Transform for time-frequancy analysis and the process to extract a correlation coefficient from time-frequency data of two timeseries.
Chapter \ref{chap:harmonization_theory} describes the regression model upon which harmonization procedure is based. This is the analytical method to remove inter-site variability from our data and avoid biases towards the acquisition source.
Chapter \ref{chap:adversarial} describes the structure and the general idea beyond a domain adversarial neural network: a deep neural network that aims
to make Control/ASD classification following a procedure that makes data site-independent to classify following a procedure that makes the classification Control/ASD output sites-independent.

\textbf{Implementation \& RESULTS:} In this last section, the results obtained applying methods to our materials are presented. We illustrate the results of harmonization applied to our data and classification result of Pearson-based correlation coefficients and Wavelet-based coefficients and in the end we study the most relevant feature from our data, that played an important role in discrimination of Control/ASD.
Chapter \ref{chap:harmonization_results} Presents the results of the analytical harmonization to our data to have a visual understanding of how this process affects a feature distribution.
Chapter \ref{chap:results} shows all the classification results obtained with a deep neural network on harmonized and non-harmonized (raw) data and the results of the domain adversarial neural network and compares them with results obtained using a simpler Random Forest classifier. Results of data dimensionality reduction are presented as well to assess if this is a meaningful procedure to reduce dimensionality on this dataset.
At the end, chapter \ref{chap:shap_results} shows the implementation of a feature importance assessment procedure and the results obtained with machine learning in the previous chater. We show what feature contributed the most to the outcome of a prediction, and then to the discrimination of healthy and ASD patients, and subsequently we study what brain areas are mainly involved in this discernment.


\textbf{Appendix:} In appendix we present the basic physical principles of MRI imaging, a useful but not essential theory to understand the work carried out in this thesis.

\addcontentsline{toc}{chapter}{BACKGROUND}
%\chapter*{BACKGROUND}


\chapter{Introduction}
\section{Autism spectrum disorder} \label{chap:autism}
Autism spectrum disorder (ASD) is a neurological disorder that is becoming a growing social issue, drawing more and more attention expecially during the last decades, both for its social impact on families and society and because of the raise in number of diagnosied cases.

ASD is classified as a neurodevelopmental behavioural disorder \cite{guze-1995} \cite{who-1993} and refers to a broad range of conditions manifesting as deficits in social communication and interaction such as reduced sociability or empathy, repetitive behaviours, resistance to changes, and sometimes even speech difficulties.\cite{rapin-2008}

%\cite{hyman-2020}

Early signs start appearing by the age of 2 - 3, but the most accurate assessment procedures so far are comportamental: for a toddler, parents answer a set of questions about every-day behaviour of their children like the checklist for Autism in Toddler \cite{robins-2009} consisfing on a list of question such as \textquotedblleft When you point at something, does your child watch in that direction? \textquotedblright or \textquotedblleft does your children show interest towards other children? \textquotedblright

However, since there isn't a definitive biological test, often ASD is discovered even after the adolescence always through a comportamental assessment test.
Currently ADI-R (Autism Diagnostic Interview - Revised) and ADOS (Autism Diagnostic Observation Schedule) are considered the ‘gold standard’ tools for diagnosis of ASD \cite{ozonoff-2015} \cite{lecouteur-2008}.
Nevertheless, besides these behavioural test, to achieve a more accurate diagnosis, medical and biological informations should be taken into account as well .

For a better treatment of this condition, it would be of great importance to make an early diagnosis, in order to provide in time the help and support people need.
Hopefully, treating children when this disorder is still it in its early stage, could bring to a regression of this condition.

The main factors that bring to the developing of autism aren't very clear so far, and is acquiring more consensus between neuroscientists and medical doctors that there is not just a single cause, but a concatenation and cohexistence of various triggering factors.
Among them, the most strongly suspected are identified to be genetic and environmental, where environmental is to be intended as exposure to air pollution.
%or certain pesticides (even though is difficult to demonstrate a strong relation)
Regarding these environmental causes, it is suspected that during pregnancy, expecially during the first days of embrional development (day 20-24 of gestation).\cite{ratajczak-2011}, exposition of the mother to air pollution is linked to an increased risk for her child to develop this disorder.

Genetic origin of this disorder is assiciated with a rare gene's mutation, such as a deletion, duplication or inversion, and even though most of the mutation that increase the risk of developing autism are not been traced.
It has been assessed though, that it has an high inheritance trait. From studies on twins it has been assesses that on omozygotes twins there is around 90 \% or probability to them both to develop ASD, while for heterozygote this probability falls to aroint 7\% for men and 1-2\% for women
\cite{freitag-2007}
%tesi di elisa???.

%Ambiental factors such as prenatal air pollution or certain pesticides are also suspected to be associeted with a increase of the risk to develop this disorder, even thouh the etiology is way more complex and it can't be assessed so accurately.
%Currently ASD is diagnosided by syntoms-based tests concerning behaviour assessment.
From a neurological point of view, ASD can affect a brain both in its structure and in its functionality.

Structural studies usually focus on volumentric and morphometric analyses to examine differences in brain anatomy.
It has been studied how autism could alter the symmetry between the two emispheres \cite{postema2019} of the brain. In children it has been observed an increase in total brain volumne as well asan enlargment of the left superior temporal gyrus, but this trend is not well defined for older ages.\cite{riddler-2017}.


Functional neuroimaging researches mainly focus on impaired functionality. Different studies pointed out a reduced information processing due to synaptic dysfunction that manifests in a reduced or altered brain functional connectivity. \cite{spera-2019}.
Functional connectivity measures aim to describe statistical dependencies between brain areas in time studying temporal correlation between different brain parts even between those chich are spatially separated.
Altered brain activity is found by different but controversal studies because both hyper functional connectivity and hypo-FC were detected in asd patients, while some found a the cohexistence of an hyper and hypo functionality between different areas of the brain.

In particular it appears functionality abnormalities to have an age-dependent trend since over connectivity is usually observed in young children while under connectivity in adolescences and adults.\cite{supekar-2013}.

\section{f-MRI as investigation tool}

In the last decades, different approaches to study this problem have been carried out, to find a relation between different brain areas involved in this lower or higher functional connectivity.
Different diagnostic tool were proposed to investigate this matter, from different perspectives and at different scales, such as magnetic resonance imaging (MRI), functional-MRI (f-MRI), or electroencephalography (EEG).

Magnetic resonance imaging is employed to obtain images of brain both for structural studies and for functional through the  technique of functional MRI (fMRI).
An other tool employed to study temporal signal is the EEG which differs from f-MRI because of the type of signal it aims to detect and because of the different resolution, since with EEG is possible to study signal in scale time comparable with the neuronal activation time timescales $\sim \mu s$, while with f-MRI we look for a signal due to the hemodynamic response following a neuronal activation (timescales $\sim s$).


f-MRI is a diagnostic tool that investigate blood flow variations across the whole brain structure.
The measure the blood flow is directly linked with the neuronal activity, an increase of neuronal activity leads to a boost in blood flow and an increase of vessel's size within a specific brain area, because of the bigger demand of oxigen and other nutrients to the neurons in that area.

The different neural connections are identified by measuring the BOLD fluctuations, (acronym for blood-oxigen-level dependent) from different areas of the brain, using the signal difference between oxy (diamagnetic) and deoxyhemoglobin (paramagnetic).
%Oxyhemoglobin appears then brighter than deoxyhemoglobin
These spontaneous fluctuations in the BOLD signal during resting state are considered a strong indicator for the assessment of the properties of the brain system.

f-MRI can be performed during the resting state of a patient or during a task. We refer to these procedures as rs-fMRI and task-based fMRI, and while task-based fMRI is a good instrument to investigate the local activation of a single brain area during a task, is resting state fMRI which is drawing more attention since it was proven to be suitable for examining functional connectivity among all the brain regions, and highlight the difference between a normal drain and one affected by this disorder.

Resting state f-MRI is an acquisition performed while the patient is in a relaxed state, and is not performing any active task, he is simply asked to stay still with eye closed or open while fixating a cross.
%either resting or, more in general in a task-negative state.
Following this setup the brain is at rest and its activity is not perturbed by active tasks such as moving an arm or passive actions like being exposed to different visual stimuli, which are common activities for task-based functional connectivity, a different acquisition setup to investigate BOLD fluctuations between subjects in a task-stimulated state and control states.
Resting state setup revealed to be a powerful tool to investigate the intrinsec generated brain activity and study the altered functional connectivity networks in subjects with mental disorder

\section{Machine learning to deal with non-linear problems}
During the study of characteristic traits between control and ASD patients, different attributes are involved and characterise data in a strong and evident way, the most important of which are age, sex and FIQ (full intellective quotient).
Age for example affects both structural and functional data, with an observed overgrown of the brain volume in toddlers and hyperconnectivity mentioned before, that are both traits that tend to decrease going in on age, and on control subjects, even though it is not well characterised, seem to show a decreasing of brain structure and functional connectivity in older ages \cite{zhangC-2016}
Sex is the other feature that gives characteristic traits to data, studies show that some brain area show an increased functional connectivity in females, and from a combined analysis of both age and sex it appears that in men some brain structures show more pronounced aging effect than women
\cite{coffey-1998}
In addition, if we limit our focus just on functional data, the eye status at scan plays an important role, about this, functional connectivity with open steady eyes results to be different from closed eyes, with strong differences and higher connectivity in different brain areas between the two coohrts of subjects.
\cite{costumero-2020}. Furthermore, an other aspect to be take into account regarding closed eye patients, is that during the scan acquisition, they may fall asleep, and this would heavily modify the functional brain activity, resulting in a modified functional connectivity pattern.

It appears clear that the distinction of asd patient among normal ones is not a simple and straightforwand task because of all these confounding factors concorring to the characterization of acquired data.

To take on this challenge, one of the most promising tools that allow us to deal with complex, non-linear problems is the use of machine learning algorithm to study data extracted from fMRI such as pairwise correlations between different regions of the brain, or image of brain themselves from MRI scans to study with convolutional neural networks (CNN).
%One of the most promising and powerful tool to tacke non-linear problems is machine learning.
Machine learning algorithm belong to artificial inteligence tools and make use of alghorithms that learn from data, and modify their parameters with the aim of recognising distinctive properties from data and make predictions or classification, on new unseen data, trying to gradually reduce the error and make amore accurate predictions.

A relevant aspect that makes machine learning and artificial neural networks so popular is as cited before, the ability to deal with non-linear problems, and they accomplish so by introducing several non linear functions during training. This allow to obtain non-linear outputs from each input data, which would hopefully help in the distinction between characteristic traits of healthy people and of ASDs patients.

As a drawback, a restraint of machine learning, is that to perform well, an algorithm need to be trained on dataset of large dimension, because the bigger the dataset, the more it is able to generalize information and the better are its performances on new data.
This is true for all the kind of machine learning algorithms and expecially for Deep Neural Networks which are the principal kind of algorithms we are going to employ in this work.
\section{The need for an harmonization procedure}

To achieve the goal of obtaining a large dataset, particularly in medical field where data are not easily available from a single centre or are not in a sufficient amount to perform a large-scale analysis, several data from different acquisition centers need to be put together.
In the last years, this issue is becoming increasingly popular and several multicenter medical dataset such as the Human Connectome Project, the Alzheimer’s Disease Neuroimaging Initiative (ADNI) or Autism Brain Imaging Data Exchange (ABIDE) were created to try to create a large collection of data to permorm significant statistical analysis.

Unfortunately, this procedure brings with it a downside regarding the unavoidable bias towards the site the data belongs to.
This bias is a result of hardware and scan's procedure differences, between centres and it would be a really hard request asking to all medical center across the world to uniform to a single common acquisition protocol.

To tacke this problem and try to uniform inter-scan variability, we need an harmonization procedure to remove site-dependent information leaving all the rest of important information unchanged.

In this work, two different aproaches are proposed: an analytical harmonization and a deep learning aproach.

Analytical harmonization is a procedure that modifies data with shifts and rescaling to remove only inter-site related effects in multi-site data, while trying to preserv all other information as biological-related features.

The second aproach, the deep learning one, tries to make classification of Control/ASD data extracting from input data both information related to the class (Control/ASD) to site. It then use the latter to remove the bias toward sites before making a prediction.

\chapter{fMRI}\label{chap:fmri}

Functional Magnetic Resonance Imaging is a non-invasive diagnostic tool which aims to measure the neural activity of different parts of the brain.
The signal it detects comes from the blood oxygenation level fluctuations following a neuronal activation.
This kind of analysis is referred as blood oxygenation level-dependent (BOLD) imaging.

From a physical point of view the variation in oxygen content in blood affects the local susceptibility of the blood.
Blood can in fact be portrated as a colloidal mixture where blood cells constitute around 40 \% of volume.

Signal coming from blood cells is mainly due to the presence (in each blood cell) of several molecules of hemoglobin: a protein containing 4 heme groups each one including an iron atom which binds an oxygen atom and carries it throughout veins and capillaries.
Hemoglobin can be in two states: oxyhemoglobin and deoxyhemoglobin, these two molecules differs in the presence or not of the bounded oxygen molecule which reflects in a difference in their magnetic susceptibility as oxyhemoglobin has diamagnetic properties while deoxyhemoglobin is paramagnetic.
This difference is due to unpaired iron electrons in the deoxyhemoglobin which leads to an unshielded molecule against the external magnetic field.
The presence of deoxy-hemoglobin causes local field inhomogeneities, leading to a reduction of signal coming from water molecules.

This reduction is due to molecules go out of phase with one another more quickly, reducing the total magnetization.

This way, when a brain area is activated, it demands a greater amount of oxygen carried by oxyhemoglobin molecule which results in an increasing of the signal.


The process that from a neuronal stimulus leads to a measurable blood signal is governed by the hemodynamic response function (HRF) shown in figure \ref{fig:hrf}. This figure represent the blood response immediately after a neuronal stimulus (which occurs on a timescale of $\approx$ 10-100 $\mu$s). The BOLD signal takes $\approx$ 6s to reach a peak and a total of 20-30 s to return to its zero baseline.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{mri/hrf}
\caption{Evolution of BOLD signal of an adult brain during time. At the instant t=0 the neuronal stimulus occurs; after that, it takes around six seconds to reach a maximum of signal, and a total of more than twenty-five seconds to return to its baseline value}
\label{fig:hrf}
\end{figure}

The entire scan duration ranges from 5 to more than 8 minutes



This effect is characterized by $T_2^{\ast}$ relaxation time so this is the tuning parameter related to this kind of acquisition; this gives the alternative name of BOLD-images as $T_2^{\ast}$-weighted images.

Three different types of tissues are present in the brain: cerebrospinal fluid (CSF), and gray and white matter.

\chapter{Machine learning}

%Machine learning is a branch of Artificial Intelligence (AI) that aims to learn from data and gradually improve its accuracy.
Machine learning is a branch of Artificial Intelligence (AI) that aims to learn from data and apply this knowledge to operate on other, new, data such as making predictions or classifying them, and by training training process, gradually improve its accuracy in the task it performs.

Algorithms are trained to make classifications or predictions and, as a rule of thumb, the bigger and homogeneous is the dataset to train the algorithm, the better accuracy and generalization (namely perform well on different datasets) can be achieved.

According to the analysis we aim to perform, and the type of dataset we are working on, machine leraning algorithms can be divided into two macro areas: supervised and unsupervised learning.

From a dataset, consisting on a collection of data, each one containing a certain number of features, an \textbf{unsupervised} algorithm goal is trying to learn the principal properties from the dataset's structure and to extract information from data without human labour to annotate and label each data.
Analysis with unsupervised algorithms include clustering or dimensionality reduction.
Clustering is maybe the most simple and intuitive example of an unsupervised learning: it is basically a classification process through which unlabeled data are reorganised and classified into subgoups according to some common properties or distance measures that arise from the analysis.
In this way, data of each group, called cluster, share a certain degree of similarity in feature probability distribution.
%A common algorithm for clustering is k-clu?
%modificatoo
Dimensionality reduction is another common example of unsupervised learning process: it is performed with the task of finding an different representation of the data, with a lower dimensionality, and preserving as much information as possible about data.
This can be accomplished either by compressing data into a lower-dimensional space, or by searching the main source of variance across the data and create a representation in such a way that the dimensions of the new representation are statistically independent.



%They usually aim to learn the probability distribution of the features in our data, and to do so, they search for a new representation of the data, often simpler than the initial one.
%Simpler can be associated with
%\begin{itemize}
%\item lower dimensionality of data namely the reduction of the space where data lie of informally a reduction of the number of features in our data
%\item sparse representation of data transform the data in a representation containing zeros in most entries, to try to aligh input data along this new  representation's axis. Usually resulting in an increasing of data dimensionality to avoid information lost deriving from setting most entries to zero
%\item Independent representaiton focus on the main sources of variance across data, trying to create statistically independent representation ofa initial data.
%\end{itemize}

On the other hand, when we are dealing with supervised learning each input data is associated with a label, specifying the class which data belong to, so the algorithm is to learn the common features for each class trying to predict the label given the corresponding input, or to classify them according to the right label.
Generally speaking, given an input data x and an associated label y, they try estimating p(y|x). These algorithms are referred as supervised because of the human labour needed to label each input data.

A subset of Machine Learning is Deep Learning, the main difference lies on the algorithms' structure and the learning techniques, as an instance, traditional machine learning methods consist on algorithms like Random Forest, Support Vector Machines (SVM), K-nearest neighbor (KNN) and several more models, while deep learning models include Artificial neural networks (ANN), Convolutional neural networks (CNN) and more.
As we will see in furter paragraphs, they have a common structure similar to a neural structrure, made by different neurons connected to each other.




\section{Random forest}
Da rivedere un paio di frasi.
Random Forest classifier belongs to the category of \emph{ensemble} classifiers, and is a common machine learning algorithm.
It combines multiple decision tree models to reduce overfitting of data and achieve a better generazability to create a more powerful model.
Therefore, to properly understand a random forest algorithm, we need to start understanding how a Decision Tree classifier works.

\paragraph{Tree classifiers} \hfill

A decision tree aims to learn distictive traits from input data by asking yes/no questions. Focusing on the valus of a single input feature, the classifier splits the dataset based on the value of that feature according to an if/else statement like \textquotedblleft if feature\_i $>$ a \textquotedblright.
Doing so each branch of a decision tree consists of a question which splits the dataset into two smaller sub datasets.
Considering different features, the process is then recursively repeated untill it reaches an end point called leaves, corresponding to the ultimate partition, containing a single data point belonging to a single class.
The goal of this tree is to construct branches so that the partition are informative about the class labels.
In a practical way, given a dataset X made of different data samples, each one containing n features, the algorithm construct a tree following these steps:
Starting from the top node called root, it searches among the n features the one which allows the best split between classes and split the dataset into two subsets, each one constituting a node. This split is performed according to a pre-defined objective function we want to maximize throughout the tree construction.
Usually these kind of functions regard some kind of impurity measures such as the difference between the impurity of a parent node and the sum of the impurity of the child.
The lower the impurity of the child, the bigger the Information Gain.
This splitting process is repeated for each node in order to achieve the best information gain for each split, and
%Then repeats the previous step and split each subset in order to have the best information gain about categories for each split.
the process is iterated and the tree is grown untill a single node is left.
The important features according which a tree is forked at each node, can be visualized and if we plot the tree structure.
This property is one of those which make decision trees so popular: they are easy to interpret and understand since their structure and the information of each node can be plotted to have a visual feedback of what is going on during the growing process.
Other positive sides of decision trees are, their easy to use implementation which requires little data preparation and their rapidity since their complexity goes like $O(n_{features}n_{samples}^2log(n_{samples}))$ \cite{scikit-2011}

%decision trees are among the most popular classifiers for different reasons: theey are implemented in several packages for different languages, they are easy to use since they require little data preparation, and are simple to understand and interpret, since they can even be visualized.


Three common impurity measures or splitting criteria are commoply used in decision trees: Gini impurity, entropy and classification error.
%We are not focusing on these functions, a more detailed explanation can be found in \cite{raschka-2019}.
%Gini Impurity is the frequency with which a randomly chosen element in the dataset is incorrectly classified if it were randomly labeled according to the class distribution in the dataset.
As an example we report the Gini impurity measure. If a node contains a sub-dataset Q with a total number of data n, belonging to different classes k, the gini impurity measure is obtained by the simple relation
\begin{equation}
H(Q)=\sum_k p_{k}(1-p_{k}) = 1- \sum_k p_k^2
\end{equation}
Where $p_k$ is the fraction of data in Q belonging to class k.
%\begin{enumerate}
%\item Starting from the top node called root, it searches among the n features the one which allows the best split between classes
%\item Split the node by asking a question about the feature that would provide the best classes' split
%\item Iterare point (1) and (2) until a single data point is left
%\end{enumerate}

An example of a decision tree dealing with flower classification is shown in figure \ref{fig:decisiontree}.
We choose this figure because is easier to understand how a decision trees works with this example where each feature is labeled with an intuitive name and contains a physical quantity easily comparable with everyday life such as petal lenght expressed in centimeters.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{ml/decisiontree_example.png}
\caption{A graphical representation of a Decision Tree for a classification task of the Iris dataset consisting on three different classes of iris: Setosa, Versicolour, and Virginica. Each node of the tree shows the criterion used for its split, the gini values, the total number of samples in that node and the main class that data belong to}
\label{fig:decisiontree}
\end{figure}

Instead of looking at the whole tree, we may be interested in what feature contributed the most to the final output, to this end, a decision tree usually weigths feature with a number from 0 to 1 according to how much a feature contributed to the impurity decrease along the tree.
\cite{raschka-2019} \cite{muller-2017}

Usually though, one of the main drowbacks of decision trees is that the iterating process towards the reaching of a leaf, can bring to the creation of a over-complex model that overfits the training data, that is, the model accurately learned the training dataset but is not able to generalize well to a test dataset.
To prevent overfitting, one possible strategy is to early stop the iteration towards the leaf, and leave a node with more than a single sample left.
An other strategy, which leads to the construction of a Random Forest, is the creation of a stronger model, more prone to generalize data reducing overfitting.

\paragraph{Random Forest} \hfill

A Random Forest is a collection of decision trees where each tree is slightly different from the others, each tree tends to overfit, but it does so in a different way, so that we can reduce overfitting by averaging different results.
Random forest gets its name because of the insertion of randomness during the construction of these different trees.
The process whereby a Random Forest is built, given an input dataset X with N different samples, there are two main steps where randomicity plays an important role: when selecting the number of samples on to which built a tree n\_trees, and when selecting the number of feature to use to grow each tree.
In summary, a random forest, once selected the number of tree to be created:
\begin{enumerate}
\item Randomly select n$\leq$N samples from the input dataset, this operation is called bootstrap.
\item Grow a decision tree from this bootstrap sample, but to each split, it randomly limit the number of feature available and compute the best split on this remaining subset of feature. This avoids correlations between trees resulting in better performances
\item Repeat steps 1) and 2) n\_trees times.
\item Once all the trees are created, if we want to make a prediction, we put together all the trees, and use each one of them to make predictions.
The final prediction of the random forest is assessed by majoring vote: the predicted class will then be the one predicted by the majority of classifiers.
\end{enumerate}

%This steps are implemented in the Python's package Scikit-Learn


%Randomness within trees of a single model is introduced with three strategies:
%\begin{enumerate*}
%\item Randomly select some input data and buiding a tree with this subset of data
%\item Randomly select a different subset of features at each split
%\item A combination and implementation of both the previous strategies results in a model in which each tree has reduced predictive power, but when put %all together, it results in an improved predictive power with a drastical reduction of overfitting.
%\end{enumerate*}

Introducing this kind of randomness in a model results in a lower correlation between models and this brings to a reduced variance between outputs.
Informations regarding features importance can be extracting from a random forest classifier by extracting them from each tree, and by averaging the impurity decrease from all the decision trees in the forest.



\section{Deep Learning: ANN}

We discussed in the previous paragraph one of the most important machine learning algorithm, but, as mentioned before, there's a subset of machine learning algorithm with a charactristic common structure, thanks to which they get their name as Deep learning algorithm.
The structure of algorithms belonging to this family are inspired by a brain neuronal structure, and even if in a simplified way, they try to emulate the learning process of a brain.
These algorithms own the adjective \textquotedblleft deep \textquotedblright to their structure: they are organised in layer, each one containing several foundamental unit called neuron.
Neurons between layers are connected to each other like synapsis transmit a signal between neurons in a biological brain.
For this structure which reminds a neuronal brain structure,they are caller artificial neuronal networks
%Some kind of algorithms are called Artificial Neural Networks (ANN) and, such as a neuronal structure, they are made of several neurons organized into layers; each layer contains certain amount of neurons each one linked to each others.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\includegraphics[width=1\linewidth]{ml/perceptron_example}
\caption{A schematic representation of a perceptron: this perceptron receives an input vector t, with n features $x_1, ... x_n$, each one weighted with a different weight $w_1, ... w_n$ and a offset $w_0$, and compute a linear combination of them and returns an output, activated by a step function: it returns 0 if the linear combination's value doesn't reach a certain threshold, or returns the value itself if it does.}
\label{fig:perceptron}
\end{wrapfigure}

The foundamental unit which constiture a neuronal network is an artificial neuron, one of the most relevant example of artificial neurons is the \emph{perceptron} shown in figure \ref{fig:perceptron}.

A perceptron is the foundamental unit of a supervised deep learning algorithm: it takes as input a data, for example an n-dimensional array $\textbf{x} = \left\{ x_1, x_2, ..., x_n\right\}$ and returns an output: a real number computed by applying a function to a linear combination of all the inputs $ y = f(z) = f \left ( \sum_i w_{i} x_i + b \right )$.
The function that determines the output is called \emph{activation function}, and can be either a simple step function, or a slightly more complex function we will briefly discuss in section \ref{sec:activation_functions}.



\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\includegraphics[width=1\linewidth]{ml/ann_example}
\caption{Schematic representation of an artificial neural network containing an input layer, three hidden layers and an output layer: a single vector contained n features (input 1, ... input n) is given as input to the first Input layer i. All the inputs are linked to every neurons of the first hidden layer, which would compute a linear combination of its inputs. All the hidden layers are linked to the other hidden layers, and at the end, n different outputs are returned.
}
\label{fig:ann}
\end{wrapfigure}


A deep neural network is a hyerarchical organization of neurons into layer, connected to each other.
Input data are passed to the first input layer where each neuron acting like a perceptron, produces an output using the activation function which is set the same for every neurons of the layer, but can differ from the activation function of other layers.
Once collected each and every output from all the first layer neurons, they are passed to the second layer and become the input to each neuron belonging to this layer.
This is reiterated through all the layers up to the final output layer.
As schematized in figure \ref{fig:ann} an artificial neural network is mainly composed of three parts: an input layer, come middle layers also  called \emph{hidden layer} and a final layer. Each neuron of a layer is linked to all the neurons of the previous and next layer.
The connection between two neurons is weighted, according to an initial setup of the network during which weights are randomly set.

When reach the end of this process, we obtain the output: a numerical value related to every single input data $\textbf{x}$.

Using matrix formalism the entire input-output process can be written as

$\textbf{y} = \sum_j^n w_{ij} x_j + b_i $ where $\textbf{x} \in \Re^n \ \textbf{y} \in \Re^m $ being m the output dimension given by the number of neurons in the output layer.


During the training process, given an input, the algorithm calculate an output (prediction) and this process is referred as  \emph{forward propagation}.
This prediction is then compared with the actual value this output should be, and during the \emph{backpropagation} the algorithm modifies its weights in order to minimize the difference between the actual and the predicted value.

The goal of a machine/deep learning algorithm is to learn from data using a train dataset, and generalize it in order to perform well on an unseen dataset called Test dataset.
Performing well means to produce a low error on the Test dataset after minimizing the error on train dataset.


To fully define a neural network we just need some parameters called hyperparameters, the principals of which are:

\begin{itemize}
\item Number of layers
\item Number of neuron for each layer
\item Activation function for each layer
\item Number of epochs (or iterations)
\item Learning rate
\end{itemize}




One of the most important concept in machine learning when characterising a network is its Capacity.
It refers to the level of complexity that a model is able to learn.
This is strictly linked to one of the critical issue in machine leraning: the concept of underfitting and overfitting.

Underfitting occurs when the model isn't able to learn the required amount of informations during train; this usually happens for shallow network, when the model has a small number of parameters in regard to the number needed to explain the input features, having low parameters results in poor performances because the model isn't able to learn the underlying structure of a complex dataset.

On the contrary, overfitting occurs when the model has too many parameters compared to those required to learn the input features.
What happens then is that the model memorizes all the data it sees during train but it is not able to generalise them.
As a result, the model performs well on the Train dataset and has low performances in an unseen dataset, called Test datset.

As an example in a two-dimensional space, if we aim to fit a dataset consisting of some points sampled from a quadratic curve, using three different functions: a linear function, a quadratic function and a polynomial function with grade equal to the number of data points.
As shown in figure \ref{fig:underoverfitting} the linear model is not able to describe the data distribution while the polynomial function has too many parameters, and it is perfectly able to fit our distribution, but it would perform very poorly to describe a dataset sampled from the same quadratic distribution, furthermore, if the number of parameters (the grade of the polynomial in this case) is greater or equal to the number of data points, we obtain that an infinite number of different curves are suitable to fit our data, and finding the best one, which performs well on the test dataset is an hard task.
We should therefore pay attention when choosing the number of parameters on a model to avoid under- or more likely over- fitting.


\begin{figure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{ml/underoverfitting_mod}
\caption{Underfitting, overfitting and appropriate fit in a 2D dataset: three models with different capacities are trained on blue data, if the model is too simple, in this case if the fit function has a few free parameters, it is not able to properly fit data, on the other hand, if it has too much parameters in respect to how much we would need to fit our data, it perfectly match train data distribution, passing through each data point, but performs very poorly on the test dataset (orange points). Whit an appropriate number of parameters, (middle figure) the model is able to fit train data, and generalizes well to Test data.
}
\label{}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{ml/underoverfitting_grafico}
\caption{}
\label{}
\end{subfigure}
\caption{General, qualitative trend of a loss function vs. capacity of model: the more capacity it has, the more is able to reduce the error on train set (blue dotted line) with a tendence to overfit train data. Beyond a certain capacity the error on the test dataset (red line) increases because overfitting of training data results in a lower ability to generalize to new data.
}
\label{fig:underoverfitting}
\end{figure}

\paragraph{Regularization strategies} \hfill
Some regularization procedures are often implemented to avoid overfitting, the strategy is to build a model with a capacity slightly higher than the necessary in order to perform well on the training dataset, and then, to avoid overfitting, implement some regularization techniques to achieve good generalization performances.

Some of the most popular are Dropout and Batch-Normalization

\begin{itemize}
\item Dropout is a strategy which allows us to obtain the same effect of training train different model with our input data and take the average output at the end without paying high computational costs.
It is usually applied to hidden layers' neurons and accomplish this task by randomly dropping a certain fraction of hidden neurons and their connections, during each training cycle. The percentage of neuron to drop, correspons to the probability p for a single neuron to be dropped and it is set by the user.
A visual representation of what occurs is illustrated in figure \ref{fig:dropout}.
When discarding some neurons in a layer, the remaining neurons needs to rescale their weights to account the missing connection; doing so, every neuron cannot rely on the the input of all the preceding neurons and the network is forced to learn more robust patterns from the data.
\item BatchNormalizzation is a regularization scheme that has been commonly adopted since its introduction in 2015. It is based on the observation that a neural network works and performs better when its input are normalized because this prevents the saturation of its neurons.
A neuron can in fact saturate and settle to a certain value because of an high input, this causes the neuron outputs value to be always close to the asymptotic end of its activation function, resulting in a biased and less accurate prediction.
What is essentially done is then a simple scaling of each neuron's input: for a layer l with d neurons its input $\mathbf{x} = \{x_1^l, x_2^l, ..., x_d^l\}$ is normalized by removing the mean value across all the input data, and divide for their variance $x_i^l \rightarrow \tilde x_i^l = \frac{x_i^{l} - \mathbb{E}[x_i^l]}{\sqrt{Var(x_i^l)}} $

\end{itemize}

\begin{figure}
\includegraphics[width=.5\linewidth]{ml/dropout.png}
\caption{Schematic representation of a 0.5 dropout procedure: in a standard neural network, each neuron is linked to all the others, but if we implement a 0.5 dropout, for each iteration, neurons on hidden layers have a 50\% probability to be dropped and excluded from the computation of outputs}
\label{fig:dropout}
\end{figure}

\paragraph{Cross validation procedures} \hfill
To monitor the evolution of the model's performances during train is a common practise to split the train dataset into two subsets: one that will be the actual train dataset, and a smaller one, called validation dataset used to make constant checkups on how well the model is learning with the train set, by making constant test of its performances.

The validation dataset is used for the fine tuning of hyperparameters and it typically consists of 10-30\% of the whole train dataset.
When assessed the best hyperparameters combination for our model, is possible to actually train the model using the entire Train dataset.
Even if we manage to use the entire Train dataset for the training process,
However, when dealing with small datasets, dividing it into Train and Test can be a non trivial issue because of the shortage of data for a proper train procedure.

To work this out, a prodecure called k-fold cross validation can be implemented, at the cost of increasing computational costs.
It consists in the creation of k different partition of the main whole dataset (before splitting it into train + test).
From these partitions, k-1 are used as train, and the last is used as Test. Moving forward every combination of partitions is used so that each different partition is used as test, and the others as train.
Then for each run k-th there are two subsets (a train and a test) from the original dataset. This way, for each iteration, the Train dataset is different, and the model is tested on a different dataset each time.
In practise what is usually done, is not a sequential partitioning the dataset, to avoid creating folder containing all the same label if the dataset was ordered, but randomically picking data in order to create subsets containing the same proportion between classes as the main dataset.
After that, we can imagine this process like the creation of k different models, each one trained on a different partition(k-1 folds) and tested on the remaining k-th fold, and we take as a result the average score across all these models.


\begin{figure}
\includegraphics[width=0.4\linewidth]{ml/kfold_example}
\caption{Schematic representation of a k-fold cross validation procedure with k = 5: Dataset is partitioned into 5 subsets, four of which are used for training and the remaining one for the testing dataset. during each iteration a model is trained over 4 different folds and tested on the remaining one, untill each fold has been used at least once as part of training and once as test.
}
\label{fig:kfold}
\end{figure}


\section{Activation functions}
The activation function of a neuron, and consequently of a layer defines the output of each neuron belonging to that layer, there are different activation function, linear or non-linear.
A linear activation function outputs a value $f(x) = w^T x + b$ where $w^T$ indicates the transpose of the weigths vector.
In practise, however, it would not be very useful to introduce a linear activation function since we aim to introduce non-linearity in our model to tackle more complex models.
There are several non-linear function available and, depending on what data we are working on, or on what kind of classification task we are performing, some of the most popular are


\begin{figure}[h!]
\centering
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ml/relu}
\caption{Rectified linear unit (ReLU) function: it returns zero if its argument is negative and returns the argument itself if it is positive.}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ml/sigmoid}
\caption{Sigmoid function: returns a value between 0 and 1, according to equation \ref{eq:sigmoid}}
\end{subfigure}
\caption{}
\end{figure}




\begin{itemize}

\item The ReLU functions $\phi(z) = max(0, z) = \begin{cases} 0 \ for \ z<0 \\ z \ for \ z>0 \end{cases}$ which returns the maximum value between the input and zero, it essentially puts to zero all negative inputs and leaves the positives untouched.
A modified version of the ReLU function called Leaky ReLU was introduced defined as
$\phi_{leaky}(z) = \begin{cases} \alpha z \ for \ z<0 \\ z \ for \ z>0 \end{cases}$ where $\alpha$ is a coefficient usually $< 1$, typically of the order of $10^{-2}$


\item The sigmoid function, or logistic function is defined as
\begin{equation}\label{eq:sigmoid}
f(x) = \frac{1}{1+e^{-x}}
\end{equation}
 and outputs a real number between 0 and 1. For this reason is a common choice when we have to predict a probability, for example in a binary classification problem it can be interpreted as the probability that output belongs to the class 0 or 1.


\item The softmax function for multiclass classification, gives the probability of the input belonging to each (mutually exclusive) class. The softmax can't be applied independently to each output $z_i$, since it depends on all elements of $\textbf{z}$ The probability of belonging to the i-th class over a total of M classes is
$p(i = i|z) = \phi(z) = \frac{e^z_i}{\sum_{j = 1}^M e^{z_j}}$ being M the total number of classes
\end{itemize}


\section{Loss functions}\label{sec:loss_functions}
%Ricorda: \hat y_i = sigma(x_i) che è il mio output del modello
%Articoli da cui ho preso info: ml for physicists e dispende di baldini

In order to train a network and improve its performances we need to compare the predicted (output) value with the actual value and compute some sort or error, or distance between these two values, and try to gradually minimize it.


The function to minimize is denoted as the \emph{loss function}
Computing the loss, returns a value of how much the prediction is far from the actual label, and then, the network's weights are modified in order to reduce this value and searching for a minimum of this function.


and the main technique to do so, is the likelihooh maximization

The most common loss function is the cross-entropy loss, it relies on the concept of cross-entropy between two distributions $\hat y \ and \ y$ and is defined as
$H(y, \hat y) = -\sum_i^n y_i \cdot log(\hat y_i)$
Where the sum is intended over all the n possible values a variable y can assume (all the n possible classes in a classifiction problem).
%In case of multi-class classification we call this function categorical crossentropy,
If we only have two classes, we are performing a binary classification and the loss becomes, (if we explicit the sum for two $i = \left\{0, 1\right\}$ and take the average value across all the data samples) we obtain the binary crossentropy:
\begin{equation}\label{eq:binary_crossentropy}
J = -\frac{1}{N} \sum_{j= 1}^N y_j log(\hat y_j) + (1-y_j)log(1-\hat y_j)
\end{equation}

The estimation of the minimum of this function can be seen as the maximization of the likelihood:
Given a probabilistic model depending on m different parameters $\theta_1 ... \theta_m$, the likelihood is the probability of observing a value $\hat y_i$ given a set of parameters $\{\theta_i \}_1 ^m$
\begin{equation}
 L(\theta_1, ..., \theta_m | \hat y_i) = P(\hat y_i|\theta_1, ...\theta_m).
\end{equation}
For a subset of observed value we can write the likelihood as $L = P(\hat y_1, ..., \hat y_n | \theta_1, ...\theta_m)$ .

This probability, if we are dealing with independent variables, can be rewritten as the product of the single probabilities, of observing each sample $\hat y_i$ given out model with parameters $\{\theta_i \}_1 ^m$:
\begin{equation}\label{eq:likelihood_product}
L =\prod_{i=1}^n p(\hat y_i|\theta_1, ...\theta_m).
\end{equation}

%Our goal is to perform the maximization of this function during a logistic regression:
%A logistic regression is the estimate of the best model's parameters that models the discrete probability of two events:

In our practical problem if we aim to maximize this function, the parameters to be optimized $\left\{\theta_i \right\}_1^n$ are the weights of the DNN model $\left\{w_i \right\}_1^n$ and our data is a set of N datapoints $\left\{ x_1 ... x_N\right\}$ each one associate with a label  $\left\{ y_1 ... y_N\right\}$ which can be either 0 or 1 which can be indicated as D = $\left\{ (\mathbf{x_i}, y_i)\right\}$.

From now on we will denote our parameters to optimize as $\mathbf{w} = w1, .., wm $
Given an input data $\mathbf{x_i}$  and an activation function (sigmoid for example, which we recall is of the form $\sigma(z) = \frac{1}{1+e^{-z}}$) we model the probability of $x_i$ to belong to the class $y_i = 1$ as:
%or in other words, the probability to observ $y_i$ given an input $x_i$ is given by

\begin{equation}
 P(y_i = 1 | x_i, \mathbf{w}) = \sigma(\mathbf{x_i\cdot w}) = \frac{1}{1+e^{-\mathbf{x_i \cdot w}}}
\end{equation}

%which in terms of a regression can be wrote as 1 - the probability to belong to the negative class namely
And since we are dealing with a binary classification, where $y_i$ can be either 0 or 1, the probability to belong to one class is 1 minus the probability to belong to the other class
\begin{equation}
P(y_i = 1) = 1 - P(y_i = 0)
\end{equation}

Putting this all together for a set of data $D = \{ (\mathbf{x_i}, y_i)\}_1 ^N$ substituting this result in the log-likelihood function we obtain the likelihood of my observations $\hat y_i = \sigma (x_i|w)$, from equation \ref{eq:likelihood_product} becomes
%of observing our data under our model
\begin{equation}
L = P(D|\mathbf{w}) = \prod_{i=1}^N \sigma(\mathbf{x_i w})^{y_i} (1-\sigma(\mathbf{x_i w}))^{1-y_i}
\end{equation}

The best parameters are those that maximize the likelihood:
$\begin{cases} \frac{\partial L}{\partial w_1}(w_1, ..., w_m) = 0 \\ ... \\ \frac{\partial L}{\partial w_1}(w_1, ..., w_m) = 0 \end{cases}$
To compute the derivative of a product is a nontrivial task, hence is much simpler to compute the logarithm (since the logarithm is a monotonic function, so compute the maximum of a function is the same as computing the maximum of the logarithm of that function).

We obtain then
\begin{equation}
log(L) = \sum_{i=1}^N y_i log(\sigma(x_i \mathbf{w})) + (1-y_i)log(1-\sigma(x_i \mathbf{w}))
\end{equation}

which taken with a negative sign, and averaged over all the N data samples corresponds to the cross-entropy function in equation \ref{eq:binary_crossentropy}.


This result obtained for binary classification can be generalized to perform a multi-class classification called Softmax regression.
In a multi-class classification, labels $y_i$ become binary vectors of dimension M, with all but one entry equal to zero, and the only entry equal to 1 specifies the class.

In this case, given a model with parameters $\mathbf{w_k}_{k=0}^{M-1}$ the probability for an input $\mathbf{x_i}$ to belong to class m' is given by the softmax function

\begin{equation}
P(y_{i,m'}= 1 | \mathbf{x_i}, \{\mathbf{w_k} \}_{k=1}^M ) = \frac{e^{-\mathbf{x_i^T w_{m'}}} }{\sum_{m = 1}^{M} e^{-\mathbf{x_i^T w_{m}}}}
\end{equation}
and the relative negative log-likelihood called the \textbf{categorical crossentropy} will be in the form:

\begin{equation}
L = -\sum_{i=1}^{n} \sum_{m= 0}^{M-1} y_{im} log(P(y_{im}=1|x_i, w_m)) + (1-y_{im})log(1-P(y_im=1|x_i, w_m))
\end{equation}

where there's a summation over the index i which indicate the i-th sample data and over the index m, concerning all the possible M classes.


%The algorithm employed by a feedforward network for weights' fine tuning is called \emph{backpropagation} (backward propagation of errors).

\section{Gradient descent and Backpropagation}

To perform the minimization of our loss function, even if it'd be theoretically possible to find a minimum by means of an analythical way, in practise, in all the neural network applocation, the number of weigths is so big that a numerical methods must be employed, the most popular of which is the \emph{gradient descent}.

Let's denote the loss function as $J(w) = \frac{1}{n}\sum_{i = 1}^N E(y_i, \hat y_i)$
Given a generic loss function $J(\textbf{w})$ where $\textbf{w}$ is a vector of weights, the minimum of this function corresponding to the vector $w_0$ can be found by following these simple steps:

\begin{enumerate}
\item Choose a random initial guess for \textbf{$w_0$} and start iterating
\item At iteration i+1 we have a weights vector \textbf{$w_{i+1}$} given by the formula \textbf{$w_{i+1}$} = \textbf{$w_i$} - $\eta \nabla$ J(\textbf{$w_i$})
\end{enumerate}

 where the $\nabla$ indicate the gradient of the cost function with respect to \textbf{$w_i$} components, and $\eta$ is the learning rate of our gradient algorithm, it specifies how big each step will be during the descent toward the minimum of the cost function.

%To find the right variation to the weights we can use the gradient descent algorithm, so we compute the gradient of the cost function $\nabla J(w)$ so that we can compute the new weight $w = w + \Delta w$ where $\Delta w = - \eta \nabla J(w)$ which expressed in component becomes $\Delta w_j = -\eta \frac{\partial J(w)}{\partial w_j} = -\eta \cdot \sum (\phi(z^{(i)}) - y^{(i)} )x_j^{(i)}$.

A drawback of this gradient descent algorithm is that before upgrading the weiths we need to compute all the gradients for each data in input data namely, after the whole dataset is been seen, resulting in a huge computational cost.

An optimized version of this algorithm calle dStchastic Gradient Descent (SDG) is then often used because it has some advantages such as decrease computational cost and, as the name suggests, introduce stochasticity resulting in less chance for this algorithm to get stuck in a local minimum.
It works by approximating the gradient of the cost function calculated with all the input data, with a gradient computed using only a small subset of input data called minibach.

If we have N input data, we can create subsets containing m elements, and obtain N/m subsets.
Therefore, the gradient is computed on a mini-batch and weights are updated and this process is repeated for each mini batch.
%mettere equatione ml_physicists p.17  (19)???
Once the gradient was computed over all the mini batches, this is called an epoch, which is one of the hyperparameters to be set when choosing a model and a training strategy.

%whose difference is that the upgrade of the weigths is sequential, for each training sample, so that the summation is not computed.

Even though SGD already performs quite well, it can be further tuned introducing the concept of momentum.
Momentum is represented by a parameters $0\le \gamma \le 1$ and it takes track of the discending direction by running an average over all the preceding encountered gradients, and helps the algorithm speeding up the descending process if in a certain direction the gradient is persistent.
But in addition we can take into account even the steepness all over the dimension, and to do so we must introduce second order momenta in our algorithm, also called uncentered variance.

To accomplish this, we would ideally need to to calculate the hessian matrix but this comes at the cost of increasing computational cost.
Recently intriduced algorithm can accomplish this task by approximate the calculus of the second momenta.
Doing so, we keep track of the curvature and take big leaps in steepest direction and small steps in flatter ones, allowing us to adaptively change the descending step size according to the shape of our multi-dimensional curve.
One of the most popular among these algorithms is Adam: it accomplish this computational task by making use of two different optimization algorithms: AdaGrad and RMSProp. and adapts the learning rate taking into account both the first and the second moment, leading it to perform better and quicker in findind the minima than simple SGD with momentum algorithms.



After the computation of the loss value, the next step is the process called backpropagation, through which network's weights are updated.
Backpropagation can be summarized in 3 steps:

\begin{enumerate}
\item calculate all the activation of each neuron in a layer
\item calculate the error of the output layer L, which for each neuron will be $\Delta_j^L =\partial E/\partial z_j^L$ being z the output of this neuron
\item Backpropagate the error exploiting the chain rule of derivatives to compute the error of every neuron in the previous layer l $\Delta_j^l = \sum_k \frac{\partial E}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l}$
\item Compute the gradient of the function E with respect to model's weights and modify them using equation ..
\end{enumerate}



\section{Dimensionality reduction: PCA}
PCA is a method to perform dimensionality reduction, it is an unsupervised learning algorithm which aims to reduce the dimensionality of input data, preserving as much information as possible,
It does so attempting to learn a new representation of data with lower dimensionality than the initial one and whose element have no linear correlation with each other. It performs then an orthogonal transformation of data, in order to find the direction along which data variance is biggest.

Concretely, if we consider a set of n input data vectors lying in a space $\Re^p$, we can represent them as a matrix $X \in \Re^{n \times p}$ where n is the total number of input data and p is the dimensionality of each data (the number of features in each data vector).

We can suppose without losing generability that feature distribution across our data have zero mean, so that for each feature $x_i$ with i = 1, ..., p, $\mu_i = \mathbb{E}[x_i]=0$.

PCA computes the covariance matrix $\Sigma \in \Re^{p \times p}$ given by $\Sigma(X) = \frac{1}{n-1} X^T \cdot X$, where each entry can be written as $\sigma_{jk} = \frac{1}{n-1}\sum_{i = 1}^n (x^{(i)}_j - \mu_j)(x^{(i)}_k - \mu_k)$, and if we are in the hypotesis of zero mean $\mu_j = \mu_k = 0$, we obtain $\sigma_{jk} = \frac{1}{n-1}\sum_{i = 1}^n (x^{(i)}_j x^{(i)}_k)$.

Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal we actually have the variances of each initial variable. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal.

We are interested in finding a new representation performing a linear transformation that reduces the covariance between different features.

The eigenvectors of the principal components represent the direction of the maximum variance
and the corresponding eigenvalues, defines their magniture.
 %The principal components of the matrix X are given by the eigenvectors of the matrix $X^T X$ so that we can write $X^T X = W \Lambda W^T$ where $\Lambda$

%PCA finds a representation through a linear transformation z = $X^T W$.

To find them, PCA makes use of Singular Value Decomposition (SVD): a linear algebra analysis which is basically a factorization of a n x p matrix X that (in a case X is a real matrix), to rewrite it as X = $U S W^T$ where U and V are respectively m x m  and n x n orthogonal matrices whose columns are called left- and right-singular vectors of X, and S is a  m x n diagonal matrix.
Diagonal values of S: $S_{ii} = s_i$ are unically determined by X and are called singular values of X.

According this analysis, we can rewrite
\begin{equation}
\begin{aligned}
X^T X & = (U S W^T)^T(U S W^T) \\
& = W S^T U^T U S W^T \\
& = W S^2 W^T
\end{aligned}
\end{equation}
where we used the definition of orthogonal matrix for U $U^T U$ = I.

We therefore rewrite the covariance matrix $Var[X] = \Sigma$ as $\Sigma = \frac{1}{n-1}W S^2 W^T$.

%Ne sei sicuro di questo?????
Here the singular values of X: $\lambda_i$ are related to eigenvalues of the covariance matrix by the relation $\lambda_i = s_i^2/(n-1)$

Using this results if we compute the new data matrix Z = $X^T$ W, its covariance matrix
\begin{equation}
\begin{aligned}
Var[Z] & = \frac{1}{m-1}Z^T Z \\
 & =\frac{1}{m-1} W^T X^T X W   \\
 & = \frac{1}{m-1} W^T W S^2 W^T W \\
 & = \frac{1}{m-1}S^2 .
\end{aligned}
\end{equation}
This shows that when we project the data x to z using the linear transformation W, the resulting transformation has a diagonal covariance matrix which implies that the individual elements of z are mutually uncorrelated.

To reduce the dimensionality of our data, we need to extract the first  $\tilde p$ eigenvalues from the covariance matrix, ordered in a descending order, and collect the corresponding projection matrix $W_{\tilde p}$ made of the corresponding vectors of the righ-singular vectors W.

And the projection of our data from p-dimensional space into $\tilde p$-dimensional space is $\tilde Z = X\tilde W_p$

An important parameters to quantify \say{how well} PCA is able to explain is the \emph{variance explained ratio}, given by the ratio of an eigenvalues and the sum of all the eigenvalues.

\begin{equation}
\frac{\lambda_i}{\sum_{k = 0}^p \lambda_k}
\end{equation}

The singular vector with the largest corresponding singular value is the first principal component, and with decreasing order, the other mutually orthogonal vectors are the next principal components.
In figure \ref{fig:pca_example} is reported a graphic example of two PC extracted from a set of two-dimensional data, along the direction where variance is greater, there's the first principal component and the second component is in the orthogonal direction.

\begin{figure}[h]
\centering
\includegraphics[width=.5\linewidth]{ml/pca_example.png}
\caption{Example of the first two principal components in a 2D dataset}
\end{figure}



%ration lambda_i/sum lambda_i ??

\section{How to assess network's performances}
Once the model has been trained, it is finally applied to the test set to assess the performances of the model to an previously unseen dataset.
A comomon metric for model performances' evaluation is Accurcay: it is simply defined as the ratio between the total number of correct prediction and the total number of predictions (total number of the test's data), so, using T and F for true and false, and P and N for positive and negative, accuracy is so defined
\begin{equation}
Accuracy = \frac{TP + TN}{TP+TN+FP+FN}
\end{equation}
Even though accuracy may seem a good parameter, it isn't the most accurate one, when we're dealing with a unbalanced test dataset, if for example, in a binary classification with A and B classes, if just 2 out of 10 samples belongs to a different class, let's say A and B, and the model predicts every data belonging to the class B, we will get a score of 80 \% accuracy, which is not a thruthful result because the model isn't making any distinction between the two classes.
There are then other ways for quality assessment that overcome this problem. One of this is based on the introduction of two quantities: precision and recall.
Precision is the ratio between true positive and total positive cases (TP + FP), which is a measure of how many positive predicted cases were actually positive, while recall measures the percentage of actual positives that were correctly classified, or in other words it represents the true positive rate.
\begin{equation}
Precision = \frac{TP}{TP+FP} \qquad \qquad Recall = \frac{TP}{TP+FN}
\end{equation}
Unfortunatelly precision and recall are linked in a sense that often enhancing one decreases the other and vice-versa.
%esempio page 283
There's therefore a trade off between recall and precision. If we want for instance a model to score more true positive, risking to have more false positives, we can set a threshold.
To reach less the x\% positive missing means set the recall to (100-x)\%, this operation is called setting the operating point and is done by setting a threshold.
This threshold though is not always estblished at the beginning, and operating point that would be the best suited for our classification goals is not clear a priori.
One way to summarize the information a precision-recall curve can provide us, is to compute the area under the curve, known as \textquotedblleft average precition \textquotedblright
What is done is study the model under all the possible thresholds.
This is achieved by studying the precision-recall curve of which is reported an example in figure \ref{fig:precisionrecall}.
The closer a curve lies on the upper right corner (high precision and high recall), the more correctly the model is working.

Besides the precision-recall curve, a similar curve is usually employed to study different thresholds' effect, is called the receiver operating characteristics curve, usually referred as ROC curve.
% Insert ROC curve computed using the random forest already implemented in my data.
The ROC curve is build from the true positive rate (recall) and the false positive rate (FPR) and shows the evolution of TPR vs FPR.
The ideal curve would be close to the top left (high tpr and low fpr) and the less accurate is the model, the more this curve tend to lay down to the bisector line.

To summarize the model's performances with a single number using ROC's curve information, we compute the Area Under the Curve AUC.
The reference value for an AUC is 0.5 which is obtained when a model is just randomly predicting and it corresponds to a curve laying on the bisector.
The AUC can be explained as the probability that randomly picked point from the positive class, will have an higher score (according to the model) than a randomly picked point from the negative class so that the percentage value of AUC is an estimate of the probability that the model is able to distinguish between the two classes.

An example of ROC curve is shown in figure \ref{fig:roccurve}


\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=1.\textwidth]{ml/precisionrecall_example.png}
\caption{Example of a precision-recall curve plotted for different thresholds values}
\label{fig:precisionrecall}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=1.\textwidth]{ml/roccurve_example.png}
\caption{Example of a ROC curve values of True positive and False positive rates for different thresholds values}
\label{fig:roccurve}
\end{subfigure}
\caption{}
\label{}
\end{figure}



\chapter{Explain a machine learning model using a game theory aproach}

As methods to learn pattern from data becomes more complex, they become harder to interpret, deep learning represent an example of a technique to search for nonlinear relations between data, but introducing nonlinearity, makes an outcome difficult to interpret and it ends up considering a machine learning model as a black box without any hint of its inner precesses that occurred when a result is output. But if a model perform well, or bad on a dataset, we would like to know why and what feature influenced the most the outcome of our model.

One state-of-art explainability algorithm is built making use of important concepts from game theory, for this reason in the following section we are going to explain some useful theory underlying explainatory models.


\section{Shapley values}
%Versione definitiva
Game theory is a branch of mathematic related on the study of mathematical models to  conceive social situations among competitive players \cite{ross-2021}

It have had a great development in the XX century expecially during and after the Second world war thanks to the contribution of mathematicians like John Von Neumann, John Nash and Lloyd Shapley.
Game theory mainly focuses on two main research areas: cooperative and non-coperative game.
\begin{itemize}
\item Non-cooperative game concern competition between individual player who don't cooperate each other and can't form coalitions. The main task on non-cooperative games can be summarised as the search for a good strategy for each player. Each player's objective is in fact the maximization of his own utility function.
A big contribution to the development of this theory came from von Neumann, or John Nash with the concept of Nash equilibrium \cite{nash-1950}.

\item Cooperative games concern competition between groups of player forming coalitions, each coalition plays as a single participant and earns a payoff; one of the the main tasks related to cooperative games is to find a way to divide the total utility among the member of a coalition in an equally way proportionally of how much they contributed to the final score.
\end{itemize}
In 1951 Lloyd Shapley in 1951 Introduced a way to compute the exact amount of payoff for each player making use of what were named after him: Shapley values \cite{shap-1951}\cite{shap-1953}.
Shapley introduced its values in the field of coalition games, therefore to properly understand his work, we need to briefly illustrate what a coalition game consists of.

A \textbf{coalition game} involves N players, and different subsets called coalitions created from these N players.
Each subset of players S gain a payoff at the end of the game.
A function $\nu$ maps every subset to its payoff $\nu(S)$ = payoff(S) $\in \Re$.
On the basis of which player had more influence in this final score, we ask how to split this payoff in a fair way between each player of the subset, where \textquotedblleft fair \textquotedblright is to be intended as to each, proportional to his own contribution.
A solution for this problem comes from \textbf{shapley values} $\phi_i(\nu)$.
They are specific for each player i $\in$ N in a coalition S $\subseteq$ N and the idea behind them is the marginal contribution of that player to the final score, where marginal contribution is defined as the difference on the score of the coalition when player i joins the coalition.
In other words they are the difference between the coalition's score with player i and the coalition's score without him $\text{marginal contribution = }\nu (S \cup \{ i \}) - \nu(S)$. \cite{algaba-2021}

Shapley defined his coefficients (Shapley values) for a player i as a weighted average of marginal contribution values, over all the possible subsets that include player i.

The mathematical formulation Shapley introduced for his value is
\begin{equation}
\begin{aligned}
\phi_i(\nu)  & = \frac{1}{N }\sum_{S\subseteq N\ \setminus \{ i \} }  {N- 1 \choose |S|}^{-1} [\nu (S \cup \{ i \}) - \nu(S)] \\
& = \sum_{S\subseteq N\ \setminus \{ i \} }  \frac{S! (N-1-S)!}{N!} [\nu (S \cup \{ i \}) - \nu(S) ]
\end{aligned}
\end{equation}
%That can be interpreted as $\frac{1}{N} \sum \frac{\text{marginal contribution of i to the coalition}}{\text{number of coalitions excluding i of the same size os S}}$
And they exactly represent the amount of reward for each player i.



\addcontentsline{toc}{chapter}{MATERIALS AND METHODS}
\chapter*{MATERIALS AND METHODS}

\chapter{Dataset: ABIDE I $\&$ II}


Data we are going to work on belong to the ABIDE dataset (Autism Brain Images Data Exchange): a project founded with the aim of investigate autism using MRI structural and resting state fMRI scans, to address the problem with these two different aproaches, collecting data acquired over the years from different medical centers, and putting them together in a single dataset.

The whole ABIDE dataset was published in two releases: ABIDE I released in August 2012 and containing 1112 patient scans, and ABIDE II released in June 2016 containing 1114 scans.
ABIDE I includes scans collected from 17 different sites, and the 1112 patients consist of 539 patients with ASD and 573 typical control patients.
ABIDE II includes scans collected from 19 different sites, and the 1114 patients consist of 593 patients with ASD and 521 typical control patients.
Not every site belonging to ABIDE II is different from those of ABIDE I, but, even though some medical centers are the same, the scanner type, or acquisition pipeline and parameters may have been changed during the time interval between the two releases, so in the following analysys, they are regarded as different acquisition sites.
For each site autism was diagnosed either by gold standard diagnostic instruments, clinical judgment or a combination of clinical gold standard procedures.

In addition to scan images, ABIDE provides every information related to each patient, as age, sex, intelligence quotient (FIQ), eye status during the scan (open or closed), and every additional information provided by patients.

The vast majority of patients are males as shown in the histogram \ref{fig:mf_site}, for a total amount of 1804 males and 422 females , while control/asd patient number, for each site is almost  for each site as
while the control/asd case per site is more or less balanced for almost every site, with the exception of KKI-1 that provided two times more controls than asd patients, and KUL-3 (Katholieke Universiteit Leuven) and NYU-2 (NYU Langone Medical Centre, Sample 2) that only provided ASD cases. For a visual comparison the number of controls/asd for each site is displayed on the histogram in figure \ref{fig:controlcase_site}

\begin{figure}[h]
\centering
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/controlcase_site.png}
\caption{Histogram of control and ASD subject per site from the whole ABIDE I \& II dataset}
\label{fig:controlcase_site}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/mf_site.png}
\caption{Histogram of male and females subjects per site from the whole ABIDE I \& II dataset}
\label{fig:mf_site}
\end{subfigure}
\caption{}
\label{}
\end{figure}


Patients in ABIDE dataset have ages ranging from 4 to $>$ 50 years old, but as shown in figure \ref{fig:abideages} the vast majority of participant are younger than 40, precisely $>$ 97 $\%$ of participant are under 40 y.o., also, the majority of sites provide only young patients in a restricted age range, but  as can be seen from figure \ref{fig:age_site} there are some sites that acquired patients with a wide age rande.

In our analysis we can restrict to patients belonging to an age range between 4 and 50.

Intelligence quotient, whose distribution is shown in figure \ref{fig:abidefiq}, was not provided for every participant, in fact 171 patients out of 2226, coming from sites UM1 and EMC1 (figure \ref{fig:fiq_site}) lack this information.
In our further analysis, before proceding these values were replaced by the average value of all the other provided values.
The lack of a common acquisition protocol is also evident from the eye status at scan feature: as shown in figure \ref{fig:abideeyesite} each site acquired scans either with open eyes or with closed, without a common method, and sometimes this information is not even specified.
In its entirely, the whole dataset consists of more than 70\% of patient acquired with open eyes, as shown in figure \ref{fig:abideeye}.

\begin{figure}
\centering
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/fiq_site.png}
\caption{Boxplot of patient's FIQ (together controls and ASDs) per site}
\label{fig:abideages}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/abide_age_site.png}
\caption{Boxplot of age of patients per site }
\label{fig:abidefiq}
\end{subfigure}
\caption{}
\label{}
\end{figure}



\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{abide/abide_ages.png}
\caption{Age distribution of the whole ABIDE I \& II dataset}
\label{fig:abideages}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{abide/abide_fiq.png}
\caption{FIQ distribution of the whole ABIDE I \& II dataset}
\label{fig:abidefiq}
\end{subfigure}
\caption{}
\label{}
\end{figure}



\begin{figure}
\begin{subfigure}{0.4\linewidth}
\includegraphics[width=\linewidth]{abide/abide_eye.png}
\caption{}
\label{fig:abideeye}
\end{subfigure}
\begin{subfigure}{0.7\linewidth}
\includegraphics[width=\linewidth]{abide/eye_site.png}
\caption{}
\label{fig:abideeyesite}
\end{subfigure}
\hspace{0.1 cm}
\caption{Histograms of patients with open, closed and unknown eye status in the whole ABIDE I \& II dataset (fig \ref{fig:abideeye}) and per site (fig \ref{fig:abideeyesite})}
\label{}
\end{figure}






\chapter{Image preprocessing}
\section{Common preprocessing steps}
\label{sec:preprocessing_steps}
For a better undertanding of how MRI and f-MRI and signal detection work, we refer to appendix..
(talk briefly about echo time and TR ?)

During an MRI and f-MRI scan session, data are usually acquired slice by slice, and the thinner is the slice, the more spatial resolution we can accomplish.
But there's a trade-off between spatial and temporal resolution: decrease the slice's thickness would lead to a better space resolution but at the cost of increasing repetition time to mantain the same Signal to Noise ratio (SNR).
This is because the signal is proportional to the number of hydrogen nuclei, which is proportional to the slice volume.

In order to obtain a strong BOLD signal, echo time plays a significant role as well: to obtain the maximum strenght signal it has to be set $T_E = T_2^{\ast}$ so in the case of BOLD signal it should be around 30 ms.
Images acquired using a shorter echo time have a weaker BOLD signal because of the lack of signal to detect. \cite{Triantafyllou2011}

Acquired data suffer from different source of noise and artifacts deriving from both hardware and physiologigal sources.
As an instance, breath rate can affect BOLD signal because of the induced local motion of the brain's vessels, or the change in blood oxygenation and pressure

One of the main artifact intrinsit to the signal nature is distortion and field inhomogeneities, deriving from making the scan sensitive to the BOLD signal which intrinsecally is the detection of a signal loss due to field distortion.
This could be corrected by employing small coils inside the scan to smooth magnetic field differences, this process called shimming still left some artifacts, therefore common data preprocessing pipelines use extra acquisitions to create a field map of the remaining field inhomogeneitie and a shift and stretching voxel to correct for them.

Head motion is due to the physical movement of the patient inside the scanner. It results in a misalinement from one acquired volume to the next.
Physiological noise is caused by the cardiac cycle and the breathing of the patient. They respectively provoke pulsatile motion of arteries, CSF and tissues and small head motion. As a second effect, the variable amount of air in the chest affect the magnetic field $B_0$.
To correct for head motion, motion correction steps are performed at the beginning. It works by spatially applying transformations as rotation or translation volume by volume, aiming to overlap every acquired slice to a chosen reference volume, like the first or that in the middle.

For EPI data, slice timing correction is usually performed as well. It aims to correct artifacts deriving from the sequentiality of acquisition for each slice of the brain is acquired at different time.
The entire time elapsed to acquire all the slices is called repetition-time, and it usually is from 1 to 3 seconds.
Slice timing correction uses interpolation in time to shift the BOLD timeseries of each voxel, in order to align them to a reference starting time.
The use of interpolation, though can lead to a slight loss of high frequencies information.

A further common step is spatial smoothing of both structural and functional data. It operates calculating a weighted average of each pixel over neighbored voxels. To this end, a gaussian kernel with a chosen FWHM is applied to create the weights.
Spatial smoothing is useful to avoid abrupt changes of signal between two neighbouring voxels.

Band-pass temporal filtering is commonly applyed to BOLD data, aiming to reduce artifacts from hardware like the slow changing in the baseline of the BOLD signal over time. A low pass filter removes high frequencies above a cut off frequency, it is commonly applied in processing resting state fmri data because the physiological signal is driven by low frequences oscillation while high ones are associated to noise.

A common further step is nuisance regression, which aims to reduce the structured noise: it works computing timecourses called nuisance regressors,
These signals include motion and BOLD signal fluctuation from white matter or cerebro spinal fluid.
From them, the variance is computed and this value is removed from the data using multi linear regression analysis.

When we need to run a group analysis, meaning analyse and compare different patient's images, one of the most relevant step is \emph{registration}: structural (T1) data are aligned over a standard coordinates system space to universally describe location of the different brain parts, to make sure that the same voxel coordinate corresponds to the same brain area for all the subjects.
The most common template, provided by packages like FSL is Tailarch and Monreal Neurological Institute's MNI-152 that replaced the previous Tailarch and Tournoux template and was adopted by the International Consortium of Brain Mapping (ICBM) as the international standard; it is also called ICBM152 for this reason.

To understand why MNI152 differs from the Tailarch and Tournoux we have to spend a few words about the latter. \footnote{https://doi.org/10.1038/nrn756}
The Teilarch and Tournoux atlas was published in 1988 and introduced some innovative aspects to tackle the problem of the great variability in brain anatomy between people which limited the accuracy of statistical analysis.
they introduced a common coordinate system to identify different brain locations, based on some anatomical landmarks, and introduced a spatial transformations to match different brains.
They choose two anatomical areas: the anterior commissure and the posterior commissure, which are relative invariant between different brains and conjuncted them with an axis.
An horizontal plane passing through this axis was chosen such that it was perpendicular with the interemishpere axis.
This way they created a 3D coordinate system called the Tailarch coordinate system.
Afterwards, to match different brains they described a set of spatial transformation, one for each different brain quadrant, to transform a brain to match another's principal anatomical structures.

From this template, a first MNI template was created, called MNI305, created by manual scaling 241 brains to the Tailarch template and averaging them to obtain  new template, and then transform 305 additional scans to this new template and averaging them to create a second average and final template (MNI305).

From this template, MNI152 was finally created and published in 2001, by registering 152 T1 scans to the MNI305 template and averaging them.

This template is used for the structural registration of a patient's brain.
When we work with functional images though, a second registration step occurs within each patient to align EPI (functional) data to the structural registered image of that subject.

Looking at picture \ref{fig:registrationchart} we can see an example of step by step functional and structural image registration to a MNI152 template: first the functional image is registered to the structural and next, they are both registered to MNI template.

Once the structural and functional images were registered to a standard space, is possible to extract brain region information using an atlas.

Atlases are in the same space as the template image (MNI or Tailarch space for example) and consist of a 3D standard brain template where different brain areas are marked with different color intensities to associate each area to a label.
This division into areas and the subsequent labeling, is called parcellization.

There are different atlases, each one including a different parcelization of the brain, which is obtained by divding it into N labeled ROIs
(Regions Of Interest) to focus on anatomical and/or functional regions, according to the study we are carrying out.


One of the most popular atlas is Harvard Oxford atlas obtained by manually labelling 37 MRI scans, aligning them to the standard MNI template and finally averaging each transformed label \cite{chappell-neuroimaging}.


\begin{figure}
\centering
\includegraphics[width=.3\linewidth]{mri/registrationchart.png}
\caption{Registration steps:
 starting from the first row there are the acquired functional and structural images and the MNI template respectively
 The second row shows the functional image registered on the structural space
 The third row shows the same images as above, but overlaid with red boundaries extracted from the structural image
 The fourth row shown the final images both registered to the MNI 152 template
}
\label{fig:registrationchart}
\end{figure}



\section{Implementation: CPAC, preprocessing pipeline, atlases}

ABIDE I data are available in a preprocessed format, the Neuro Bureau Preprocessing Initiative took care of data preprocessing and shared its results making them publicly available.
Four preprocessing software and aproaches were employed each from a different group and every one publicly available \footnote{http://preprocessed-connectomes-project.org/abide/cpac.html} for download.
They consist of data from ABIDE I preprocessed by using
\begin{itemize}
\item Connectome Computation System (CCS)
\item Configurable Pipeline for the Analysis of Connectomes (CPAC)
\item Data Preprocess Assistant for Resting-State f-MRI (DPARSF)
\item Neuroimaging Analysis Kit (NIAK)
\end{itemize}
The preprocessing steps implemented by the different softwares are similar, they differ on their foundational software (Python, MATLAB..) the algorithm implementation and their parameters and preprossessing steps' order.

But since only ABIDE I dataset is avaible preprocessed, we need to repeat the preprocessing procedures in order to obtain a dataset including both ABIDE I and II preprocessed with the same pipeline

In our work, we choose to preprocess data using CPAC (Configurable Pipeline for the Analysis of Connectomes): a configurable, open source pipeline, based on Nipype platform.
CPAC was run on a Docker container installed on a computer and our hardware and software setup consisted on:
\begin{itemize}
\item 16-core Intel i7-5960X processor and 64 Gb RAM
\item Ubuntu 20.04 operating System
\item CPAC 1.8.1 installed on Docker v. 20.10.11
\end{itemize}

We were allowed to run 3 patients in parallel, reserving 4 cores for each partecipant and up to 12 Gb memory to each patient, necessary to save intermediate-steps outputs.

We choose to employ CPAC because, basing on machine learning classification results, \cite{Yang2020} CPAC prove to be the most efficient preprocessing pipeline to preprocess ABIDE dataset.
CPAC employs tools like AFNI, FSL and ANTS to perform image correction of structural MRI and rs f-MRI.
Data were preprocessed following the same steps as the pipeline employed to create the ABIDE-preprocessed dataset.
This pipeline includes both antomical and functional preprocessing.
Anatomical pipeline steps consist in:
\begin{itemize}
\item Skull removal using AFNI's 3dSkullStrip
\item Tissue segmentation using FSL-FAST, to separate gray matter, white matter and CSF, uging a thresholding probability map whose threshold's values were set the same as ABIDE-preprocessed pipeline values
\item Registration to a standard template using ANTS, with a spatial resolution of 2mm
\end{itemize}
Functional pipeline consists of the following steps
\begin{itemize}
\item Slice timing correction using AFNI-3dTshift
\item Motion estimate and correction using AFNI-3dvolreg
\item Distortion correction using PhaseDiff and AFNI 3dQWarp
\item Create a brain-only mask of the functional data using AFNI 3DAUTOMASK
\end{itemize}
At the end of functional preprocessing steps, timeseries are extracted from each patient, making use of different atlases such as Automated Anatomical Labeled (AAL), Harvard-Oxford (HO), CC200, CC400, DesikanKilliany.
As explained in section \ref{sec:preprocessing_steps}, different atlases differ in numbers and types of ROIs
%An atlas is a 3D standard brain template, each one including a different parcelization of the brain, which is obtained divding it into N labeled ROIs (Regions Of Interest) for anatomical and/or functional analysis.
For example the DesikanKlein atlas employed in c-pac consists of 94 ROIs of which 32 cortical regions each side, 3 ROIs belonging to cerebellar vermis and 29 subcortical regions.
The Automated Anatomical Labeled created was created in 2002 \cite{mazoyer-2002} and consisted of 90 total ROIs, 45 each emisphere, since then, different modified and improved version were released, the last of which AAL3 consisting of 166 ROIs.
However, the AAL employed in CPAC is an intermediate version and consists on 116 ROIs\footnote{http://preprocessed-connectomes-project.org/abide/Pipelines.html}.

CC200 and CC400 are atlases created for functional parcellization, consisting of 200 and 392 ROIs respectively \footnote{http://ccraddock.github.io/cluster\_roi/atlases.html}.

According to previous studies \cite{spera-2019} the atlas that gave best classification performances was Harvard-Oxford, so this was the atlas we choose to employ for our work.
The H-O atlas consists of a subcortical and a cortical atlas, with a total of 117 ROIs of which 21 subcortical and 96 corticals (48 for each emishpere).
The H-O atlas employed by CPAC is provided by FSL library \footnote{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/}, and as includes both subcortical and cortical atlases, even though there's only 111 ROIs, namely 14 subcortical and 97 corticals.
The lacking ROI in the subcortical areas are L/R cerebral white matter, L/R cerebral cortex, L/R lateral ventricle and the brain stem \footnote{Subcortical ROIs https://neurovault.org/images/1700/ \quad Cortical ROIs https://neurovault.org/images/1705/ }.
In this unified HO atlas (subcortical and cortical) the entire cerebral cortex area was removed because is replaced by the finer parcellization of the cortical areas from the cortical atlas.
The extra cortical ROI in this H-O atlas is present in the 83th position and is named 3455; but there are no information abut this ROI neither on Harvard-Oxford documentation nor on CPAC's/FSL's documentation, and because is only made of 2 voxels it was excluded for our further analysis.

\begin{figure}[h]
\centering
\includegraphics[width=.4\textwidth]{mri/hoatlas.png}
\caption{Horizontal section of Harvard-Oxford subcortical (sx) and cortical (dx) atlased}
\label{fig:hoatlas}
\end{figure}


As is possible to notice from figure \ref{fig:confrontoabidepreproc} timeseries extracted after our preprocessing pipeline do not exactly match those from ABIDE preprocessed, even if it appears clear that the trend is the same, but there are some local differencies between the two plots.
This is most likely due to two main reasons: the differences in software version, and the lacking of a detailed step-by-step pipeline legend showing the value of all the sub parameters employed during the CPAC analysis pipeline.
Abide preproccessed data were obtained using one of the first version of CPAC; nowadays, after more than 7 years, CPAC and the libraries it relies on were upgraded several times and this may have slightly affected the final outcome of the process.
We found though the old pipeline configuration file employed on abide preprocessed \footnote{https://github.com/preprocessed-connectomes-project/abide}, but since it belongs to a previous version of CPAC, it lacked lots of sub-parameters and sub-settings added during these years.
For this reason, in our pipeline configuration file, parameters in common between our file and abide preprocessed' were set the same as abide's, and for all the others we choose to left the CPAC's default values.

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{abide/timeseries51456.png}
\caption{Comparison of 4 timeseries between ABIDE-preprocessed and our timeseries. Data show 4 randomly chosen ROIs: ROI 0, 6, 50, 108, belonging to patient 51456 from ABIDE I. Corresponding to ROI 10, 26, 1901, 4702}
\label{fig:confrontoabidepreproc}
\end{figure}




\chapter{Connectivity measures}
Once all 110 timeseries were extracted from each patient, we need to create a correlation matrix, comparing each timeseries with all the others.
The total number of combination that is possible to obtain from n timeseries is given by
\begin{equation}
N_{comb} = \frac{n\cdot(n-1)}{2}
\end{equation}
So in the case of HO atlas with 110 ROIs we obtain 5995 combination each one expressed by a correlation coefficient computed either as Pearson coefficients or resulting from same wavelet analysis.
\section{Correlation and Z-Fisher transform}

Pearson correlation often simply called correlation coefficient is the measure of a linear relation lying between two sets of data $x_1$ and $x_2$, is defined as the covariance of (x1, x2) over the product of the standard deviation of the two sets and has the important properties to be scale-invariant in magnitude.
%standard deviation of samples or of mean?
\begin{equation}
Corr(x, y) = r_{xy}=  \frac{Cov\left( x, y\right)}{\sigma_x \sigma_y} = \frac{E\left[ \left( x - \mu_x \right) \left( y - \mu_y \right)\right]}{\sigma_{x} \sigma_{y}}
\end{equation}
This correlation coefficient assumes values between -1 and +1 where the extremes correspond to exact anti-correlation or correlation respectively, so that, if a linear relation lies between the two sets, a high absolute value indicates that the two series tend to be simultaneously greater or lower than their respective means

Given two series x and y, of length n correlation can be easily computed by
\begin{equation}
r_{xy} = \frac{\sum_{i = 1}^n \left( x_i - \mu_{x_i}\right)\left( y_i - \mu_{y_i}\right)}{\sqrt{\sum_{i = 1}^n \left( x_i -\mu_{x_i}\right)^2}\sqrt{\sum_{i = 1}^n \left( y_i -\mu_{y_i}\right)^2}}
\end{equation}

For each patient the Pearson correlation coefficient was computed for each timeseries' pair. Before further analysis, a common way to proceed is to transform each coefficient with Fisher z-transformation.
The reason behind this common operation appears clear when we are dealing with high correlate variables, when Pearson correlation distribution results in an highly skewed distribution, Fisher's transform sought to transform it into a normal distribution of which the standard error is approximately constant equal to $\sigma = \/\sqrt{N-3}$ where N is the total number of points, and it does not depends on the values of correlation.

With this property, Fisher's transform is important also when we want to test some hypotesis about correlations, we can run our test with the transformed variables which are normal distribuited with a known variance.

Thus, this transformation allows us to obtain a variable which is normally distribuited even when Pearson correlation coefficients follow a bivariate normal distribution or they are leaning toward the extremes.
In our data, this difference in distributions is not pronounced, as is possible to notice from figure \ref{fig:feature1521_distribution} becuse on average we are not working with highly correlated or uncorrelated variables, but we are dealing with a bivariate distribution, then performing this transformation is a common practise to work with more normally-distributed variables.

\begin{equation}
z = \frac{1}{2}ln\left(\frac{1+r}{1-r}\right) = arctanh(r)
\end{equation}

At the end of this analysis, we obtain correlation matrices like that shown as an example in fig \ref{fig:corrmatrices}, referred to patient 20243 from ABIDE I dataset.
\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{corrmatrix.png}
\caption{}
\label{ref:corrmatrix}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{zscorematrix.png}
\end{subfigure}
\caption{Correlation and z-values matrix computed from timeseries extracted using Harvard-Oxford atlas, from patient 20243 belonging to ABIDE I dataset}
\label{fig:corrmatrices}
\end{figure}


%\begin{figure}[h]
%\centering
%\includegraphics[width=.5\textwidth]{}
%\caption{Distribution of feature 1521: Pearson correlation values in lightblue and Fisher transformed values in orange}
%\label{fig:feature1521_distribution}
%\end{figure}




\section{Wavelet analysis}
A different aproach to compute a correlation coefficient is by making use of wavelet analysis.\cite{ferrante-2015} \cite{vandenberg-1999}
Wavelet is a mathematical tool for analyzing time series or images, and provides a comprehensive way for investigating the bivariate relationship between timeseries both in time (or space) and frequency domain.
%A second type of correlation analysis was performed on timeseries by using wavelet analysis.
To understand wavelet transformation it could be helpful to compare it with Fourier analysis.
Fourier analysis allows us to expand a periodic function f(t) into series, ideally infinite sums of weighted sines and cosines with different frequencies
\begin{equation}\label{eq:fourier_series}
f(t) = a_0 + \sum_{k = 1}^\infty (a_k cos(2\pi k t/T) + b_k sin (2\pi k t /T))
\end{equation}
In equation \ref{eq:fourier_decomposition} terms corresponding to the k-th frequency are called harmonics and are multiples of the fundamental frequency corresponding to k = 1.
%modificatoo
The Fourier Transform is a natural extension of the Fourier series to aperiodic functions defined over the real axis, which don't allow a discrete superposition of sines and cosines terms, and for this reason they need to be represented by a continue superposition.
The FT transform takes a function from time or space domain and turns it into spatial or temporal frequency domain.
is a complex function because sines and cosines terms can be represented by a complex exponential.
\begin{equation}\label{eq:cft}
\tilde{F}(\omega) = \int\limits_{-\infty}^{+\infty} f(t)e^{-i \omega t} dx
\end{equation}
To deal with discrete signal, for example a sequence $x_n$ obtained from a discrete sampling of the continuous signal x(t), is possible to make use of the Discrete Time Fourier Transform, wich allows us to write the $\tilde F(\omega)$ as in equation \ref{eq:dft} after the right arrow.

The Discrete Fourier Transform, is useful to analyse discrete periodic signal, it acts on a function defined on a finite domain returning a sequence of samples of the discrete fourier transform sampled at an interval equals to the reciprocal of the duration of the input sequence.
%é vero o dipende dall'implementazione della discrete fourier transform?

\begin{equation}\label{eq:dft}
 \tilde{F}(k) = \tilde{x}_k = \sum_{n = 0}^{N-1}x_n e^{\frac{-2\pi i k n}{N}}
\end{equation}
where N is the total number of points of the timeseries x(t).
The frequencies at which samplings are computed are $\omega_k = 2\pi k/N$.
The series we obtain from relation \ref{eq:dft}: $\tilde{x}_k$ is periodic and its period it equal to N.

One of the main drawback of FT though, is the lacking of spatial information, for example for a non stationary signal, is possible that the signal contains a variable frequency component in different space or time locations.

A time-frequency analysis provides for this lacking computing both frequency and spatial information from a signal, allowing us to analyse non stationary signals and obtain informations both on time (or space) and frequency domain.
Since we are working on signal defined over a time domain, from now on we would refer only to  \textquotedblleft time \textquotedblright  and  \textquotedblleft frequency \textquotedblright, but we keep in mind that all the relation are true for spatial and spatial frequency domain as well.

The process through which we exctract informations about frequencies (F) and time location (T) is a convolution of the signal x(t) with a function w(t) called windowing function usually centered at zero which rapidly decays symmetrically.
\begin{equation}
\tilde{x}(F, T) = \int_{-\infty}^{\infty} x(t)w(t-T) e^{-2\pi i F t} dt
\end{equation}
The convolution term $w(t-T) e^{-2\pi i F t}$ changes according to the type of analysis we are performing. Parameters F and T are often simply called a and b, where b is a time translation parameter, and it acts the the same way through all the type of analysis, by simply shifting our convolution function throughout the entire timeseries x(t).
On the other hand a is the parameter that differenciate the most different kinds of analysis: it is the frequency parameter and the way it is introduced in a convolution determines the developing of the transform.
The two main types of time-frequency analysis are

\begin{enumerate}
\item Windowed Fourier Transform (WFT): $\psi_{ab}(t) = e^{it/a}\phi(t-b)$ where $\phi(t)$ is a window function of constant width and the parameter a acts as frequency modulation (freq $\sim$ 1/a): the lower is a, the greater the number of oscillation inside the window $\phi(t)$.
\item Wavelet Transform (WT): $\psi_{a, b}(t) = \frac{1}{\sqrt{a}}\psi(\frac{t-b}{a})$ where $\psi$ is a function called \emph{mother wavelet}; a is a positive real number and defines the dimension of $\psi$: with a$>$1 we obtain a dilation and a$<$1 corresponds to a contraction. b is any real number (positives and negavies) and defines the location of the wavelet in time.
\end{enumerate}

In Wavelet Transform, the equivalent to the window function in WFT is a wavelet function $\psi$.

A wavelet, literally \textquotedblleft small wave \textquotedblright is a wave function limited both in space and period: begins at zero, grows and decrease in a limited time period and returns to zero. So that is a function local in both the temporal and frequency domain.

To be defined as so, a wavelet is required to satisfy two properties: to have zero mean (it must be oscillating) and unitary squared norm \cite{percival-2013}.

\begin{equation}\label{eq:waveletproperties}
\int_{-\infty}^{\infty}\psi(x)dx = 0 \qquad \qquad  \int_{-\infty}^{\infty} |\psi(x)|^2 dx = 1
\end{equation}

It is defined over the space $L^2$ of Lebesgue measurable functions that are both absolutely integrable and square integrable. In this space the two properties can be satisfied. A wavelet transform is then formally a mapping from $L^2(\Re) \rightarrow L^2(\Re^2)$.
%This kind of transformation of going in a higher dimensional space is a redundant operation

%modificatoo
There are different wavelets that satisfy properties \ref{eq:waveletproperties}, such as Haar (fig \ref{fig:wavelet_haar}), Meyer (fig \ref{fig:wavelet_meyer}) \cite{Daubechies}, Mexican hat (fig \ref{fig:wavelet_mexicanhat})
or Morlet (fig \ref{fig:wavelet_morlet}) \footnote{https://it.mathworks.com/help/wavelet/gs/introduction-to-the-wavelet-families.html}.



\begin{figure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{wavelet/haar.png}
\caption{}
\label{fig:wavelet_haar}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{wavelet/mexicanhat.png}
\caption{}
\label{fig:wavelet_mexicanhat}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{wavelet/meyer.png}
\caption{}
\label{fig:wavelet_meyer}
\end{subfigure}
\caption{Visual comparison between three different types of mother Wavelets, Haar wavelet (fig \ref{fig:wavelet_haar}) is a simple square wave function, mexican hat (fig \ref{fig:wavelet_mexicanhat}) is a Wavelet belonging to gaussian function family and represents the negative normalized second derivative of a gaussian function; and Meyer wavelet (fig \ref{fig:wavelet_meyer}) a wavelet with applications in fields like adaptive filters}
\label{fig:different_wavelets}
\end{figure}




What we are going to use in our analysis is Morlet wavelet, defined by equation \ref{eq:morletwavelet} and shown in figure \ref{fig:wavelet_morlet}.
With this wavelet the trade-off between time and frequency resolution can be controlled by the choice of $\omega_0$.\cite{muller-2004}
We choose to run our analysis with a value of $\omega_0 = 6$ since it provides a good balance between the two resolution \cite{grinsted-2004} and besides, this value gives the best ratio between Fourier Period and the scale parameter a during a Transform, equal to $\lambda = 1.03$.
This way results in frequency domain are more interpretable since the scale parameters a used for the Transform is almost equal to the Fourier period.

%omega 0 = 6: \footnote{https://it.mathworks.com/matlabcentral/answers/824015-understanding-cwt-morlet-time-and-frequency-resolution?s_tid=srchtitle}

\begin{equation}\label{eq:morletwavelet}
\psi\left(t\right) = \pi^{-1/4}e^{iw_0 t}e^{-t^2/2}
\end{equation}

\begin{figure}[h]
\includegraphics[width=.4\linewidth]{wavelet/morlet}
\caption{Morlet Wavelet}
\label{fig:wavelet_morlet}
\end{figure}

Two main type of analysis can be performed using wavelets: continuous wavelet transform (CWT) and discrete wavelet transform (DWT) and the CWT is what we employed for our analysis.

Continuous wavelet transform decomposes a time series in time-frequency domain by successively convolving the timeseries with several scaled and translated version of the mother wavelet $\psi$: $\psi_{a, b}\left(t\right) = \frac{1}{\sqrt{a}}\psi(\frac{t-b}{a})$.
If the timeseries is described by a function f, assumed to be real in the equation below, the convolution of f and $\psi_{a, b}\left(t\right)$ is

\begin{equation}
W_{a,b}(f) = \int_{-\infty}^{+\infty}  f(t) \cdot \psi_{a, b}^\ast \left(t\right)   dt
\end{equation}
Where the $\ast$ indicates the complex conjugate.
This integral is performed for different values of a and b.
It can be useful to visualize this integral as: set a value for $a$, a wavelet centred in $b$, is slided across the signal by changing the value of $b$ and for each of these values, a coefficient is extracted by integrating the product between the wavelet and the signal; in this way, coefficients are function of frequency (or scale) and time. This operation is repeated for different values of a and b, and allows us to assemble a matrix called \emph{scalogram}, which is basically a plot of these coefficients in time-frequency domain.

To computationally implement CWT, a discretized version was implemented on MATLAB tools Wavelet toolbox, but it does not be confused with the Discrete wavelet transform \footnote{https://it.mathworks.com/help/wavelet/gs/continuous-and-discrete-wavelet-transforms.html}

The difference between the continuous wavelet transform implemented on MATLAB and discrete wavelet transform lies in how finely stretching and shifting parameters are sampled: the CWT discretizes scale more finely than the discrete wavelet transform.

In the CWT, parameters are discretized based on a fractional power of two, by setting a = $2^{j/\nu}$  \footnote{This way, the discretized wavelet can be written as $\frac{1}{2^{j/\nu}} \psi\left ( \frac{t-b}{2^{j/\nu}}\right )$ } \cite{liu-1994} \cite{dabauchies-1992} where $\nu $, j are integers.
The parameter $\nu$ is often referred to as the number of “voices per octave” because increasing the scale by an octave (namely to double the frequency) requires $\nu$ intermediate steps, for example from $f = f_0 2^{\nu/\nu} \ to \ f = f_0 2^{2\nu/\nu}$ , $\nu$ steps are required.
The larger the value of $\nu$, the finer the discretization of the scale parameter.
In the DWT, the scale parameter is always discretized to integer powers of 2, $2^j$, j=1,2,3,..., so that the number of voices per octave is always 1.
Since it employs a more rough discretization, DTW is usually used for denoising and compression of signals and images.

In our analysis we are going to use DTW implemented in matlab with the default parameters of 12 voices per octave.

%Using CWT is possible to obtain singal's information on a matrix: a time-frequency plot also called scalogram by convolving the signal, using different wavelets parameters a and b from a grid

One problem that arises from working on a timeseries, which is a signal with a finite support, is that when a wavelet is located at the beginning of at the end of the signal, the wavelet extends itself outside the boundary of the signal, and the convolution would require nonexistent values beyond the boundary.
The confidence value obtained is then lower than other obtained for central values of time location.
To overcome this problem it would be possible to accept this data information loss and truncate the values beyond boundaries; whereas an other aproach could be to artificially extend data using methods such as zero padding which assumes that the signal is zero outside its original support, or symmetrization which extend the signal symmetrically outside the boundaries or smooth padding which recovers a signal by extrapolating values from the first derivative values or from the signal itself.
Symmetrization method is the one employed for the subsequent analysis.
Areas of the scalogram affected by these edge effects are indicated as \textquotedblleft outside the Cone of influence (COI) \textquotedblright.

Figure \ref{fig:cwt_timeseries} shows the timeserie belonging to ROI 2201: Right Angular Gyrus of patient 51056 from ABIDE I, and its corresponding Continuous Wavelet Transform.
x axis corresponds to time points and on the y axis frequencies or periods derived from the parameter a can be represented.Just as an example, we choose to display the y axis as periods.
The COI is shown as a dotted white line and values outside the COI, where edge effect becomes effective are shown with a lighter faded.
The color is a visual representation of the magnitude of each wavelet coefficient.

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/timeseries.png}
\caption{}
\label{fig:timeseries_cwt}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/cwt_timeseries.png}
\caption{}
\label{fig:cwt}
\end{subfigure}
\caption{Timeseries (fig \ref{fig:timeseries_cwt}) of ROI 2201: Right Angular Gyrus of patient 51056 from ABIDE I, and its corresponding Continuous Wavelet Transform  (fig \ref{fig:cwt}).
x axis corresponds to time points and y axis the periods derived from the scale parameter $a$ of the wavelet convolving function.
The COI is shown as a dotted white line and values outside the COI, are shown with a lighter faded.
The color represents the magnitude of each wavelet coefficient.}
\label{fig:cwt_timeseries}
\end{figure}



\paragraph{Wavelet coherence} \hfill \newline

Continuous wavelet transform can only be used to analyse one signal at a time; if our goal is to analize and compare two signals using wavelet transform, the analysis to perform is called wavelet coherence.

From two Continuous Wavelet Transforms is possible to compute the Cross Wavelet Transform (XWT) which allow us to examine their cross-wavelet power and relative phases.

Using XWT is then possible to compute the Wavelet Coherence.

Denoting as $W^{X}(a, b)$ the continuous wavelet transform of a signal X, is defined the \emph{wavelet power spectrum} of a signal X(n) as
\begin{equation}
W^{XX}(a, b) = W^X(a, b) \left[W^X(a, b)\right]^{\ast}
\end{equation}
where the $^{\ast}$ represent the conjugate transpose.
Similarly, the \emph{wavelet cross spectrum} of two time series X and Y is defined as
\begin{equation}\label{eq:cross_wavelet_spectrum}
W^{XY}(a, b) = W^X(a, b)[W^Y(a, b)]^\ast
\end{equation}
whose module $|W^{XY}(a, b)|$ represents the amount of joint power between the two time series, and is called \emph{cross wavelet power}.
From the cross wavelet spectrum (eq \ref{eq:cross_wavelet_spectrum}), is possible to compute the the complex argument
\begin{equation}
\Delta \phi (a, b) = arctan\left(\frac{Im\left[ W^{XY}(a, b)\right]}{Re\left[ W^{XY}(a, b)\right]} \right)
\end{equation}
which represents the relative phase between X and Y, for each given value of the parameters $a$ and $b$, and is defined over the interval $[-\pi, \pi]$.

The wavelet coherence $R^2(a, b)$ is finally defined as reported in equation \ref{eq:WC}. This coefficient ranges in the interval (0, 1) and represents the localized correlation coefficient between X and Y in the time-frequency domain.
\begin{equation}
R^2(a, b) = \frac{|S(W^{XY}(a,b))|^2}{S(|W^X(a,b)|^2) S(|W^Y(a,b)|^2)}
\label{eq:WC}
\end{equation}

Where S is a smoothing operator both in frequency and time defined as \cite{torrence-1999} \cite{grinsted-2004}
\[
S = S_{scale}S_{time}(W) \quad S_{time} = W\cdot c_1 ^{\frac{-t^2}{2a^2}} \quad S_{scale}(W) = W\cdot c_2 \Pi(0.6)
\]
where $c_1, c_2$ are normalization constants, $\Pi$ is a boxcar (rectangle) function and 0.6 is an empirically determined factor for Morlet Wavelet \cite{torrence-1998}.

An example of Wavelet coherence scalogram is shown in figure \ref{fig:wcoherence_scalogram}. It represents the wavelet coherence computed from two timeseries (fig \ref{fig:wcoherence_timeseries})of patient 51056: ROI 57 corresponds to Right Angular Gyrus and ROI 65 to Right lateral occipital cortex.
The color represents the magniture of the cross-power, and phase information is represented as oriented arrows, pointing towards an imaginary 360 degrees circle, where the zero phase shift is represented by an arrow pointing right and a 180 degrees, by a left arrow.



\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/timeseries_for_wcoh.png}
\caption{}
\label{fig:wcoherence_timeseries}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wavelet/wcoherence.png}
\caption{}
\label{fig:wcoherence_scalogram}
\end{subfigure}
\caption{Plot of two timeseries from patient 51056 in figure \ref{fig:wcoherence_timeseries} and their relative Wavelet Coherence scalogram.
On the x axis the timepoints and on the y axis the frequencies of this time-frequency decomposition.
The color represents the magnitude of the cross-power coefficients and the relative phase shift is represented as arrows pointing to the right is phase shift is zero ad to the left is is 180 degrees. Starting from 0 degrees arrows rotate counterclockwise.
}
\label{fig:wcoherence}
\end{figure}



Our goal is to extract from this Wavelet coherence matrix, a single value, interpretable as a correlation coefficient, just like we did with Pearson coefficients.
We can accoplish so, by extracting two types of information from this scalogram: the magnitude of the correlation and relative phase between the two signal.


We can extract the magnitude information of each entry of the scalogram, but before doing so, we need to assess the level of significance of this coherence matrix over the noise, this way we can estimate the statistical significance level of our values, and only take the significative ones.

The theoretical procedure \cite{grinsted-2004} \cite{bernas-2018} is to generate, using Monte Carlo methods, a large (1000) samples of wavelet coherence matrices using red noise timeseries. \cite{hartmann-2014}
These red noise samples should be generated, for each time series, with the same 1-lag autoregressive coefficients (AR1 coefficient) as the two timeseries we are computing wavelet coherence on, and then, for each pair of red-noise timeseries, we should compute wavelet coherence matrix.
However, since AR1 coefficients have little impact on the significance level \cite{grinsted-2004}, we choose to generate a single sample of 1000 pairs of red noise and for each pair we computed the wavelet coherence matrix.

For each pair of noise, the corresponding wavelet coherence matrix is stacked to obtain a $A\times B \times x \times 1000$ 3D noise matrix to extract the distribution of the entries.
This null distribution of wavelet coherence coefficient is used to estimate the threshold to 5\% significance level for the subsequent analysis.
We refer to the value corresponding to this 95-th percentile as $a_{95}$.


As a second step we extract the relative phase information of each entry of the matrix, and combining these two informations together, we can estimate the time of in-phase (or out of phase) coherence which can be seen as the percentage of time synchronicity (or antisynchronicity) between the two timeseries. \cite{bernas-2018}
This time of in-phase coefficient is defined as:

\begin{equation} \label{eq:wcohcoefficients}
c_{ij} = \frac{100}{N}\sum_{a, b}^N I\left\{ R_{ij}^2(a,b) > a_{95}\right\}\cdot I\left\{-\frac{\pi}{4}<arg(W^{XY}(a, b))_{ij} < \frac{\pi}{4}  \right\}
\end{equation}

Where the indices i and j refers to the timeseries, N is the total number of points inside the cone of influence (COI) $I\left\{ ...\right\}$ is either 0 or 1 depending on if the condition inside is satisfied; $a_{95}$ is the threshold value above which the computed Wavelet coherence coefficient is regarded as significative.

Similarily to this coefficient is the time of counter-phase coherence modifying from the formula above the phase condition into

\[
I\left\{arg(W^{XY}(a, b))_{ij} < -\frac{3\pi}{4} \ \lor \ arg (W^{XY}(a, b))_{ij} >\frac{3\pi}{4} \right\}
\]

In short with this analysis, we are just considering coefficients with an high significance level (above 95\%) and with a small $\in [-\pi/4, \pi/4]$, or big phase shift ( $< -\frac{3\pi}{4}  \lor  >\frac{3 \pi}{4}$).

Wavelet coherence maps were calculated using MATLAB's Wavelet Toolbox, which employs the Morlet wavelet, for the wavelet decomposition, as  mentioned before, and decomposes the frequency range using 12 subscales per octave and 9 octave.

%Figure \ref{fig:wcoherence} represents the wavelet coherence scalogram obtained from two ROIs of patient 51056.

From equation \ref{eq:wcohcoefficients} both coefficients in-phase and anti-phase matrix coefficients were computed.
The correlation coefficients matrix for each subject, were then flattened and used as input to the neural networks.
An example of correlation matrix created with in-phase and out-of-phase coefficients from data of patient 51056 is shown in figure \ref{fig:win_wout}



\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{wavelet/win_corrmatrix.png}
\caption{Coefficients of In-phase percentage}
\label{}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{wavelet/wout_corrmatrix.png}
\caption{Coefficients of Off-phase percentage}
\label{}
\end{subfigure}
\caption{Correlation matrix created with Wavelet coefficients of in-phase and off-phase percentage. Patient 51056, ABIDE I }
\label{fig:win_wout}
\end{figure}




\newpage


\chapter{Harmonization}
Since nw, a big percentage of neuroimaging studies have been carried out with limited data acquired from a single centre, to minimize varibility in data due to the different instrumentations employed.
In recent years, however, there's the tendency to create shared datasets, by pooling together data acquired from different centers.
A positive side of creating a large dataset putting together data from multiple centers is the possibility to work on dataset of bigger dimension, resulting in obtaining more statistically accurate analysis, and facilitates the generalization and the robustness of the model.
As a drowback, however, it introduces variability in our analysis such as differences due to scanner models, acquisition parameters, generally known as scanner effects.
A level out procedure becomes necessary to avoid the model to learn these differences deriving from inomogenity of data because of differences in data acquisition procedures.
This procedure is generally referred as harmonization.
Harmonization was tested on different datasets and has proven to be an effective way to reduce scanner effects  in different kind of datasets such as genes' microarray data, diffusion tensor imaging or structural MRI. \ref{johnson-2007} \ref{fortin-2017}.


\section{Harmonization - theory}\label{sec:harmonizationtheory}


The state-of-art procedure used in genomic is called ComBat (named after Combating Batch effects), used for structural MRI but aplicable to any kind of imaging data, is used to mitigate scanner effects.
It is based on a previous method initially proposed in 2007, for gene expression studies to compute batch effect corrections, later implemented by Fortin et al \ref{fortin-2018} for the harmonization of cortical MRI volumes, and finally in its current improved version, developed by Pomponio et al. \cite{pomponio-2019} in 2019.
In this paragraph we are going to explain how ComBat technique works and after that, we'll see how it was modified and improved in the latter implementation by Pomponio et al.

ComBat technique is based on simpler Location and scale adjustment family methods, but it represents an improved version of them: it uses the empirical Bayes to improve the estimation of the site parameters.
It aims to reduce inter-site variability while preserving biological variability such as differences due to sex, age, FIQ (Full intellective quotient), ICV (Intra Cranical Volume) and so on.
The model assumes that a feature can be modeled as a linear combination of the biological variables plus the site effect, and the errors introduced with site effect can be modeled as both a multiplicative and an additive term and that it can be standardized by adjusting the mean and the variance across the batches.

Let $y_{ijf}$ be the numeric value of the feature f for the patient i acquired with the scan (or equivalently, from the site) j, so that i = 1, 2, ..., K indexes the scanner, j = 1, 2, ..., $N_i$ indexes the total number of subjects acquired for each scanner i, and f ranges from 1 to F being F the total number of features. Besides, let's assume that this value y, can be written as
\begin{equation}
y_{ijf} = \alpha_f + x_{ij}^T \beta _f + \gamma_{if} + \delta_{if} \epsilon_{ijf}
\end{equation}
where:
\begin{itemize}
\item $\alpha_f$ is the mean value for the feature f,
\item $x_{ij}$ is the entry of the matrix X created with the covariates of interest such as age, sex,
\item $\beta_f$ is the vector of regression coefficients corresponding to X for the feature f,
\item $\gamma_{if}$ and $\delta_{if}$ represent the additive and multiplicative terms for site-i effects related to feature f respectively,
\item $\epsilon_{ijf}$ is a error term which is assumed to follow a normal distribution with zero mean and variance $\sigma_f^2$.
\end{itemize}

The final location-and-scale-adjusted data $y^{\ast}_{ijf}$ are given by
\begin{equation}\label{eq:harmonized_data}
y^{\ast}_{ijf} = \frac{y_{ijf} - \hat \alpha_f - X\cdot \hat \beta_f - \hat \gamma_{jf}}{\hat \delta_{jf}} + \hat \alpha_f + X \cdot \hat \beta_{jf}
\end{equation}
where $\hat \alpha_f , \hat \beta_f , \hat \gamma_f ,  \hat \delta_{jf}$ are estimators of the corresponding parameters, based on the model.

One of the most disadvantages of using a simple location and scale batch adjustment is that it requires a large batch size for the implementation because is not robust to outliers in small sample sizes.

ComBat uses empirical Bayes to provide a more robust adjustment for the parameters $\hat \gamma_{if} \ \hat \delta^2_{if}$, making the model able to deal with small-sized dataset as well.

%Specifically it is assumed that that the site-effect parameters follow the normal distribution and the inverse gamma distribution respectively
%\begin{equation}
%\gamma_{if} \sim N(Y_i, \tau^2_i) \qquad \delta^2_{if} \sim \frac{\beta^{\alpha}}{\Gamma (\alpha)}(1/x)^{\alpha +1}\cdot e^{-b/x}
%\end{equation}

The process ComBat employs to estimate feature-dependent parameters, and to adjust data for batch effect can be summarized in 3 steps:

\begin{enumerate}
\item \textbf{Standardization}: Data are standardized feature-wise, so that every feature has similar overall mean and variance.
least square regression, is then performed to determine parameters $\hat \alpha_f, \hat \beta_f, \hat \gamma_{jf}$ and subsequently, $\hat \sigma^2_f = \frac{1}{n} \sum_{ji} \left ( y_{ijf} - \hat \alpha_f - X \hat \beta_f - \hat \gamma_jf \right )^2$ being n the total number of patients.


The standardized data are then calculated by equation \ref{eq:harmonization_std_data}

\begin{equation}\label{eq:harmonization_std_data}
Z_{ijf} = \frac{y_{ijf}-\hat \alpha_f - X \hat \beta_f}{\hat \sigma_f}
\end{equation}


\item \textbf{Empirical Batch parameters estimate}:
%Site effect parameters estimate using parametric empirical priors:
We assume that standardized data follow a normal distribution $Z_{ijf} \sim N(\gamma_{jf}, \delta^2_{jf})$ and we seek for a proper estimation of parameters $\gamma_{jf}, \delta^2_{jf}$.
it is also assumed that these site-effect parameters follow the normal distribution and the inverse gamma distribution respectively
\[
\gamma_{jf} \sim N(\eta_j, \tau^2_j)  \qquad \delta^2_{jf} \sim \text{Inverse Gamma}(\lambda_j, \theta_j) =  \frac{\theta^{\lambda}}{\Gamma (\lambda)}(1/x)^{\lambda +1}\cdot e^{-\theta/x}
\]

And these hyperparameters $\eta_j, \tau^2_j, \lambda_j, \theta_j$ are empirically estimated from standardized data  $Z_{ijf}$ by using the method of moments.
%Breve box to exmplain method of moments??
we then obtain improved estimation of parameters $\gamma^\ast_{jf} \ \text{and} \ \delta^\ast_{jf}$ and we use them for the third and last step, where we adjust our data using them.

\item \textbf{Adjust the data}: After calculated the site-effect parameters we are finaly able to adjust our initial data using the relation

\begin{equation}
y^{ComBat}_{ijf} = \frac{\hat \sigma_f}{\delta^\ast_{jf}}(Z_{ijf} - \gamma^\ast_{jf}) + \hat \alpha_f + X \hat \beta_f
\end{equation}
which is just an equivalent way to write equation \ref{eq:harmonized_data}, using parameters estimated in the previous passage.
\end{enumerate}


So far, this was the main idea behind ComBat technique, but we said before that the last implementation of this technique by Pomponio et al. \cite{pomponio-2019} is an improved version of it.
They differn in the modelling of the biologically covariates, which in the formulas above are expressed by the terms $\hat \alpha_f + X \hat \beta_f$ which is a simple linear model.
Pomponio substituted this linear model with a Generalized Additive Model (GAM). In this model covariates such as sex, age, FIQ, are represented by terms $x_{ij}, \ z_{ij}, \ w_{ij}$ which allow a better parametric modelling to deal with non-linear trends such as the trend of cortical thickness in relation to age.
This way, the terms $\hat \alpha_f + X \hat \beta_f$ in equation \ref{eq:harmonized_data} are substituted by a linear combination of function F of these covariates
\begin{equation}
\hat \alpha_f + X \hat \beta_f \ \longrightarrow \ F_f(x_{ij}, z_{ij}, w_{if}...) = a_f + g_f(x_{ij}) + h_f(z_{ij}) + p_f(w_{ij}) + ...
\end{equation}
Where functions $g_f, h_f, p_f$ can be either linear or non-linear functions of our covariates, according to how we want to model these covariates, or in other words, according to wether we want to specify some kind of non-linear relationship between covariates and our data.





\chapter{Domain-adversarial NN}\label{sec:domain_adversarial_theory}
An other approch is trying to train a neural network to recognise discriminative features, making train as much site-independent as possible.
The general idea underlying this network comes from a reviewed and readapted version of the solution of the problem tackled in Ganin - Ustonova article
%Insert article reference
Their work takes place when we possess two different datased let's call them a Source and a Target dataset, containing features following two different distributions.
Moreover, source dataset is labeled while Target is unlabeled, so their goal was to create a network able to distinguish relevant features from the labeled dataset and make them domain independent.
We illustrate the main aspects of this problem using a shallow neural network with a single hidden layer consisting of D nodes.
Let's suppose the network takes as input an m-dimensional features vector;
the hidden layer is then a function $G_f : R^m \rightarrow R^D$ of the type $G_f(x; \textbf{W}, \textbf{b}) = f(W\cdot x + b)$, where $\textbf{W}$ and $\textbf{b}$ are the matrix and vector parameters to be optimized.
Similarly the output will be a function $G_y:R^D \rightarrow [0, 1]$ of the type $G_y(G_f(x; W, b); V, c) = f'(V\cdot G_f(x) + c)$.
Where V and c are parameters to be optimized: a vector and a scalar respectively.
A usual train carried on only on the (labeled) Source domain, will therefore bring to the minimization of the quantity
\begin{equation}
\underset{W, b, V, c}{min} \left[ \frac{1}{n}\sum_1^n L_y^i(Gy(G_f(x_i)), y_i | W, b, V, c)\right]
\end{equation}.
To tackle the domain independence problem, an other term is added to the summation: a term coming from a domain classifier layer: $G_d:R^D \rightarrow [0, 1]$ of the type $G_d(G_f(x; W, b); u, z) = f'(V\cdot G_f(x) + c)$ that learns the vector and scalar parameters u and z, by trying to classify the domain.
With this addition, the complete quantity to be optimized becomes
\begin{equation}
%complete this equation
E(W, b, V, c, u, z) = \frac{1}{n} \sum_{i = 1}^n
\end{equation}
So the purpose of this kind of training is to find a saddle point by finding minimizing parameters for feature loss and maximizing those related to the site loss, namely
\begin{equation}
(\hat \theta_f, \hat \theta_y) = \underset{\theta_f, \theta_y}{argmin} E(\theta_f, \theta_y, \hat \theta_d) \qquad \qquad \hat \theta_d = \underset{\theta_d}{argmax} E(\hat \theta_f, \hat \theta_y, \theta_d)
\end{equation}

This could be achieved by training a network in an adversarial way, trying to minimize the category label loss and maximize site label loss.
this is achieved by adding a domain classifier branch just after a main feature extractor branch.
The domain classifier branch is characterized by a \emph{gradient reversal layer} at the top of it.
This layer has no parameters and don't act during forward propagation, but, during backpropagation, it inverst all the weight from the subsequent layer before passing them to the preceding one.
So as shown in figure -insert figure- the network is forked after the feature extractor branch, on one side, he starts the label predictor branch, and on the other, there's the domain classifier branch with the gradient reversal layer at the beginning.
The setup of this work, differs from Ganin's because we don't have a labeled Source and an unlabeled Target domain, but they are both labeled, so there's no need to keep them separated and also, in our case, the Target domain is not just made of two domains, but of as many sites our data belong to.


In this work, the label classifier branch ends with a sigmoid function wich return only one class label $y_i \in [0, 1]$ while the domain classifier branch ends with a multi-node layer of a number equal to the total sites' number.
An adversarial loss is also implemented, aimed to aviod that if, for instance, the domain classifier, correctly classify one input, the computed error would be low, and it would have a low contribution during the back propagation, and leaving the weights almost unmodified after the backpropagation, and therefore leaning towards that site.





\chapter{SHAP}\label{sec:shap}



\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{shap/shap_presentation}
\caption{}\label{fig:shap_waterfall_example}
\end{figure}


SHAP (SHapley Additive exPlanation) is a technique based on game theory wich provides a method for model explanability.
is a powerful tool to get rid of the \textquotedblleft black box \textquotedblright idea of a machine learning model and try to understand its deep mechanism and the reasons why a model gave a certain output, or a certain prediction related to an input sample like a vector of feature or an image for example.
As is shown in figure \ref{fig:shap_waterfall_example} just as a visual example, a common machine learning model acts like a black box that returns an output value after some non-linear unknown calculations on some input values; with an explanatory model, we are able to quantify the contribution of each feature of our input data and assess what and why are the most important features and how much they contributed to the final outcome.





The idea behind SHAP is to use these Shapley values to exaplain every single feature's contribution in our machine learning model, treating it as a cooperative game

To apply the concept of Shapley values to a machine learning model we just adapt some terms we used for game theory: the payout is the model's prediction and the players are the feature in our input data.
For a single feature, its Shapley value is defined as the average marginal contributison of the value of that feature across all possible coalitions.

%To get the idea of how we can explain a model, we start asking: how much is a prediction driven by the fact that a feature has a certain value A, instead of its baseline value $\bar A$? the baseline value $\bar A$ is the result of ... ?
Shap can be regarded as a local explainatory method, and before it, several algorithms belonging to this cathegory were implemented as an attempt to create an explainatory technique for machine learning models.
In general, local explainatory model's objective can be defined as the attempt to explain an output f(x) after a single instance x that in our case can be a vector of features.
Local methods differs from global methods, because the latter provide us a global explanation of the model, across all the instances, they are able to attribute an importance to each feature to determine which contributed the most to the output of the model.
One of the key point of SHAP is its flexibilityu, being both a local and global explainatory model.

Being a local model, SHAP share with other algorithms some implementation features: given a single input vector x, to simplify the workflow, they introduce a coalition vector x' = $\left\{ 0, 1 \right\}^F$ : where F is the total number of features: x' is a binary vector of the same length of x, where binary means made just by zeros and ones: zero means that in our analysis we are going to withheld the corrisponding variable from x in, and the one that we are including it.

To map the x' vector to the corresponding feature values vector a mapping function $h_x(x')$ = x is introduced to returns the actual features' value from the corresponding input vector x.
Our goal is to find a simplified model g(x') to work with, because is simpler to work with a binary vector x' than with the actual vector x.
We want that this simplified model, when applied on the coalition vector x', is able to approximate our output f(x): $ g(z') \approx f(h_x(z'))$ if z' $\approx$ x'.
The first important condition on this function g is that it has to be a linear function
\begin{equation}
\label{eq:shap_g}
g(z') = \phi_0 + \sum_{i = 0}^N \phi_i z_i'
\end{equation}
And several methods before SHAP were created that satisfy this condition such as LIME \cite{ribeiro-2016} or DeepLIFT \cite{shrikumar-2017} but they lacked additional properties that SHAP, proposed and implemented by Lundberg \cite{ludberg-} has:

\begin{enumerate}
\item Local accuracy:
\begin{equation}
g(x') =\phi_0 + \sum_i \phi_i x_i' = f(x)
\end{equation} when x = $h_x(x')$ and not just an approximation $f(x) \approx g(x')$. Here $\phi_0$ represents the coefficient corresponding to a vector x' with all entries equal to zeros.
\item Missingness:
\begin{equation}
x_i' = 0 \Longrightarrow \phi_i = 0
\end{equation}
\item Consistency: if we have two different functions g' and g, indicating eith $z' / i$ the setting where $z_i' = 0$, if
\begin{equation}
 g'(z') - g'(z' / i) \ge g(z') - g(z' / i) \Longrightarrow \phi_i(f', x) \ge \phi_i(f, x)
\end{equation}
\end{enumerate}
%The most important properties of this models is called local accuracy and states that g and f must give the same result when passing all the features: g has to match f when x = $h_x(x')$.
%We attribute a contribution $\phi_i$ to each simplified vector $z_i'$

In 1975 was demostrated that the only coefficients satisfying all of these properties are Shapley values, and methods which employ different coefficients usually violate local accuracy and/or consistency.
For these reasons SHAP employs Shapley values into pre-existing algorithms and enhances them providing an unified aproach to assess feature importance for different models without any violation of the axsioms above.



%To assign the importance value to a feature we need to train two different models: one with that feature included and the second one with that feature excluded and thereafter compute the difference between the two models' prediction.
%
%For each player i and a map function $\nu$ shap values are defined as:
%We are then going to use them to explain a deep learning model with the SHAP method.

To compute the Shapley value for a feature we have to evaluate the model on all the possible subsets S we can create with our data S$\subseteq F \setminus \left\{i\right\}$. That in a problem with an high number of features would result in a huge computational work. To bypass this problem, we can choose not to calculate the exact shapley value but just an approximation of it, depending on the model we are working on, SHAP implemented different algorithms such as Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees, KernelSHAP which is a model agnostic explanation method, or DeepSHAP which is optimized to work faster on deep models by making use of the knowlegde of the structure of a neural network.
Each one of these algorithms is built upon previous algorithms like LIME or DeepLIFT

LIME is a local, model-agnostic, interpretability model, made to explain the prediction of ant classifier in faithful, but only local way, meaning that it can accurately explain a single prediction but it is not able to generalize to many.
Since Kernel SHAP is built upon LIME algorithm, it pick up an important strategy from it, to avoid the computation of all the permutaiton
%An innovative aspect of LIME is the strategy it employs to resolve the problem of high computational work when it deals with a large number of feature in a single data: instead of computing all the possible permutations,
%a we deal with a large number of permutations: theoretically we should evaluate the model for each possible feature permutation so that for a model with p features we should repeat this process p! times which in our case would bring to 5995! $\sim 10^{20046}$.
KernelSHAP adopts an aproximation methods based on the introduction of a perturbed dataset: starting from a single instance x, it randomly creates a perturbed data by randomly setting to 0 or 1 the corresponding entries on several coalition vectors x', doing so it creates a perturbed dataset made of different x' vectors.

The next step is to make prediction for all of these perturbed vector.
Since x' and x are related through the mapping function h, for each x'we can retrieve the corresponding values from x and input this vector to our model
and then uses the pre-trained model to make predictions on this perturbed dataset.

As reported before, 1 means that we are including the corresponding feature in this analysis while 0 is associated to a lack of the corresponding feature.
Since not all the machine learning models can easily deal with lacking features, to simulate the lacking of a feature, when we have a 0 in the x' vector, the algorithm replaces the corresponding feature's value with the most uninformative value.
This is chosen using a set of data from training called \emph{background} datset, of arbitrary size, from it, features are averaged, and the this value is replaced in our vector to simulate the absence of that feature.

Using this synthetic dataset, the model is evaluated on each vector and these scores are collected, and these outputs are subsequently used to accomplish a minimization task to finally find the shapley coefficients.
Minimization is performed in a fit, but before it, each output value is weighted according to the distance of the synthetic vector with the original space vector by using coefficients $\pi_x$ defined as
\begin{equation}
\pi_x = \frac{p-1}{{p \choose |z'|} \cdot |z'| (p-|z'|)}
\end{equation}
Where we indicated as p the total number of features in the original feature vector x anc with |z'| the number of element in that subset, and cosequently (p-|z'|) is the number of features not included in the subset.

To perform a fit, the loss function to minimize is in the form $L(f, g, \pi_x) = \sum_{z\in Z} [f(h(z))-g(z')]^2\ \pi_x$
where g(z') is the simplified function expressed in equation \ref{eq:shap_g}, given by a combination of coefficients $\phi_i$ that are the parameters of the fit.


%it creates different perturbed copies of it, modifying its features evaluating the model on each one, then, it weigths each one according to its distance from the original instance and calculate a linear fit to explain the predictions.
%This idea is what is implemented in KernelSHAP:
%The idea behind algorithms like KernelSHAP comes from LIME algorithm \cite{ribeiro-2016},
%it avoids the computation of all the permutations by choosing a background subset from input data and evaluate the model only using this subset.
%So if we want to study the effect of a single feature, we ideally switch off that feature, and SHAP does so, by replacing its value by randomly picking it from the background dataset.
%By following this simplified procedure:

%So if we want to evaluate k data samples, (each one containing p features), and we choose a background dataset of size n, the algorithm will perform evaluations only n*k times following this aproach:
%To avoid this calculus Shapley employ by defining a background dataset with a subset of Train data we used to train the model, then during shapleyt computation it just fill in the missing features with features from this background.


In a nutshell, the main steps of KernelSHAP algorithm are
\begin{enumerate}
\item Given a feature vector $\mathbf{x} = [x_1, x2, ..., x_p]$, where $x_1, .., x_p$ are the features (numbers), a model f and a background set $X_{bckg} = \{ \mathbf{x1}, ... , \mathbf{xn}\}$, we want to explain \textbf{x}
\item replace a subset of entries of x with values from the background dataset:
\item create n different copies of the original input fector x, all slightly different from each other because of these replacements
\item Evaluate the model on each copy
\item Use these outputs to fit simple model and find shapley coefficients
\end{enumerate}



%LIME \cite{ribeiro-2016}
% shap \cite{lundberg-2017}
%deepLIFT \cite{shrikumar-2017}

As mentioned above, KernelSHAP is a model-agnostic algorithm, in the sense that can easily used with every kind of model, but SHAP includes some model-specifics algorithms that make use of a previous general knowledge of a model's structure to optimize the explainer's performances and speed up the process. DeepSHAP, is one of these model-specific algorithms and it is what we are going to use in our analysis.

DeepSHAP alghorithm works with them same principles of KernelSHAP, making use of a background dataset to simulate missing values, but it is also optimized to perform better on deep models: it is in this sense, an enhanced version of the DeepLIFT algorithm because of the introduction of the background dataset and because of the use of Shapley values rather than DeepLIFT values.

DeepLIFT aims to compute the difference between the output of our model with our data and a reference value computed as the output of our model with some reference data.
The choice of the reference data depends on what kind of data we are working on: images, or genomic data for example.
for genomic the most common way to produce reference output is to shuffle some inputs, evaluate the model on them and average across all the scores.
It is also shaped to perform on deep learning algorithm computing its DeepLIFT value during the backpropagation process.
DeepLIFT attributes to each feature $x_i$ a value $C_{\Delta x_i, \Delta y}$ that represent the effect of that input being set to a reference value rather than its actual value.
This reference value is chosen from the backgound dataset and represents an uninformative value for that feature

In a nutshell, let t represent the output of some inner neuron and let x1, x2, ... , $x_n$ be some preceding neurons necessary to compute t, if we label $t_0$ the reference output, we can compute the value $\Delta t = t-t_0$ and use them to define the DeepLIFT values $C_{\Delta x_i \Delta t}$ so that
$\sum_{i = 1} ^n C_{\Delta x_i \Delta t} = \Delta t$. For a given input neuron x with difference from reference $\Delta x$ and a target neuron t, is defined a multiplier $m_{\Delta x \Delta t} = \frac{C_{\Delta x \Delta t}}{\Delta x}$ which represents the contribution of $\Delta x$ to $\Delta t$.

For a more detailed explanation of how DeepLIFT works we refer to its presentation article \cite{shrikumar-2017}

%showed that the per node attribution rules in DeepLIFT (Shrikumar, Greenside, and Kundaje, arXiv 2017) can be chosen to approximate Shapley values. By integrating over many backgound samples Deep estimates approximate SHAP values such that they sum up to the difference between the expected model output on the passed background samples and the current model output (f(x) - E[f(x)]).

DeepSHAP combines shap values for smaller components of a network to compute values for the whole network, it does so recursively passing Deep LIFT multipliers during backpropagation: we can indeed re-writing multiplier values in terms of shap values, for a target neuron f3 we have $m_{x_j f_3} = \frac{\phi_i(f_3, x)}{x_j - E[x_j]}$ obtaining $\phi_i(f_3, y) \approx m_{y_i, f_3}(y_i - E[y_i])$


To express the significance we can rely on the relation between feature importance in a qualitative way and shap value: feature with high absolute shapley value are important.
The absolute importance value for a feature f is calculated as the mean of the magnitude of all the shapley value for that feature, so the mean is performed across all the samples we used to calculate these values
\begin{equation}\label{eq:shap_magnitude}
I_f = \frac{1}{n} \sum_{i = 1}^n \|\phi_i^{(f)}\|
\end{equation}

Shap is implemented as a free Python package with MIT license, developed and mantained by Scott Lundberg \footnote{https://github.com/slundberg/shap}.
This package includes different classes such as TreeExplainer tuned to perform rapidly on tree and forest-like models, linearExplainer which deals with linear model and is able to compute the exact shapley values and not just an aproximation, and the algorithm we are using DeepExplainer, suitable to explain deep learning models and to compute an approximate value of shapley values.


\newpage

\addcontentsline{toc}{chapter}{IMPLEMENTATION AND RESULTS}
%\chapter*{IMPLEMENTATION AND RESULTS}



\chapter{Analysis workflow}\label{sec:analysis_workflow}

We mainly carried out three different kind of analysis using the DNN explained before, and compared them with the scores obtained from the adversarial network.

For each one of these analysis, we implemented a K-fold cross validation scheme to report our results. We determined and reported model performances as the mean AUC value and the standard deviation of the scores for each k-fold. A schematic


\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{ml/flowcharts/flowchart_analysis}
\captionsetup{singlelinecheck = off}
\caption[]{
Flowchart of the analysis carried out in this work:
\begin{enumerate}
\item Select one type of coefficient: Pearson correlation, Wavelet coherence coefficients
\item Select the desired attributes of patients in order to make cuts on dataset, for example use the whole ABIDE I + II dataset or just ABIDE I, limit analysis to a certain age range, sex, or eye status open or close.
\item Use these data + labels to make classification following the three pipelines described in section \ref{sec:deep_learning_results} and the fourth analysis pipeline using the adversarial neural network
\item For each of these branches, implement a K-Fold cross validation and return the results in terms of mean AUC and std deviation of the values across every fold.
\end{enumerate}
}
\label{fig:workflowanalysis}
\end{figure}


Since we are not working with a huge amount of data $(< 1600)$ samples, we choose a 5-fold CV, we made this choice because with a 10 k-fold CV, the error on the mean AUC score was high, so we choose to hold a greater number of data in the Test to reduce variability among them and hence reduce the standard deviation of the model's score.
K-fold CV was implemented using the stratifiedKFold class provided by sklearn library for Python

The data we are using to perform classification are the Pearson correlation (Fisher-transformed) data, or the correlation values coming from the analysis with wavelets.



In detailes the four types of analysis we carried out are:
\begin{enumerate}
\item Classification of asd/Controls using raw (not harmonized) data
\item Classification using the whole harmonized dataset, creating the harmonization model on all the control subjects and applying the model on all ASD subjects, and then split this dataset into a train and test subsets for each k-fold. The flowchart to schematise this implementation is showh in figure \ref{fig:harmon_upstream_flowchart}.
\item Classification with harmonization implemented inside the K-fold cross-validation, creating the model on controls and applying it to ASD: following this procedure data were splitted into a train and a test dataset, therefore using train data we computed the harmonization model only on control patients and then apply the model to asd patient and both Control and ASD Test data.
The whole flowchart is shown in figure \ref{fig:harmon_kfold_flowchart}
\item Classification using the domain adversarial neural network.
\end{enumerate}

Harmonizing data inside a k-fold is the best way to mantain Train and test data as much separated as possible and avoid data leakage between train and test.
It is a common source of error to introduce some kind of data leakage between the Train and the Test dataset, when we harmonize on the entire dataset, the harmonization model permorms a fit using all the data, so each data is modified according to information obtained from all the other data, so in data belonging to the train dataset there's already information about the Test dataset, and this leads to a misleading increase of model's performances.
Some articles perform a Leave One Site Out Cross validation. In our case we can't do that if we want to mantain Train and Test independent because if we compute the harmonization model only on the train dataset, we wouldn't have information to apply it to the test dataset since its data belong to a site not included in the model.



\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{ml/flowcharts/harmon_kfold}
\caption{Flowchart: implementation of harmonization inside each fold}
\label{fig:harmon_kfold_flowchart}
\end{figure}



\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{ml/flowcharts/harmon_upstream}
\caption{Flowchart: implementation of harmonization upstream}
\label{fig:harmon_upstream_flowchart}
\end{figure}



Analysis were carried out using different selection criterias and thresholds on the dataset which brought us to different datasets combination.
We excluded from out analysis data coming from sites NYU\_2 and ... because as noticed in section \ref{sec:} they didn't provide control data but only ASD patients, and without them, we can't perform harmonization on this sites because we would need to create the model only on Control data.
We left with a total number of 1501 perason correlation data and
The first classification attempt was using the whole dataset from ABIDE 1 and 2, consisting of only male patients aged between 5 and 40 years old, without any further constraint on eye status at scan.
Then, with these same constraint we tried to classify separately ABIDE 1 and ABIDE 2 datasets.
The second analysis was performed on a dataset with the same constraint as above, but with an additional condition on eye status choosing to select only patient who kept eyes open throughout the entire scan time.
The latter analysis were carried out with these same constraints, but separately on ABIDE 1 and ABIDE 2 dataset.
So in whort we have 6 different analysis shown in table \ref{tab:controlasd_per_subset}




\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrr}\toprule
Dataset &Tot &Controls &ASD \\\midrule
AB I+II eye = all, sex = males, age(5, 50) &1470 &737 &733 \\
AB I + II eye = open &1026 &514 &512 \\
AB I &841 &426 &415 \\
AB I eye=open &568 &281 &287 \\
AB II &629 &311 &318 \\
AB II eye = open &458 &233 &225 \\
\bottomrule
\end{tabular}
\caption{Total number of data and control/ASD amount for each subset and thresholds}
\label{tab:controlasd_per_subset}
\end{table}


A further analysys concerns the feature's dimensionality reduction, that is the reduction of features from the input dataset using PCA
For each one of the analysis above, PCA was implemented to restrain the effect of overfitting.

From the initial input data consisting of 5995 features, we tried to classify using n principal components
The workflow of the entire analysis can be schematized in the workflow diagram \ref{fig:workflowanalysis}



\chapter{Harmonization - results}

% data do not present a clear age-trend  (and FIQ?)

As described in section \ref{sec:harmonizationtheory}, ComBat-based harmonization procedure simultaneously models and estimates biological and non biological terms, from data and algebraically removes the estimated addivite and multiplicative site-effect terms.

Harmonization procedure was tested on the whole ABIDE I + II dataset to have a visual representation of how features are modified after being processed with harmonization pipeline.

To test harmonization procedure and evaluate its performances in making sites' feature uniform, we used NeuroHarmonize \footnote{https://github.com/rpomponio/neuroHarmonize} package for Pyhton, developed by Pomponio et al. \cite{pomponio-2019} which implemented and added some features to the previous NeuroComBat Python package \footnote{https://github.com/Jfortin1/ComBatHarmonization}.

To use Neuroharmonize package we need to input the feature data to harmonize and the corresponding covariate information with site, age, sex, and other biological quantities we desire to take into account.
For our data these information were provided by a csv file on ABIDE website so that for each patient are collected as much information as they could.
A regression model is thus created with parameters estimated with the procedure explained in section \ref{sec:harmonizationtheory} and it can be applied to this input data or to other new data provided that they belong to the same sites we used to create our model.

We tested harmonization procedure on 1494 male patients coming from ABIDE I and ABIDE II of which 722 controls and 772 cases, choosing as covariate to preserve the Age, and the FIQ (Full intellective quotient) to mantain important biolgical trends in the data and avoid overcorrection.

%C'è davvero un biological trend?? mettere figure.

The central idea is to create the model on control subjects only, to create a model without the influence of informations related to ASD patients whose feature might follow a different distribution than control patients.
Then once created the model on control subject, it is applied to both control and ASD subjects, so that site-related information are eliminated from our data.

Under this general way to proceed, we used a Random Forest Regression to test the site classification performances before and after the harmonization procedure.
In this analysis, we split the model into a Train and Test subsets, using the Test subjects, we created a model with only controls as explained before,
The model was then applied to the case subjects belonging to train, and to both case and controls subjects of the Test set.

A random forest classifier was trained on this dataset attempting to make binary classification of the site, for each pair of sites.

This aproach, allows train and test dataset to be still independent, because harmonization model was only created on Train subjects and once learnt the parameters, is applied to the Test dataset, so the data we use to train the random forest, are kept separated from the test set, so that there's no information leakage of the test data into the training data.
\newline

\textbf{Note:} As can be seen from figure \ref{fig:controlcase_site} two sites namely \texttt{NYU\_2} and \texttt{KUL\_3} provided only ASD patients, and, since we are estimating the mean and the variance for each feature across all sites, when we train a model only on control cases, and we apply it on patients belonging to new sites, we don't have information to scale and modify those data so the model would output NaN values.
In our analysis we excluded these sites since we can't perform a proper harmonization of them, however, just in the following figures to have a visual feedback of how harmonization works, we substituted NaN values with original unmodified data.
\newline

Figure \ref{fig:heatmap_harmonization} shows the result of classification on the dataset described above. Scores are reported in terms of AUC for the classification of each pair of sites, and sites along the x and y axes are ordered by incresging average age of patients coming from that site.
Specifically, fig \ref{fig:heatmap_harmonization_noharmon} shows the AUC score of the model trained with the raw, not harmonized dataset, it is clear from this binary classification that almost every site is well distinguishable from the others by the value of AUC, almost uniformly near to 1.
On the contrary, in fig \ref{fig:heatmap_harmonization_harmon} values are lower, meaning that sites are not so distinguishable anymore, however, a certain distinctive features are still present for sites presenting high difference in mean age.
This is a consequence of choosing age as covariate, which preserves differences in age-dependent features.


%C'è da dire che anche dopo l'armonizzazione, il sito SU_2 rimane distinguibile, anche se dalle feature sembra essere stato allineato con gli altri. Quindi stessa storia potrebbe essere avvenuta per i wavelet.

% HEATMAPS
\begin{figure}
\centering
\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/pearson/matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with raw data}
   \label{fig:heatmap_harmonization_harmon}
\end{subfigure}

\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/pearson/harmon_matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with harmonized data}
   \label{fig:heatmap_harmonization_noharmon}
\end{subfigure}

\caption{Comparison of two heatmaps with AUC score of binary classification site vs site with \textbf{Pearson}-based correlation coefficients of raw (fig \ref{fig:heatmap_harmonization_harmon}) and harmonized data (fig \ref{fig:heatmap_harmonization_noharmon}) classified using a Random Forest Classifier
}
\label{fig:heatmap_harmonization}
\end{figure}



\begin{figure}
\centering
\begin{subfigure}[b]{0.70\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/wavelets/matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with raw data}
   \label{fig:heatmap_harmonization_harmon_w}
\end{subfigure}

\begin{subfigure}[b]{0.70\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/wavelets/harmon_matrix}
   \caption{Heatmap with AUC score of a binary classification site vs site with harmonized data}
   \label{fig:heatmap_harmonization_noharmon_w}
\end{subfigure}

\caption{Comparison of two heatmaps with AUC score of binary classification site vs site of \textbf{Wavelet}-based correlation coefficients of raw (fig \ref{fig:heatmap_harmonization_harmon_w}) and harmonized data (fig \ref{fig:heatmap_harmonization_noharmon_w}) classified using a Random Forest Classifier
}
\label{fig:heatmap_harmonization_w}
\end{figure}





To have a visual representation of how much each feature is modified in respect of its sourced site, in fig \ref{fig:features_raw-harmo} are shown two features among 5995: \emph{feature 324} and \emph{feature 2800} as a function of sites.
For a clearer idea of what is shown, feature 324 represents the correlation between right and left inferior frontal gyrus, and feature 2800 that between left precuneous cortex and the left inferior frontal gyrus
Their values are plotted as a box along the y axis and sites are indicated along the x-axis, it is clear how harmonization is a powerful tool to level up feature and remove site-dependent information from them.

\begin{figure}
\centering
\begin{subfigure}[b]{1.\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/pearson/Feature324raw-harmo}
   \caption{Feature 324 }
   \label{fig:feature324}
\end{subfigure}
\begin{subfigure}[b]{1.\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/pearson/Feature2800raw-harmo}
   \caption{}
   \label{fig:feature2800}
\end{subfigure}
\caption{Pearson-based connectivity coefficients per site for features 324 (fig \ref{fig:feature324}) and 2800 (fig \ref{fig:feature2800}) before and after harmonization}
\label{fig:features_raw-harmo}
\end{figure}



 \begin{figure}
 \centering
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/wavelets/Feature324raw-harmo}
    \caption{}
    \label{fig:wavelet_feature324}
 \end{subfigure}
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmonization/wavelets/Feature2800raw-harmo}
    \caption{}
    \label{fig:wavelet_feature2800}
 \end{subfigure}
 \caption{Wavelet-based connectivity coefficients of in-phase percentage, for features 324 (fig \ref{fig:wavelet_feature324}) and 2800 (fig \ref{fig:wavelet_feature2800}) before and after harmonization}
 \label{fig:wavelet_features_raw-harmo}
 \end{figure}



 If we split this plot into control and ASD patients to see the effect of harmonization separately on these subsets, \ref{fig:features_control-asd_raw-harmo}

 \begin{figure}
 \centering
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{harmonization/pearson/Feature324control-asd_raw-harmo}
    \caption{Feature 324 of Pearson correlation coefficients}
    \label{fig:feature324-control-asd}
 \end{subfigure}
 \begin{subfigure}[b]{1.\textwidth}
    \includegraphics[width=1\linewidth]{harmonization/wavelets/Feature324control-asd_raw-harmo}
    \caption{Feature 324 of Wavelet-based correlation coefficients}
    \label{fig:feature324-control-asd_w}
 \end{subfigure}
 \caption{Pearson-correlation and Wavelet coefficients for features 324 before and after harmonization with a separated plot for controls and ASD}
 \label{fig:features_control-asd_raw-harmo}
 \end{figure}




\section{Comments on harmonization}

Figure \ref{fig:heatmap_harmonization_w} shows a binary classification procedure with a random forest, for site discrimination of Wavelet-based coefficients. It appears clear that with Wavelet coefficients there is not the same outcome as with Pearson coefficients (figure \ref{fig:heatmap_harmonization}), in fact with Wavelets sites still remain distinguishable after harmonization.

%This may be due to the loss of some information linked to the site througout all the process of extracting a correlation coefficient starting from a time-frequency analysis of two timeseries.
%Accordind to this hypotesis, if we lost site information, the particular trend of a feature we see for example in figure \ref{fig:wavelet_features_raw} is just a casual trend and taking into account all of the features, there is not a specific bias connected to a site.
%This way, during procedure of harmonization, we are creating a bias towards each site and for this reason after harmonization, sites remain distinguishable, or actually, they become even more recognisable
Nevertheless, if we plot the mean value of a feature computed with Wavelet-based correlation across sites (figure \ref{wavelet_features_raw}) we note that there is a certain trend related to sites, but it is possible either that we obtain this trend just for this choice of feature or that even if the mean values of these features underwent a level out procedure, they still remain recognisable because of the values they assume which is not in the same range for all the sites and this can help the classificator recognising sites differences.

\hfill

Both for Wavelet-based coefficients in figure \ref{fig:wavelet_features_raw} and for Pearson-based coefficients in figure \ref{fig:features_raw-harmo} we can notice a certain messy trend of features of raw data in respect to site (plots on the left side), which is not due to the presence of an unbalanced Controls/ASD number of patients per site, because from figure \ref{fig:features_control-asd_raw-harmo} we observe as well, a messy trend both for Controls and ASDs.
We can therefore conclude that in our data there's some kind of bias linked to the acquisition site.
After harmonization procedure both Pearson-based coefficients and Wavelet-based coefficients assume a more regular trend (plots on the right side of figures  \ref{fig:wavelet_features_raw} and \ref{fig:features_raw-harmo})
This is a visual confirm that harmonization effectively removes site-related, but if for Pearson coefficients this is enough to remove inter-site variability, for Wavelet coefficients this difference remain the same

%This bias is more evident with Pearson-based correlation coefficients (fig \ref{fig:wavelet_features_raw}) than with Wavelet-based coefficients (fig \ref{fig:features_raw-harmo}) that appear to assume a more uniform trend.

%






\newpage
\chapter{Deep learning: implementation and results} \label{sec:deep_learning_results}
\section{Model's structure}
In this work we performed a classification task, using a shallow neural network built using keras library for Python.
Since we are working in an unavoidable overfitting condition, we tried to build a network with as less feature as possible, but still preserving good classification performances.
We then started the research of our newtork with a network comprising only 3 layers and we searched the minimum number of neurons to employ in each layer.
We started with a trivial network made by just 3 neuron in the first layer, 2 nodes in the second and at last a single-output layer. Even if this was just an attempt to put a lower limit to the number of neurons, we noted that even with this configuration, the network tends to overfit our data.
This is clear if we take a look at the learning curves in figure \ref{fig:learningcurve} which shows the training and validation AUC curves for two models: the first one of them (\ref{fig:auc_no3-2-1}) even if with the simplified structure just mentioned, shows the classical trend of an overfitting state, however, it performes a bit worse than the other two more complex models.
We continued adding neurons to each layer and tried different configurations. With a configuration of 8-8-1 performances were slightly higher, so we set the second layer to 8 neurons and tried changing the number of the neurons in the first layer, performances seemed to increase as the number of neurons increased untill they reached a stable value. Different attempts and the relasted classification scores are reported in table \ref{tab:appendice..} in appendix.
We decided to pick the configuration which gave the minimum number of neurons, beyond which performances were stable, and the addition of more neurons to create a more complex network was unjustified.

\begin{figure}
\centering
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no64-8-1}
   \caption{AUC learning curve corresponding to a model with a structure 64-8-1 nodes trained on the entire not-harmonized dataset}
   \label{fig:auc_no64-8-1}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no8-8-1}
   \caption{AUC learning curve corresponding to a model with a structure 8-8-1 nodes trained on the entire not-harmonized dataset}
   \label{fig:auc_no8-8-1}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no3-2-1}
   \caption{AUC learning curve corresponding to a model with a structure 3-2-1 nodes trained on the entire not-harmonized dataset}
   \label{fig:auc_no3-2-1}
\end{subfigure}
\caption{}
\label{fig:learningcurve}
\end{figure}


In our work we employed a the neural network schematized in figure \ref{fig:model_structure}.
This network consisted of 2 hidden layers made up of 264, and 8 neurons respectively, both activated by a ReLU function and separated by a Batch-Normalization and a Dropout layer with a dropout chance of 30\%.
After the 8-node layer we added a second Batch-Normalization layer, with a second dropout layer once again wich 30\% dropout.
After them we put a single output layer with a single neuron for the classification output.
This last layer is activated by a sigmoid function, which outputs a real number between 0 and 1.



\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{ml/models/model_structure}
\caption{Schematic structure of the deep neural network employed in this work for Control/ASD classification, with a scheme 264-8-1.
Each box contains the layer's name, layer's task (Input, Dense, BatchNormalization etc..), activation function, input shape (number of neurons) and output shape.}
\label{fig:model_structure}
\end{figure}


Since we are still working in an underlying overfitting condition we set the subsequent hyperparameters after a grid search on learning rate and number of epochs, setting a validation set of 25\% of Train data, and studying the evolution of the learning curves, of which some exaples are illustrated in figure \ref{fig:learningcurves} to find the minimal parameters' configuration which allows high classification scores.

In our model, as loss function to minimize we used binary crossentropy and the neural network's weights were optimized via Adam optimizer with a learning rate of $10^{-4}$.
Adam optimizer was chosen over Stochastic Gradient Descent optimizer since Adam reached similar performances with a lower error in respect of SGD.
Model's classification performances were assessed with a 5 k-fold Cross validation, collecting the AUC score for each fold and then computing mean and std deviation of these results.

%Adversarial model:

The other neural network employed was the site-adversarial neural network: a model able to earn from data both class and site information, and avoid some sort of bias due to site, when performing class classification.
We tried two different aproaches to this task, but later we focused only on one of them.
The first attempt to implement a model able to predict category label without any influence of site's information is a model with two different branches, one for the output of Control/ASD prediction, and one for the output of the predicted site.
With these two informations, a loss is created by combining two different loss functions, one for the class and the other for site: the idea is similrar to that proposed in the article \ref{Guan-2021}.
This new loss is in the form
\begin{equation}
L = L_1 - \lambda L_2
\end{equation}
Where $L_1$  is in our case the loss for the binary classification Control/ASD that we want to minimize (binary crossentropy), while $L_2$ is the loss for site classification (categorical crossentropy) we aim to maximize in order to avoid to learn any information somehow linked to site, and $\lambda $ is a parameter to set empirically, to control the contribution of the site's loss to the overall loss.
This way, the minimum of the total loss is reached with parameters that minimize the category classification loss and maximize the site classification loss.
Performances of this kind of model are strictly linked to the value of the parameter $\lambda$, in a sense that if a certain value gave an optimal performance on a certain dataset, on an other dataset, for example one created with some more constraints on patients, the value for which one had the best scores needed to be changed.
And also, since this kind of model gave slightly worse performances than simple deep model, we looked for an other strategy and carried out the analysis with this second type of model.

We tried to build something with the same underlying idea, namely minimize the error on classification of asd/controls, while maximizing the loss related to the site classification.
Our task is in fact to avoid the model to learn some site-related pattern from data and we accomplished so with a model whose inner weights are updated moving towards a minimization of the gradient for classification, and in the opposite direction for site classification, as explained in section \ref{sec:domain_adversarial_theory}.

%a similar structure as the DNN,
Following a similar structure of the model before mentioned, with a composition of the two losses, the structure of this model can be divided into three main components, two of which are exactly the same of the deep neural model explained at the beginning of this section.
The three components that make up this model are: a first feature extraction branch comprising th input layer, the 264-nodes and the 8-node layer, between which we put a Batch-normalization and a 0.3 dropout layer, as we did in the first DNN.
From this point on the network is splitted into two branches: one categorical classificator, similarily to the DNN consists of a last layer with a sigmoiod activation function for Control/ASD classification,
and the other branch, the third component of this network, the site classifier consisting of a gradient reversal layer as the first layer followed by a layer with 16 nodes, and after a batch Batch-Normalization layer, the last layer: a multi-output layer with N nodes, being N the total number of sites input data belong to.
The structure of this network is illustrated in figure \ref{fig:adv_model_structure}


\begin{figure}[h!]
\centering
\includegraphics[width=0.6\linewidth]{ml/models/adv_model_structure}
\caption{Structure of the adversarial neural network with two outputs: a single-neuron output for binary classification controls/ASD and a multi-class classification for site classification.
Each box contains the layer's name, layer's task (Input, Dense, BatchNormalization etc..), activation function, input shape (number of neurons) and output shape.
}
\label{fig:adv_model_structure}
\end{figure}


The final layer of the site-classification branch is activated by a softmax function, and the loss function employed is the categorical crossentropy implemented using a custom function to deal with a possible low crossentropy score when by chance the site classifier, categorize one or more input data site label correctly, which would have low impact on the whole branch classifier making the adversarial strategy less effective.

Categorical crossentropy is the loss which allows to deal with multi label output as explained in \ref{sec:loss_functions}.
\begin{notes}
\item To work with this loss we need to encode each site numerically before before use its label into our model because functions like these can't deal with string variables such as site's names, so we have to associate each one of them with a number: site\_a = 0, site\_b = 1 and so on, but if we stop to this point and input integer numbers $\{0, ..., s\}$ where s is the total number of sites, to our model, it ends up considering sites with higher number as \textquotedblleft larger \textquotedblright than that and produces not-optimal results.
To avoid this a common strategy is to transform these variables according to a technique called One-hot-encoding.
This common aproach converts each value to a vector of len s, and containing all zeros but in the entry corresponding to that site, where it puts 1, so thast each site label becomes a binary vector, with just a single 1 in position corresponding to that specific site.
\end{notes}

We tested both these different adversarial models (the one with loss combination and the one with gradient reversal) and we noticed that they had more or less similar performances, but we choose to carry out the next analysis with this second model, the gradient reversal one, because, even if, like the model with loss compositions, in the gradient reversal layer, a parameter $\lambda$ controls the weights updating during the backpropagation,
results obtained with this model are more stable in respect to the the value of this parameter.
So changing the value of this parameter does not affect significantly the performances of this model.

\newpage


\newpage
\section{Results with Pearson coefficients}

With Pearson correlation coefficients normalized with Fisher transform we obtained the following AUC scores, listed in table \ref{tab:pearson_results} with our DNN models, and we compared some of these results with a Random Forest classifies, to asses whether is convenient to use a deep model instead of a common machine learning algorithm such as Random Forest.


%If the table is too wide, replace \begin{table}[!htp]...\end{table} with
%\begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]...\end{threeparttable}\end{adjustwidth}
\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lcccc}\toprule
Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adv 264-8 \\\midrule
AB I+II &71$\pm$1 &74$\pm$2 &73$\pm$3 &69$\pm$2 \\
AB I &71$\pm$3 &74$\pm$2 &72$\pm$3 &71$\pm$4 \\
AB 2 &63 $\pm$5 &68$\pm$ 6 &64$\pm$ 4 &66 $\pm$ 4 \\
AB I + II - EYE open &72$\pm$3 &75$\pm$4 &72$\pm$3 &71$\pm$3 \\
AB I open EYE &73$\pm$ 1 &76$\pm$2 &72$\pm$1 &72$\pm$4 \\
AB II EYE open &66$\pm$3 &70$\pm$6 &69$\pm$6 &68$\pm$6 \\
\bottomrule
\end{tabular}
\caption{AUC score obtained from three different kind of analysis with Pearson coefficients}
\label{tab:classification_pearson}
\end{table}


Analysis with Random Forest were performed on the whole dataset: ABIDE I+II, and on the dataset which gave best classification performances: ABIDE I with open eye, following the three main pipelines of: Hamonization inside K-fold, Harmonization upstream, and Raw data (non-harmonized).
Comparison results between a DNN model and RF results are listed on table \ref{tab:comparison_dnn_rf_pearson}


\begin{table}[h]
    \begin{subtable}[h]{0.45\textwidth}
        \centering
        \begin{tabular}{|lcc|}\toprule
        \multicolumn{3}{|c|}{AB I + II} \\\midrule
        &DNN &RF \\
        K-fold &71$\pm$1 &66+2 \\
        Upstream &74$\pm$2 &72+1 \\
        Non-harmon &73$\pm$3 &65+1 \\
        \bottomrule
        \end{tabular}
    \end{subtable}
    %\hfill
   \begin{subtable}[h]{0.45\textwidth}
       \centering
       \begin{tabular}{|lcc|}\toprule
       \multicolumn{3}{|c|}{AB I eye = open} \\\midrule
       &DNN &RF \\
       K-fold &73$\pm$ 1 &70+3 \\
       Upstream &76$\pm$2 &71+2 \\
       Non-harmon &72$\pm$1 &68+3 \\
       \bottomrule
       \end{tabular}
    \end{subtable}
    \caption{Classification scores comparison between the DNN model and a Random Forest classifier, on Pearcon correlation coefficients, for dataset: ABIDE I + II and ABIDE I with open eyes}
    \label{tab:comparison_dnn_rf_pearson}
\end{table}


\newpage

\section{Results with Wavelet coefficients}

Following the same path as with Pearson coefficients, we tried to run classification on Wavelet coefficients

Four main analysis were carried out

\begin{enumerate}
\item In phase wavelet coefficients, reported in table \ref{tab:classification_win}
\item In phase and counter phase coefficient stacked together in a single array long twice In-phase coefficients only
\item Coefficients resulting from the subtraction of the two: In phase - Counter phase: denoted as W\_in - w\_out, reported in appendix, in table \ref{tab:classification_win-wout}
\item Counter phase wavelet coefficients, reported in appendix in table \ref{tab:classification_wout}
\end{enumerate}


\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrr}\toprule
Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adv 264-8 \\\midrule
AB I$\pm$II &65$\pm$ 2 &74$\pm$ 2 &66$\pm$ 2 &67 $\pm$2 \\
AB I &67$\pm$ 3 &73 $\pm$2 &68$\pm$ 3 &68$\pm$ 2 \\
AB 2 &62 $\pm$4 &68$\pm$ 4 &63$\pm$ 4 &62$\pm$ 4 \\
AB I $\pm$ II - EYE open &65 $\pm$4 &71 $\pm$3 &66 $\pm$3 &66$\pm$ 3 \\
AB I open EYE &69$\pm$ 4 &74$\pm$ 6 &67$\pm$ 3 &67 $\pm$4 \\
AB II EYE open &66 $\pm$2 &69 $\pm$3 &67$\pm$ 2 &68 $\pm$2 \\
\bottomrule
\end{tabular}
\caption{Classification scores: AUC of joined W\_in and W\_out coefficients}\label{tab:classification_win+wout}
\end{table}

These results were compared with a Random Forest classifier obtaining the following scores: (table \ref{tab:comparison_dnn_rf_win+wout})

\begin{table}[h]
    \begin{subtable}[h]{0.45\textwidth}
        \centering
        \begin{tabular}{|lcc|}\toprule
        \multicolumn{3}{|c|}{ABIDE I + II} \\\midrule
        &DNN &RF \\
        K-fold &65$\pm$ 2 &60+2 \\
        Upstream &74$\pm$ 2 &74+3 \\
        Non-harmon &66$\pm$ 2 &59+3 \\
        \bottomrule
        \end{tabular}
    \end{subtable}
    %\hfill
   \begin{subtable}[h]{0.45\textwidth}
       \centering
       \begin{tabular}{|lcc|}\toprule
       \multicolumn{3}{|c|}{AB I eye = open} \\\midrule
       &DNN &RF \\
       K-fold &69$\pm$ 4 &62+5 \\
       Upstream &74$\pm$ 6 &69+3 \\
       Non-harmon &67$\pm$ 3 &61+5 \\
       \bottomrule
       \end{tabular}
    \end{subtable}
    \caption{Classification scores comparison between the DNN model and a Random Forest classifier, on Wavelet coherence coefficients W\_in and W\_out, for dataset: ABIDE I + II and ABIDE I with open eyes}
    \label{tab:comparison_dnn_rf_win+wout}
\end{table}
\newpage




\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrr}\toprule
Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adv 264-8 \\\midrule
AB I$\pm$II &65$\pm$ 2 &71$\pm$ 2 &62$\pm$ 2 &64$\pm$ 1 \\
AB I &66$\pm$ 3 &73 $\pm$3 &65$\pm$ 3 &66$\pm$ 2 \\
AB 2 &62 $\pm$3 &65 $\pm$3 &61 $\pm$4 &60 $\pm$2 \\
AB I $\pm$ II - EYE open &64$\pm$ 4 &70 $\pm$3 &66$\pm$ 3 &65 $\pm$3 \\
AB I open EYE &64 $\pm$5 &68 $\pm$5 &64 $\pm$3 &64 $\pm$4 \\
AB II EYE open &66 $\pm$2 &68$\pm$ 3 &66$\pm$ 4 &65 $\pm$5 \\
\bottomrule
\end{tabular}
\caption{Classification scores: AUC, only w\_in coefficients}\label{tab:classification_win}

\end{table}


\newpage

\section{Results with PCA}
% come ho calcolato le pca? solo sul training set
%non avrebbe senso calcolare la PCA solo sui controlli perchè pca cerca la direzione di massima varianza, dove i davi variano maggiornente, e quidni dove si ha una migliore distinguibilità bewteen controls and asds. NOn avrebbe senso farla solo sui controlli.
%Il numero di PC può essere più grande del test dataset, ma non sarà mai più grande del numero di feature in esso (per come è definito il massimo numero di pc), quindi una volta estratte le PC può essere applicato a qualsiasi test dataset

When applying PCA to a dataset we seek to explain as much variance as possible, for this reason we start our pca analysis extracting a number of principal components that explain more then 90$\%$ of variance, then we gradually reduced this number halving if.
The maximum number of principal components is limited by the number of data we can use. If we have a dataset of $n\_samples$ samples, each with $n\_features$ features, the maximum number of PC is given by $N\_pc = min\{n\_samples, n\_features\}$.
This is because in a $n-feature$-dimensional space, our points lie in a $n\_samples$-dimensional hyperplane and all the variance can be explained inside this subspace.
For a better understanding of this concept, we can imagine a dataset made of only two data, each containing 3 features. If we represent these data, we construct a 3D space where each orthogonal axes corresponds to a feature, and we insert these two points, each corresponding to a data point.
If we want to explain the maximum variance we need to find a new axis, the first principal component, but through two points passes just one straight line so we can at most limit our analysis to the plane passing through that line.

In our case $n\_samples$ is always $< \ n\_features$, so the number of principal components will be limited by the number of data in the Train dataset.
We use just the train dataset to calculate principal components because this is the correct way to proceed to avoid data leakage between Train and Test dataset.

We choose to run PCA analysis using coefficients that gave the best classification results in the previous analysis namely Pearson correlation coefficients. And to not weigth down this table, we limited our dataset choice to the whole ABIDE I + II dataset and to ABIDE I only open eye, because on it we obtained a bit higher classification results.

We started our analysis choosing the proper number of PC to explain $> 90\%$ of variance and then gradually reduced them as reported in table \ref{tab:pca_explained}

\begin{table}
\centering
\begin{tabular}{ |l|c c| }
\hline
\multicolumn{3}{|c|}{Explained variance} \\
 \hline
  N PC & All dataset [\%] & AB-I eye=open [\%] \\
  \hline
 800 & 93 & -- \\
 400 & 78 & 98 \\
 200 & 62 & 77 \\
 100 &48 & 58 \\
 50 & 36 & 41 \\
 20 & 24 & 26\\
 \hline
\end{tabular}
\caption{Explained variance [\%] for different numbers of principal components, and for the two dataset used in the analysis}
\label{tab:pca_explained}
\end{table}



Results obtained from this analysis are reported in table \ref{tab:classification_pearson_pca} and shown in figure \ref{fig:classification_pearson_pca}

\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrrr}\toprule
PC &Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adversarial \\\midrule
800 &AB I+II &69$\pm$2 &73$\pm$3 &71$\pm$1 &71$\pm$2 \\
\hline
\multirow{2}{*}{400} &AB I+II &67$\pm$3 &73$\pm$5 &72$\pm$3 &73$\pm$3 \\
&AB I EYE open &71$\pm$3 &72$\pm$3 &71$\pm$3 &72$\pm$4 \\
\hline
\multirow{2}{*}{200} &AB I+II &68$\pm$3 &74$\pm$2 &72$\pm$2 &71$\pm$2 \\
&AB I EYE open &69$\pm$3 &73$\pm$2 &72$\pm$4 &73$\pm$3 \\
\hline
\multirow{2}{*}{100} &AB I+II &67 $\pm$2 &72$\pm$2 &69$\pm$1 &70$\pm$2 \\
&AB I - EYE open &70$\pm$2 &73$\pm$2 &71$\pm$4 &72$\pm$2 \\
\hline
\multirow{2}{*}{50} &AB I+II &66$\pm$4 &70$\pm$6 &68$\pm$3 &69$\pm$2 \\
&AB I - EYE open &70$\pm$4 &72$\pm$3 &69$\pm$3 &72$\pm$3 \\
\hline
\multirow{2}{*}{20} &AB I+II &65$\pm$5 &67$\pm$4 &64$\pm$5 &66$\pm$1 \\
&AB I - EYE open &68$\pm$6 &71$\pm$2 &71$\pm$6 &71$\pm$3 \\
\bottomrule
\end{tabular}
\caption{Classification resluts: AUC obtained during all the four analysis, with different number of Principal Components}
\label{tab:classification_pearson_pca}
\end{table}





\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_vs_pc_all}
   \caption{Classification on the entire dataset ABIDE I+II}
   \label{}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_vs_pc_ab1}
   \caption{Classification on dataset ABIDE I - only open eyes}
   \label{}
\end{subfigure}
\caption{AUC scores obtained from Controls/ASD classification of data using different numbers of principal components during dimensionaliti reduction with PCA.
REsults obtained with different analysis are marked with different colors: classification with data harmonization implemented inside the k-fold cross validation procedure are marked \textbf{blue}, classification of data harmonized upstream \textbf{orange}, classification of raw data (not harmonized) \textbf{green} and AUC scores of controls/ASD classification obtained with the adversarial network \textbf{red}.
}
\label{fig:classification_pearson_pca}
\end{figure}

\section{Discussion on classification results}

\paragraph{Comparison between Pearson and Wavelets}
From results reported on section \ref{sec:pearson_results} and \ref{sec:wavelet_results} it is quite evident that average results obtained with pearson correlation coefficients are systematically higher than wavelet-based correlation coefficients.
We can therefore assume that working with Wavelet-based correlation coefficient obtained through processes described in section \ref{sec:wavelet_theory} does not bring any advantages to this kind of analysis.
A possible reason for this outcome could be the loss of information during the extraction of these coefficients because of the introduction of redundancy and randomness into this analysis. Starting from two timeseries Wavelet transform compute a 2D matrix for each timeseries and uses them to calculate the cross-power spectrum as explained in the dedicated section.
Doing so we create a redundant representation of timeseries data moving from one dimension to two-dimensional data.
We furthermore compare the cross-power spectrum obtained from the wavelet transform of each timeseries, with a red-noise cross-power spectrum to assess the significativity of our data.
In the end we shrink again our dimensions to reduce to a single data point representing a correlation.

Since differences of traits between controls and ASDs are not very strong, this process can cause the loss of this weak information, which does not happen with a more direct and linear correlation calculus like Pearson coefficients.

This lower scores is found both on Wavelet coefficients of in-phase and counter-phase time percentage, and not even the combination of this two in a double-sized array, or with a coefficient made by the difference between these two coefficients brings any benefits, as an evidence that this procedure is not the most suitable to our classification goals.

\paragraph{Effects of a more regular dataset}
Let's focus for a moment just on table \ref{tab:classification_pearson} (even if what follows also applies to Wavelet results) to discuss the benefits of reducing the dataset size making cuts on covariates to run analysis on a more homogeneous dataset.
We can state that the eye status of patients during the scan has a non-negligible impact on the outcome of classification since limiting the dataset only on subjects with open eyes systematically improves classification scores of more than 1 \% for every dataset (ABIDE I+II, ABIDE I only, ABIDE II only).
As introduced in chapter \ref{chap:autism} there can be differences in brain functional areas between patients with open eyes and patients with eyes closed due to activation of different cortex and gyri areas or because during the scan, some patients may be fallen asleep, and this heavily modify the functional connectivity matrix.
For this reason, removing these source of variability could bring to a cleaner distinction of relevant patterns between Controls data and ASDs.
We can also notice that results on the dataset made by ABIDE I + II are mostly driven to the data belonging to ABIDE I than those belonging to ABIDE II, and this is true whether we restrict our analysis on open eyes oatients only or we do not.
In fact, putting together the two dataset does not lead to a great improvement of scores than limiting just on ABIDE I.
It is possible that this trend is due to teo main factors: the greater number of ABIDE I patient than ABIDE II, and the presence in ABIDE I of a great site: NYU containing a great percentage of patient which are also balanced between controls and ASDs.
This could monopolise the results since a great percentage of train and test data would belong to this center leading to a lower variability due to site-related features as demonstrated in studies like \cite{spera-2019}.
(Ma allora perchè questo trend lo vediamo anche sui dati armonizzati?)


\paragraph{Comparison with the random forest classifier}
As a general rule of thumb is alway a good practice to compare results obtained with a complex model, to results obtained with a simpler machine learning classifier like Random Forest. Instead of opting for a complex model, simpler methods should be tried to establish a baseline and allow a meaningful comparison. The principle of Occam's razor, when applied to machine learning demands that if two model perform quite the same, the simpler of the two should be picked \cite{domingos-1999}. This would be true expecially for our data since we start from a situation of overfitting and there is no need to choose a complex model a priori if this choice is not supported by data. This same principle was applied during the choice of the best structure of the deep learning model.

Comparing the results obtained by the deep model with those obtained with a Random Forest, (table \ref{tab:comparison_dnn_rf_pearson} and \ref{tab:comparison_dnn_rf_win+wout}) it is possible to deduce that a deep model is more adapt to find characteristic patterns between controls and ASD than a simpler Ranfom Forest classifier.
Scores obtained with a deep model are systematically way higher than those obtained with a random forest classifier, except for those we get cassifying data with upstream harmonization, but as explained in paragraph \ref{par:effect_of_harmonization} this procedure leads to biased data and as a consequence results are not reliable.
We are still going to use Random Forest classifier in some of the remaining anslysis just as a comparison, even though we assert that a deep model is more suitable to understand differences between the two classes of interest.



\paragraph{Effect of harmonization} \ref{par:effect_of_harmonization}
An other common trend that stands out from all the analysis is the systematic scores' improvement with the method of upstream harmonization explained on section \ref{sec:} and outlined on flowchart in figure \ref{fig:harmon_upstream_flowchart}.
This improvement disappears when we implement harmonization inside the k-fold cross validation procedure as sketched in flowchart \ref{fig:harmon_kfold_flowchart}.
These two implementation differ on the order with which harmonization is implemented: with upstream harmonization we harmonize the whole dataset and subsequently we split it into train and test to run a cross validation procedure; conversely, with harmonization implemented inside the cross validation procedure, we run harmonization only after the division of the dataset into a Train and Test subsets.
We can state that this is an evidence that the upstream harmonization is the wrong way to proceed, since its high results are affected by a bias due to data leakage which occur if we implement harmonization this way.
It is also possible that some good results on other similar studies, obtained with harmonization implemented in this same way (\cite{ingalhalikar-2021}) may be due to a similar data leakage as we have in this analysis.
As a general definition, we have data leakage when information outside the training set is used to create and train models. This additional information allows the model to know something additional about the data that otherwise it would have not known.
This leads to an improvement of model's prediction and an overestimation of performances.
In this case we have data leakage when we harmonize the entire dataset before splitting it into Train and Test, because, according to this way to proceed, when we use Train data, they already contain informations about Test data because they have been created using covariates, features and other informations belonging to the whole dataset.
We can then assess that the right way to implement harmonization is inside the k-fold cross validation, or, if we don't run a cross validation, it is important to implement it after the splitting of dataset into a train and a test goups. This way, we can create the harmonization model only on Train data we give as input to the classifier so that there are not external information in the train dataset.
We can then harmonize data belonging to test applying to them the harmonization model, with parameters created on the train dataset.
For this reason, from now on, when we refer to \textquotedblleft harmonized data \textquotedblright, we are referring to the procedure that harmonizes data inside the k-fold CV.

Comparing results of harmonized data with results on raw data we don't notice a particular improvement.


\paragraph{Results with adversarial}
DA SCRIVERE MEGLIO; L'idea è giusta?
Looking at tables \ref{tab:classification_pearson} we notice that the adversarial model perform slightly worse working on Pearson correlation coefficients while for wavelet coefficients (tables \ref{tab:classification_win} and \ref{tab:classification_win+wout}) we obtain values greater or equal than results obtained with a simple dnn.
%scrivere che quando faccio l'harmonization non uso nessuna covariata specificandola come non lineare perchè non ci sono ragioni per farlo
An possible reason, in light of the results obtained with harmonization of wavelet coefficients which is not able to make sites-related feature unrecognisable, is that when computing wavelet coefficients, linear site-dependent relationship is lost and maybe a non linear relation is created.
Same thing does not happen with pearson coefficients where linear relations remain. harmonization is able to remove linear relations on pearson but not on wavelets, and a classifier is more likely to get these non-linear relations with site and perform better on wavelets.





\paragraph{Effect of dimensionality reduction}
This same alertness we paid for the implementation of harmonization is to be applied when extracting PCA because it is possible to make the mistake of computing Princial components on the whole dataset, before its separation into Train and Test sets.
For this reason we implemented PCA extracting PCs from the Training dataset and applying the transformation to the both of Train and Test dataset.
At a first glance we can notice that reducing the number of PCs leads to a gradual reduction of classification performances.
This trend is more evident in the whole datataset, while in the ABIDE I only open eyes dataset, we obtain a flatter performances curve.
A reduction from 5995 to 800 or 400 feature is a substanctial reduction since we are lowering our dimensionality by an order of magnitude still preserving an high classification AUC comparable to that obtained without PCA. In this analysis as well, data leakage due to the prior harmonization of dataset plays an important role as it leads to a rise in classification performances for any choice of number of PCs.

An other aspect that leap out comparing results with PCA with results without it, is a light improvement of performances of the adversarial learning.
An explaination to this can be that a reduced source of error (deriving from this dimensionality reduction) brings to a reduced variability in site-related features making these confounding feature more recognizable. This could lead to a smoother weight updates that doesn't affect classification weigths as much as it does with raw data.

What arises from this analysis, is that PCA represent an important strategy to tackle the problem of dimensionality. It is plausible tbat among all the 5995 features lots of them are uninformative for Control/ASD classification and they just increase dimensions of data without any benefit. This can be linked to a biological reason: we can in fact imagine that there are some brain areas that just don't significantly change their activity between healthy and ASD people. Following this explaination, all the coefficients representing a link between them are just similar between the two groups and don't bring any benefit in this discrimination. For this reason, reducing the source of noise due to this data-points leads to a equally informative dataset.

\newpage


\chapter{SHAP - Implementation and results}

In this chapter we are going to give a look at how we implemented a feature explainatory model using the SHAP algorithm described in section \ref{sec:shap}.

We choose to run this analysis only on Pearson correlation coefficients since they gave the best classification results, and we use the whole dataset consisting of ABIDE I + ABIDE II patients with the same characteristics as we choose for classification: only males and with an age range of 5-40.
In addition we put in appendix some results obtained on the dataset ABIDE I only open eye.

METTO QUESTO IN UNA LISTA NUMERATA?
What we are looking in this section is for the most important features computed on the entire test dataset and even if for each test data they contributed poorly, there is a certain gerarchy of importance between them.
Subsequently, we ask if these most important features are the same across the different pipelines we followed or if each pipeline makes prediction using different subsets of relevant features more specifically we want to check if before and after the harmonization procedure, the most important features still remain the same or if they change depending on the kind of analysis we are carrying out.
We compared important features extracted from DNNs with those important for a Random Forest classifier to evaluate if there are feature that stand out regardless the algorithm used to compute them since we used SHAP to explain feature from the DNN and the feature\_importance class  provided by scikit learn to assess feature in a random forest classifier.
Afterwards, we select 5 analysis to focus on, because we regard at them as the most significative to count ho many features are common in these analysis between the first 60 most important features.
At the end we come back to the biological meaning of the most important feature to assess whether they are linked to some particular brain areas which caused an altered functional connectivity between heathy and ASDs patients.

\hfill

To instantiate the desired class, DeepExplainer in our case, we need the trained deep model and a fraction of the training data to use as background. We can choose as many background data as we want keeping in mind that the bigger is this background subset $N_{bkg}$, the more accurate is the computing of shapley values, but the more computationally expansive this process will be, in particular it would occupy a vast amount of RAM memory and eventually run out of it;
However, since the error we make in shap values linked to this choice is $\sim 1/\sqrt{N_{bkg}}$, a background dataset made of 100 samples is already a good compomise to have a relatively small error.
We choose though, a background size of 500 samples for the entire dataset and a size of 100 samples when working on ABIDE I only open eyes, since we had a reduced number of train data to collect background data from.

Once the explainer is created on our model, we need to input as many test dataset as we want to explain, and for each one of them, the explainer will output an array containing a shap coefficient for all the n\_features of each test data.
Once inputed a batch of test data, we obtain a matrix of the same size of the test dataset we used, shaped n\_samples x n\_features.
We choose to input all the test dataset for each fold to obtain a more statistically accurate estimate of feature importance.

Since we are presenting our results following a k-fold cross validation scheme, we have to implement this shap process inside the CV.
To do so and collect a final and general result, we computed the shap values for all the test samples of each k-fold and we stacked them together.
At the end of the CV, we have a matrix of shap values, and we can proceed to visualize the results.

\hfill

We can visualize, for each test data, the contribution of each feature on pushing or pulling the predicted output from the baseline output of our model, but to produce this kind of plot we need to save the Test datasets as well.
An example is shown in figure \ref{fig:shap_waterfall}.
In this type of plot, called \emph{waterfall plot}, is shown the result on a single test data. We have the baseline value of the model in the lower right corner and on the top left the prediction obtained with this test data. For each feature, positive shapley values, representing positive contribution to our output are colored red and negative ones are blue.
Even if this is just an example taken from a single test data, it representative of all the analysis where there is not a feature whose contribution is way greater than the others. In fact each feature makes a small contribution to the final outcome, but the sum of them push the model towards a output.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{shap/waterfall_plot}
\caption{Example of a waterfall plot obtained with a single test data. For each row is displayed the feature's name and value and its contribution to the final output: negative contribution are colored blue and positive ones red. It is also marked the reference output in the bottom right corner, and the output of this instance on the top left.
}\label{fig:shap_waterfall}
\end{figure}


If we want look at the overall result on all the test data, it would be inconvenient to create a plot like \ref{shap_waterfall} for each data. For this reason we have to look at an other plot called \emph{force plot}

The force plot of feature's importance can involve either the module of shap values or shap values themselves (positive or negative), an example of the latter is figure \ref{fig:shap_dot_kfold}: this plot shows for each instance (for each sample from the test data), the contribution of one particular feature to the final output related to that sample, and it is represented as a single colored spot, the color indicated the strength of the contribution of that feature to the final outcome, and the position in respect to the vertical line, indicates the sign of that contribution: on the right we found those that gave a positive contribution (pushed the output towards values greater than the reference output), and on the left are placed those that had a negative contribution.
This kind of plot, though, is not the best suitable for readibility in our case because of the large amount of features and the small contribution of each feature to the final outcome, in respect to other feature.
This results in a smoothed scatter plot for each feature, containing many but unnecessary details which make this kind of plot not the best suitable for our study.

We can choose to visualize the total amount of \textquotedblleft contribution \textquotedblright of each feature, by involving the absolute value of the shap values of figure \ref{fig:shap_dot_kfold} to obtain figure \ref{fig:shap_bar_kfold} representing a bar plot of all the first 20 important feature from the most to the least important.
Each bar represents a feature importance, computed from equation \ref{eq:shap_magnitude}.

For a better readability we prefer reporting the results like this, in terms of absolute shapley value, where we just consider the module of the shap values, regardless its positive or negative contribution to the output, just to understand the absolute contribution of a single feature for a given analysis pipeline.

From this plot we have a more immediate understanding of what are the most important features, and, as we noticed from the example in figure \ref{fig:shap_waterfall}, there is not a standing out feature, or group of feature that have a contribution way greater than the others, but in fact feature's importance has a quite smoothed descending trend.
An example to visualize this trend is shown in figures \ref{fig:shap_bar_kfold} representing the first 20 most important features extracted from DNN trained on data harmonized inside k-fold CV.

\begin{figure}[h!]
\centering
\begin{subfigure}[t]{.45\linewidth}
   \fbox{\includegraphics[width=1\linewidth]{shap/ab_all/shap_dot_kfold}}
   \caption{Plot of dotted shapley values: for each feature each instance (each test sample) is represented by a dot, the position indicates a positive or negative contribution to the output, while the color represents the module of that contribution}
   \label{fig:shap_dot_kfold}
\end{subfigure}
\hspace{3mm}
\begin{subfigure}[t]{.439\linewidth}
  \fbox{ \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_kfold}}
   \caption{Bar plot of shapley values: for each feature is represented the mean absolute value of all the shap values computed for each instance}
   \label{fig:shap_bar_kfold}
\end{subfigure}
\caption{Two different but closely related plots of shap values for the first 20 most important features obtained from data where harmonization procedure is implemented inside the k-fold CV scheme
}
\label{fig:shap_features_kfold_both}
\end{figure}





\newpage
\paragraph{Feature importance with DNN} \hfill

 that there are some features common to all the four pipelines, in particular features




\begin{figure}[h!]
\centering
\begin{subfigure}[c]{.45\linewidth}
  \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_kfold}
   \caption{Bar plot of shap values and related important features extracted with harmonized data, with harmonization procedure implemented inside \textbf{k-fold} CV}
   \label{fig:shap_bar_kfold_}
\end{subfigure}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_upstream}
   \caption{Bar plot of shap values and related important features extracted with harmonized data, with harmonization procedure implemented \textbf{upstream}, before the k-fold}
   \label{}
\end{subfigure}
\hspace{3mm}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_no}
   \caption{Bar plot of shap values and related important features extracted from \textbf{raw}, not-harmonized data}
   \label{}
\end{subfigure}
\begin{subfigure}[c]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/shap_bar_adv}
   \caption{Bar plot of shap values and related important features extracted from raw data using the \textbf{Adversarial} neural network,}
   \label{}
\end{subfigure}
\caption{First twenty most important features plotted with a bar plot of mean absolute shap values. Extracted from each of the four analysis we run with DNNs and Adversarial network}
\label{fig:shap_abide_all}
\end{figure}

If we plot the first twenty important feature for DNNs, obtained from each pipeline described in section \ref{sec:analysis_workflow}, we can notice that some features seem to be persistent through all the four pipelines; we find that: feature 592 and 1521 appear, even if in different order of importance, in all our four analysis;
(We replicated the plot in figure \ref{fig:shap_bar_kfold} putting it again in figure \ref{fig:shap_bar_kfold_} for a more immediate visual comparison of all these twenty important features)
 %features 244 592 777 1513 1521 3042 3334 3401 appear in 3 out of 4 analysis,
%and features 2181, 244, 1507, 2126, 1631, 724 and 1511, appear just in two of them.
They represent:
\begin{itemize}
\item Feature 592: correlation between Right Middle Temporal Gyrus ( anterior division) and Left Superior Temporal Gyrus ( anterior division)
\item Feature 1521: correlation between Left Angular Gyrus  and Right Middle Temporal Gyrus ( posterior division)

%\item Feature 244: correlation between Right Inferior Frontal Gyrus ( pars triangularis) and Right Accumbens
%\item Feature 777: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Middle Temporal Gyrus ( posterior division)
%\item Feature 1513: correlation between Left Angular Gyrus  and Right Temporal Pole
%\item Feature 3042: correlation between Right Frontal Orbital Cortex  and Left Middle Temporal Gyrus ( temporooccipital part)
%\item Feature 3334: correlation between Right Parahippocampal Gyrus ( posterior division) and Right Accumbens
%\item Feature 3401: correlation between Right Parahippocampal Gyrus ( posterior division) and Right Parahippocampal Gyrus ( anterior division)
\end{itemize}

\newpage
\paragraph{Feature importance with Random Forest}

Same comparison can take place using a Random Forest classifier for the three analysis above.
With this Random Forest classifier we extracted features shown in figure \ref{fig:shap_abide_rf_all} and we notice that are common to the three analysis feature 710, 1127, 1703, 748, 2735, 1400, 2811


\begin{figure}
\centering
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/rf_feature_importance_kfold}
   \caption{kfold}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/rf_feature_importance_upstream}
   \caption{upstream}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab_all/rf_feature_importance_no}
   \caption{Non-harmonized}
   \label{}
\end{subfigure}
\caption{Feature importance obtained with a Random Forest classifier: results on the entire dataset.}
\label{fig:shap_abide_rf_all}
\end{figure}



\begin{itemize}
\item Feature 710: correlation between Right Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus (nan)
\item Feature 1127: correlation between Left Postcentral Gyrus (nan) and Right Postcentral Gyrus (nan)
\item Feature 1703: correlation between Right Lateral Occipital Cortex ( inferior division) and Right Supramarginal Gyrus ( anterior division)
\item Feature 748: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus (nan)
\item Feature 2735: correlation between Right Precuneous Cortex (nan) and Right Middle Temporal Gyrus ( anterior division)
\item Feature 1400: correlation between Left Supramarginal Gyrus ( posterior division) and Right Inferior Frontal Gyrus ( pars triangularis)
\item Feature 2811: correlation between Left Precuneous Cortex (nan) and Right Middle Temporal Gyrus ( posterior division)
\end{itemize}

\newpage
\paragraph{Cross-analysis on feature importance}

So far, we discussed results linked to these images just as a rapid comparison of the results of this kind of analysis. Now we go deeper to fulfill a more meaningful analysis of what are important feature.

Since as explained in section \ref{sec:classification_discussion} we want to exclude from our analysis the procedure of upstream harmonization because it creates a bias in results, from now on, we focus only on the other pipelines and when we refers to harmonized data, we are referring to data with harmonization procedure implemented inside kfold.

To assess what are the most relevant recurrent feature between these different types of analysis, we choose to limit our analysis to the 1\% important features among 5995; to this end we collected the first 60 important features and we limited our analysis to only 5 classification procedures listed below.


\begin{enumerate}
\item Classification of harmonized data (with harmonization implemented inside k-fold) using the DNN \label{proc:dnn_kfold}
\item Classification of harmonized data using the Random Forest classifier \label{proc:rf_kfold}
\item Classification of raw data using the DNN \label{proc:dnn_no}
\item Classification of raw data using the Random Forest classifier \label{proc:rf_no}
\item Classification using raw data with the Adversarial Neural Network \label{proc:adv}
\end{enumerate}



Firstly we checked if and how many common features we find among the first 60 important features between these five classification.
But turns out that there are no common features between all these 5 pipelines taking into account only the first 60.
%If we exclude, though, from our analysis the adversarial learning classification we find that features 1513, 3058, 1067 are common between the remaining classification methods.
%They represent:
%\begin{itemize}
%\item Feature 1513: correlation between Left Angular Gyrus (nan) and Right Temporal Pole (nan)
%\item Feature 3058: correlation between Right Frontal Orbital Cortex (nan) and Left Angular Gyrus (nan)
%\item Feature 1067: correlation between Right Postcentral Gyrus (nan) and Right Superior Temporal Gyrus (posterior division)
%\end{itemize}

Now, we carry out analysis on subgroups of the previous classification methods and we report the number of common important features among the first 60 most relevant as a number and as percentage. This kind of analysis could be useful to assess whether the harmonization procedure significatively changes the number of important feature and if the type of classifier and the related technique of feature importance assessment heavily affect the result of this feature importance.

\begin{itemize}
\item Focusing just on \textbf{DNN} we compared pipelines \ref{proc:dnn_kfold}. and \ref{proc:dnn_no}. finding that 22 features (37 \%) out of 60 are common.
\item Focusing just on \textbf{Random Forest} classifier, we compared important features between \ref{proc:rf_kfold}. and \ref{proc:rf_no}. and found that 33 features out of 60 are common (55\%).
\item Comparing the same analysis pipeline with DNN and Random Forest we find that with \textbf{harmonized data}: procedure \ref{proc:rf_kfold}. and \ref{proc:dnn_kfold}. there are 13 common features (22 \%)
\item Comparing results on \textbf{raw data} obtained with DNN (pipeline \ref{proc:dnn_no}.) and \textbf{Random Forest} (\ref{proc:rf_no}.) we obtain 7 common features  (12\%)
\item Comparing results of the \textbf{Adversarial network} (\ref{proc:adv}.) with the \textbf{harmonized data} with DNN (\ref{proc:dnn_kfold}.) we obtain 21 common features (35\%)
\item Comparing results of the \textbf{Adversarial network} (\ref{proc:adv}.) with DNN classification of \textbf{raw data} (\ref{proc:dnn_no}.) we obtain 28 common features (47\%)
\end{itemize}



\paragraph{Brain areas related to important features} \hfill

Since each feature is representative of a correlation between two brain areas, and each brain area is involved in more than 100 features, we wonder not simply if there are some feature that repeat themselves, but if there are some brain areas which are particularly involved between the first 60 features of these classification pipelines.
For this purpose, we plotted an histogram to represent the number of occurrences for each brain areas among the first 60 features for each classification procedure, and subsequently we created the histogram of the overall important brain areas pooling all of these results into a single histogram .

Let's give a look at the histograms of the most recurrent brain areas between all the 60 important features for each classification procedure: figure \ref{fig:histograms_60}

\begin{figure}
\centering
\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_no}
   \caption{Important areas from Raw data classified with DNN}
   \label{}
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_adv}
   \caption{Important areas from Raw data classified with Adversarial network}
   \label{}
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_kfold}
   \caption{Important areas from harmonized data classified with DNN}
   \label{}
\end{subfigure}
\end{figure}
\begin{figure}\ContinuedFloat
%\medskip
\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_rf_kfold}
   \caption{Important areas from harmonized data classified with Random Forest}
   \label{}
\end{subfigure}

\begin{subfigure}[b]{1\columnwidth}
   \includegraphics[width = 1\columnwidth, height = 6cm]{shap/ab_all/important_areas_features_rf_no}
   \caption{Important areas from Raw data classified with Random Forest}
   \label{}
\end{subfigure}
\caption{Histograms of important brain areas extracted from the first 60 important features of different classification procedures}
\label{fig:histograms_60}
\end{figure}


Since it is immediate to notice that there are recurrent brain areas across all the analysis, we are willing to visualize (fig \ref{fig:important_areas_features_all}) the overall histogram putting together results obtained from each analysis.
To this end, as mentioned above, we computed in figure \ref{fig:important_areas_features_all} the overall histogram of the most recurrent brain areas between these common feature.

\begin{figure}[h]
\centering
  \includegraphics[angle = 270, origin = c, width=0.75\linewidth]{shap/ab_all/important_areas_features_all}
\caption{Histogram of the most important brain areas putting together results obtained from all the five analysis listed above.}
\label{fig:important_areas_features_all}
\end{figure}

\section{Discussion on feature importance}




\newpage

\addcontentsline{toc}{chapter}{APPENDIX}

\appendix
\chapter{Principles of MRI imaging}
Nuclear magnetic resonance imaging, is a widespread imaging technique widely employed in radiology to obtain anatomical images or study physiological processes, taking advantage of its flexible sensitivity to different tissues.
The physical principle of this imaging is the interaction of a nuclear spin of any tissue's molecule, with an external magnetic field $\textbf B_0$.

Molecule that contribute the most to the signal is water's, and specifically, hydrogen nuclei are the dominant source of signal in MRI.

When a nucleus undergoes an external magnetic field, the interaction would make the angular moment $\overrightarrow{\mu}$ align with the magnetic field,
% Talk about energy levels. Review lines below.
but, since a proton has an intrinsec spin, it has a magnetic momentum which brings it start preprocessing
%since protons have a thermal energy associated with the temperature T, they are not static, so they start precessing
around the symmetry axes given by the direction of the magnetic field $\overrightarrow{B_0}$.

The torque resulting from the application of a static magnetic field, applied to a spinning proton is given by
\begin{equation}
\overrightarrow{\tau} = \overrightarrow{\mu} \times \overrightarrow{B}
\end{equation}
This torque is what cause the proton precession and the rotation frequency $\omega_0$ of the spinning proton is given by the Larmor equation
\begin{equation}\label{eq:larmorfrequency}
\omega_0 = \gamma B_0
\end{equation}
where $B_0$ is the magnitude of the external magnetic field and $\gamma$ is a constant called \emph{gyromagnetic factor} and is characteristic of each particle or nucleus.
The gyromagnetic factor of a proton in water has a value of $\gamma = 2.7\cdot 10^8 \frac{rad\cdot s}{T}$.
For a typical scanner with a magnetic field of 2-3 T, Larmor frequency assumes values of $\sim MHz$ just below radio waves.

When protons undergo a magnetic field, they can assume two states in relation to the direction of the external field: parallel and anti-parallel
%Proton spins can assume two values: up and down in relation to the external field direction ;
therefore, after a body underwent a magnetic field, its total magnetization is given by the net difference between spins up and spins down.
% spin excess? p.4
% write abou quantum energy levels
This net magnetization can be represented by a single vector pointing parallel or anti-parallel to the direction of the external field and if we want to detect a signal related to this magnetization, we have to perturb this vector from its equilibrium.
This would cause the precession of this magnetization vector around the magnetic field
This precession would therefore cause a changing flux which can be detected by an external coil.
% only if it is perturbated from its equilibrium state.
This is achieved by using another external magnetic field under the form of a radiofrequency impulse, with a frequence resonting with the Larmor frequence of preprocessing. This flips the spin in a direction usually perpendicular to the original $B_0$ one, which is supposed to be along $\hat z$ axis. Thus protons will start rotating in the x-y plane, still continuing preceding around their symmetry axis.
Doing so, they gradually lose their initial energy and tend to realign along z-axis.
This process is called relaxation and it follows an exponential decay with a time constant $T_1$ called longitudinal relaxation (longitudinal in relation to the original $B_0 \hat z$ direction), or spin-lattice relaxation, referring to the loss of thermal energy through interaxtion with surroading lattice. The total magnetization along z-axis during time, after the rf impulse, will then regrow along z-axis following:
\begin{equation}
M_z(t) = M_0 (1-e^{-t/T_1})
\end{equation}.
An other effect to be take into account is the interaction of spin with a the local field where the external field and the neighbor's one coexist. This spin-spin interaction causes the dephasing of spin. This brings to two different relaxation velocities along z-axis and x-y plane called transversal relaxation which occurs without energy exchange, and is characterized by a time constant $T_2$. The two relaxation processes are described by Bloch equations and once integrated bring to the equation of the evolution of magnetization over time.
\begin{equation}
M_{x, y}(t) = M_{x,y}(0)\cdot e^{-t/T_2}
\end{equation}
In a real system, however, there's and additional dephasing source, coming from external field inhomogeneities.
This effect is often taken into account by the introduction of a different decay time $T_2'$ which along with $T_2$ bring to a overall time costant given by $\frac{1}{T_2*} = \frac{1}{T_2}+\frac{1}{T_2'}$.
Sometimes, though, this additional $T_2'$ is so small compared to $T_2$, that dominates over it, resulting in a rapid loss of information.
This can be avoided employing a specific rf pulse sequence called spin-echo method.
\section{spin-echo method}
The spin echo method employes two rf pulses the first with an angle of $\pi/2$ and the second of $\pi$.
\begin{enumerate}
% Add a figure like on page 123
\item The first radio-frequency pulse is applied with an angle of $\pi/2$ and this produces the spin to flip over a transversal plane and they gradually start dephasing because they undergo small field nonuniformities different from point to point, so they start fanning out, some move faster and some are delayed in relation to the average magnetization vector
\item The second rf impulse is sent, with an angle of $\pi$ to the transversal magnetization axis y'. This ensures that all the accumulated phases flip and become negative,
\item Late and early phases continue to accumulate delay, but now this delay pushes them towards the main magnetization vector,and the phase will therefore return zero. The elapsed time up to this moment is called \emph{Time to echo} $T_E$.
\end{enumerate}
\section{Image acquisition and k-space}
For imaging acquisition, both effects must be taken into account. Moreover, it is necessary to relate a signal with a spatial position.
In order to do so, in addition to the initial static magnetic field $B_0$ there's another field with lower intensity than $B_0$ which is not uniform in intensity, it follows a spatial gradient so that the total magnetic field along $\hat z$-axis is given by the sum of this two contributes, and the signal contains space-varying frequency components according to equation \ref{eq:larmorfrequency} which can be rewritten as $\omega(z) = \gamma B(z)$ being x the spatial coordinate and with $B_z$ now given by the relation $B_z(z, t) = B_0 + z\cdot G(t)$.
The physical process underlying signal acquisition is Faraday induction, according to which an electrical potential is related to the variation of a magnetic flux over time $fem = -\frac{d\Phi}{dt}$ being $\Phi$ the varying flux through the receiving coil.
The varying flux is obtained just after the application of the rf pulse, while tipped spins are precessing and realigning to z-axis, and all this process is called free induction decay (FID). This coil detect only the signal deriving from transversal plane because the one along longitudinal axes would be saturated from the strong magnetic field of the main $B_0$ coil. Changes in acquisition sequences and rf pulses allow us to emphasise one of the three fundamental parameters $T_1, T_2, \rho$ where $\rho$ is the proton concentration inside the tissue.

An image acquired with a spin echo sequence can be T1, T2 or proton density weighted

The acquired signal is examined by inverse Fourier transform, exploiting the differences from Larmor frequency which cause a phase displacement along z-axis $\phi_G(z, t)$
\begin{equation}
s(t) = \int_0^tdz\rho \ (9.14) \ pag \ 145
\end{equation}

Data are acquired under the form of a matrix called k-space.
K-space is a coordinate system used to store spatial frequencies information before applying the inverse fourier transform.
Low spatial frequencies, corresponding to large object across the whole real image, are encoded at the center of the matrix and high spatial frequencies corresponding to small objects and finer detailes, are encoded in the peripheries.

The construction of k-space is done step by step in relation to the gradient applied time by time, producing a trajectory on the k-space.
Each spatial line of k-space is acquired after TR, so the signal has to be recreated each time.
Starting from k = 0, the center of k-space, after a preencoding gradient, we move toward negative position of both k-phase and k-frequency; subsequently a frequency encoding gradient (also called readout gradient) is applied and the signal is collected along the frequency encoding direction.
The signal is then reformed and the process is repeated, from k = 0, to a new k-phase position and an other line of k-frequency is acquired.




\section{Gradient Echo sequences}
DA RIVEDERE QUESTA E EPI

The sequence employed to acquire functional imaging is the Echo Planar Imging (EPI) which is a particular sequence from the family of gradient echo sequences.
Gradient echo sequences differs from spin echo because they allow a faster acquisition time because they use an excitation flip angle less than 90 deg, this allows a minor time interval to make spins back to their longituginal component as they are not completely flipped on the transverse plane.
The transverse component just created decay and dephase according to T2*. If a gradient is applied, they dephase faster, and it is then reversed to make them rephase.
In an gradient echo sequence, the echo peak lies on T2* decay curve while in a spin echo, it lies on T2 decay curve; and thanks to whis it is susceptible to any uniformity variation among which that due to hemoglobin variation.
Since TR are short there is some transversal magnetization left after the acquisition, in \emph{spoiled} Gradient Echo, the residual magnetization is wiped out.
Ultra fast gradient echo start with a 180 degrees pulse, to invert longitudinal magnetization, in order to increase T1 weighting, and allk-space lines are acquired after this impulse. This first pulse is followed by minor pulses, and k-space is acquired after each echo.


\section{EPI}
During an EPI session, a single rf inpulse of 90 degrees is applied and the whole k-space is acquired in a single acquisition:
as shown in figure \ref{fig:epi-gradients} after the initial RF pulse, together with the gradient for slice selection (G-Select), the acquisition of k-space information starts from the left bottom as in figure \ref{fig:epi-kspace} and each line along k-frequency corresponging to the readout gradient (G-Readout) is acquired.
The period of the G-Readout is the echo time $T_E$, and is modulated such that is sensible to $T_2^{\ast}$ setting $T_E \approx T_2^{\ast}$.
During the acquisition of the whole k-space each line is shifted from the other along k-phase direction by applying brief pulses of the phase encoding gradient (G-Select) called blip, creating a snake-like path during the acquisition of the k-space matrix.
%shmitt tuner -echo planar Imaging


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{mri/epi-gradients}
\caption{}
\label{fig:epi-gradients}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{mri/epi-kspace}
\caption{}
\label{}
\end{subfigure}
\caption{}
\label{fig:epi}
\end{figure}


Pixel-to-pixel frequency difference in the phase-encoding direction emphasizes chemical shift artifact.
EPI generates gradient echoes under the FID curve created by the RF flip. As the curve decays according to T2*, gradient EPI sequences willl be T2* weighted.
Sensitivity to T2*, of course, introduces sensitivity to artifacts caused by changes in magnetic susceptibility (eg, air/tissue interfaces) and imperfect magnet shim

\chapter{Other classification results}

\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrr}\toprule
Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adv 264-8 \\\midrule
AB I$\pm$II &60$\pm$1 &71$\pm$3 &63$\pm$2 &60$\pm$1 \\
AB I &61$\pm$2 &69$\pm$3 &62$\pm$2 &61$\pm$2 \\
AB 2 &56$\pm$3 &69$\pm$4 &60$\pm$4 &59$\pm$4 \\
AB I $\pm$ II - EYE open &63$\pm$2 &68$\pm$3 &64$\pm$2 &63$\pm$3 \\
AB I open EYE &63$\pm$5 &74$\pm$3 &71$\pm$4 &72$\pm$2 \\
AB II EYE open &63$\pm$5 &70$\pm$3 &63$\pm$3 &62$\pm$3 \\
AB I $\pm$ II AGE=all, EYE=all &65$\pm$3 &71$\pm$2 &66$\pm$2 &63$\pm$2 \\
\bottomrule
\end{tabular}
\caption{Classification scores using only out-of-phase Wavelet coefficients}
\label{tab:classification_wout}
\end{table}


\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrr}\toprule
Dataset &Harmon K-fold &Harmon. upstream &No harmon. &Adv 264-8 \\\midrule
AB I$\pm$II &67$\pm$ 3 &70$\pm$ 3 &66 $\pm$3 &66 $\pm$3 \\
AB I &70$\pm$ 3 &72 $\pm$2 &68 $\pm$3 &68$\pm$ 3 \\
AB 2 &61$\pm$ 4 &65$\pm$ 4 &64 $\pm$5 &65 $\pm$4 \\
AB I $\pm$ II - EYE open &70$\pm$ 1 &73$\pm$ 2 &71 $\pm$1 &68 $\pm$2 \\
AB I open EYE &71$\pm$ 4 &74 $\pm$3 &71 $\pm$3 &68 $\pm$2 \\
AB II EYE open &62 $\pm$4 &66 $\pm$3 &68 $\pm$3 &64$\pm$ 5 \\
\bottomrule
\end{tabular}
\caption{Classifiaction scores using (W\_in - W\_out) wavelet coefficients coefficients}
\label{tab:classification_win-wout}
\end{table}





\begin{table}[!htp]\centering
\begin{tabular}{lccc}
\toprule
Structure &k-fold &upstream &no harmonization \\
\midrule
3-2-1 &60$\pm$2 &63$\pm$3 &60$\pm$4 \\
8-8-1 &64$\pm$1 &65$\pm$4 &64$\pm$5 \\
64-8-1 &69$\pm$1 &72$\pm$2 &68$\pm$1 \\
64-32-8-1 &68$\pm$1 &72$\pm$2 &69$\pm$1 \\
128-8-1 &70$\pm$2 &71$\pm$3 &69$\pm$2 \\
128-64-1 &70$\pm$1 &72$\pm$3 &69$\pm$2 \\
\cellcolor[HTML]{ffff00}264-8-1 &\cellcolor[HTML]{ffff00}70$\pm$2 &\cellcolor[HTML]{ffff00}73$\pm$3 &\cellcolor[HTML]{ffff00}70$\pm$2 \\
512-8-1 &71$\pm$2 &73$\pm$2 &70$\pm$2 \\
1024-8-1 &70$\pm$1 &74$\pm$2 &71$\pm$2 \\
1024-32-1 &70$\pm$2 &74$\pm$2 &71$\pm$3 \\
\bottomrule
\end{tabular}
\caption{Different model structures and related performances for the three main analysis we carried out during this entire work. Hilighted the structure we choose to employ through all our analysis}\label{}
\end{table}

\section{Feature importance results on ABIDE I open eye dataset}

The same tests we did to assess feature importance on ABIDE I + II dataset, we carried out on ABIDE I with only patient with open eye.
Here in figure \ref{fig:shap_ab1} we report the results obtained from shap, related to the first twenty most relevant features.
We can notice at a first glance that there are less feature common to all the four analysis:
There are no common features among them that we can find across all these four analysis, if we limit our study on the first twenty.
We have to search among the first thirty features to find something in common, which is still a good procedure since we are dealing with 5995 features and the first 40 are just the 0.7\% of them.

We find that, among the first 30 features, feature 1400, 762, 748, 3042 are common to the four analysis.
If we limit again our study to the first 20 features, we can see that are common to three different analysis these additional features: 592 3477 3868




\begin{itemize}
\item Feature 1400: correlation between Left Supramarginal Gyrus ( posterior division) and Right Inferior Frontal Gyrus ( pars triangularis)
\item Feature 762: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Left Middle Frontal Gyrus
\item Feature 748: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus
\item Feature 3042: correlation between Right Frontal Orbital Cortex  and Left Middle Temporal Gyrus ( temporooccipital part)

\item Feature 592: correlation between Right Middle Temporal Gyrus ( anterior division) and Left Superior Temporal Gyrus ( anterior division)
\item Feature 3477: correlation between Left Parahippocampal Gyrus ( posterior division) and Right Precuneous Cortex
\item Feature 3868: correlation between Right Temporal Fusiform Cortex ( posterior division) and Right Inferior Temporal Gyrus ( anterior division)

\end{itemize}



\begin{figure}
\centering
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_no}
   \caption{Non-harmonized}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_upstream}
   \caption{upstream}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_kfold}
   \caption{kfold}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/shap_bar_adv}
   \caption{adversarial}
   \label{}
\end{subfigure}
\caption{Shap results on the dataset that gave the best classification performances: ABIDE I only open eyes.}
\label{fig:shap_ab1}
\end{figure}




With a random forest we obtain the following feature importances, searching among the first 40 features, since as we can notice from figures \ref{rf_feature_importance_ab1} there are not common features between the first twenty.
We find that features common to the 3 pipelines are: 3042, 1417, 748, 1400, 762


\begin{itemize}
\item Feature 3042: correlation between Right Frontal Orbital Cortex  and Left Middle Temporal Gyrus ( temporooccipital part)
\item Feature 1417: correlation between Left Supramarginal Gyrus ( posterior division) and Left Middle Temporal Gyrus ( temporooccipital part)
\item Feature 748: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus
\item Feature 1400: correlation between Left Supramarginal Gyrus ( posterior division) and Right Inferior Frontal Gyrus ( pars triangularis)
\item Feature 762: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Left Middle Frontal Gyrus
\end{itemize}


\begin{figure}
\centering
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/rf_feature_importance_no}
   \caption{Non-harmonized}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/rf_feature_importance_upstream}
   \caption{upstream}
   \label{}
\end{subfigure}
\begin{subfigure}[b]{.45\textwidth}
   \includegraphics[width=1\linewidth]{shap/ab1_openeye/rf_feature_importance_kfold}
   \caption{kfold}
   \label{}
\end{subfigure}
\caption{Feature importance obtained from a Random Forest classifier: results on ABIDE I only open eyes.}
\label{fig:rf_feature_importance_ab1}
\end{figure}




Running on the same subgroups as in the analysis of the whole dataset:
We have 4 features common to the 5 analysis: 1400, 762, 748, 2735
\begin{itemize}
\item Feature 1400: correlation between Left Supramarginal Gyrus ( posterior division) and Right Inferior Frontal Gyrus ( pars triangularis)
\item Feature 762: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Left Middle Frontal Gyrus (nan)
\item Feature 748: correlation between Left Middle Temporal Gyrus ( temporooccipital part) and Right Thalamus (nan)
\item Feature 2735: correlation between Right Precuneous Cortex (nan) and Right Middle Temporal Gyrus ( anterior division)
\end{itemize}


\begin{itemize}
\item Focusing on random forest classifier, we compared important features between \ref{proc:rf_kfold} and \ref{proc:rf_no} and found that 34 features out of 60 are common (57\%).
\item Focusing on DNN we compared pipelines \ref{proc:dnn_kfold} and \ref{proc:dnn_no} finding that 19 features (32 \%) out of 60 are common.
\item Comparing the same analysis pipeline with DNN and Random Forest we find that with harmonized data: procedure \ref{proc:rf_kfold} and \ref{proc:dnn_kfold} there are 11 common features (18 \%)
\item Comparing results on raw data obtained with DNN \ref{proc:dnn_no} and Random Forest \ref{proc:rf_no} we obtain 13 common features  (22\%)
\item Comparing the adversarial results \ref{proc:adv} with the harmonized data with DNN \ref{proc:dnn_kfold} we obtain 19 common features (32\%)
\item Comparing the adversarial results \ref{proc:adv} with DNN classification of raw data \ref{proc:dnn_no} we obtain 23 common features (38\%)
\end{itemize}






\bibliographystyle{plain}
\bibliography{library.bib}



\end{document}
