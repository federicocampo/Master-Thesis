\documentclass[a4paper,11pt]{article}
\usepackage[vmargin=1.5cm, hmargin=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{dirtytalk}
\usepackage{mathtools}
\usepackage{hyperref}

\usepackage{booktabs}
\usepackage{makecell}

% Packager for Spread-LaTeX
\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{soul}% for underlines
%\usepackage[table]{xcolor} % for cell colors
\usepackage{changepage,threeparttable} % for wide tables



\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%\newcommand{\virgolette}[1]{``#1''}

\title{Titolo Tesi}
\author{Federico}

\graphicspath{{/home/federico/magistrale/scriptsoutput/images/}{/home/federico/magistrale/Imagess/}}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction: fMRI, Autism, ML}
Autism spectrum disorder (ASD) is becoming a growing social issue, drawing more and more attention expecially during the last decades, because of the raise in number of diagnosied cases.
ASD is a neurodevelopmental disorder and refers to a broad range of conditions manifesting as deficits in social communication and interaction, repetitive behaviours, even speech difficulties.
Early signs start appearing by the age of 2 or 3, but since there isn't a definitive medical test to diagnosite it, often it is discovered even after adolescence. An early diagnosis then, would provide the help and support they need, and, treating it at its early stage would hopefully lead to the regression of this condition.
The main factors that bring to the developing of autism aren't very clear so far, and is becoming clearer that there isn't just a cause, but a concatenation and cohexistence of various of them, the most strongly suspected causes are genetic and environmental causes.
Genetic is assiciated with a rare gene's mutation, such as a deletion, duplication or inversion, and even though most of the mutation that increase the risk of developing autism are not been traced, it has been assessed that it has a percentage of inheritability around 90\%,
Ambiental factors such as prenatal air pollution or certain pesticides are also suspected to be associeted with a increase of the risk to develop this disorder, even thouh the etiology is way more complex and it can't be assessed so accurately.
Currently ASD is diagnosided by syntoms-based tests concerning behaviour assessment,
From a neurological point of view, ASD, manifests in a reduced information processing owing to synaptic dysfunction that manifest in a reduced or altered brain activity.
This is confiermed by several but controversal studies where both hyper functional connectivity and hypo-FC were detected in asd patients, in particularly this is age-dependent, over connectivity is usually associated to young children while under connectivity, with adolescences, and adults

In the last decades, different approaches to study this problem have been carried on, trying different diagnostic tool to investigate this matter, such as magnetic resonance imaging (MRI), functional-MRI (f-MRI), or electroencephalography (EEG).
Among them, f-MRI is what is drawing more interest, and in particular resting state f-MRI was proven to be suitable for examining functional connectivity among brain regions that different highlighted to be more affected by autism rather than local brain regions.

f-MRI is a diagnostic tool that investigate blood flow variations across the whole brain structure.
The measure the blood flow is directly linked with the neuronal activity, an increase of neuronal activity leads to a boost in blood flow in that specific brain area, because of the bigger demand of oxigen and other nutrients to that brain area.
Blood vessels also increase their size
The different neural connections are identified by measuring the BOLD fluctuations, (acronym for blood-oxigen-level dependent) from different areas of the brain, using the signal's difference between oxy (diamagnetic) and deoxyhemoglobin (paramagnetic).
Oxyhemoglobin appears then brighter than deoxyhemoglobin
The spontaneous fluctuations in the BOLD fluctuations during resting state are considered a strong indicator for the assessment of the properties of the brain system.
During the study of a functional connectivity pattern, different traits are involved and characterise the signal in a strong and evident way, among these: sex and age of the patient are some of the most important both for structural and functional brain properties.
Limiting our focus just on functional data, other factor conditioning the final outcome of a f-MRI data acquisition are the FIQ (Full intellective quotient) or the eye status at scan, about this, functional connectivity with open steady eyes is different from closed eyes, and furthermore, closed eyes patient may be fallen asleep during the scan acquisition and this would heavily marking and modify the functional patterns.

It appears clear that the distinction of asd patient among normal ones it's not a simple and straightforwand task.
One of the most promising and powerful tool to tacke non-linear problems is machine learning with artificial neural network.
ANN belong to artificial inteligence tools and make use of an alghorithm that learns from data, and modify its parameters with the aim of recognise distinctive properties from the data and make prediction or decision on new unseen data.
An relevant aspect that makes machine learning and artificial neural networks so popular is the ability to deal with non-linear problems introducing several non linear non-linear functions during training acting on their inputs or on the sum of them.
A restraint of machine learning though, is that to perform well, an ANN needs to be trained on a big dataset, and the bigger the dataset, the better its performances are, expecially for deep neural networks.
To this end, particularly in medical field, several data from different acquisition centers need to be put together, and in the last years, several multicenter medical dataset such as the Human Connectome Project, the Alzheimer’s Disease Neuroimaging Initiative (ADNI) or Autism Brain Imaging Data Exchange (ABIDE) were created.
Unfortunately, one drawback of multicenter datasets is the unavoidable bias towards the site the data belongs to, resulting from hardware and scan's procedure differences, and it wouldn't be possible to uniform them to a single common acquisition protocol.
To tacke this problem and try to uniform inter-scan variability, in this work, two different aproaches are proposed: analytical harmonization and a deep learning aproach.
Harmonization is a procedure that aims to remove inter-site related effects in multi-site data, while trying to preserv all other information as biological-related features.
The second aproach, the deep learning one, tries to extract from data both disease and site information and use site information to remove the bias toward that site before making a prediction.

\section{Dataset: ABIDE I $\&$ II}

% Patient division,  patient distribution: M/F x site and C/ASD x site, distribution age, FIQ.
% structural and functional images, ¿ acquisition parameters?
% some patients are the same across ABIDE I and II

Data we are working with belong to the ABIDE dataset (Autism Brain Images Data Exchange): a project aiming to investigate autism using resting state fMRI scans acquired from different medical centers, and putting them together to collect as many data as possible.

The whole ABIDE dataset was published in two releases: ABIDE I released in August 2012 and containing 1112 patient scans, and ABIDE II released in June 2016 containing 1114 scans.
ABIDE I includes scans collected from 17 different sites, and the 1112 patients consist of 539 patients with ASD and 573 typical control patients.
ABIDE II includes scans collected from 19 different sites, and the 1114 patients consist of 593 patients with ASD and 521 typical control patients.
Not every site belonging to ABIDE II is different from those of ABIDE I, but, even though some medical centers are the same, the scanner type, or acquisition pipeline and parameters may have been changed during the time interval between the two releases, so in the following analysys, they are regarded as different acquisition sites.

Besides scan images, ABIDE provides every information related to each patient, as age, sex, intelligence quotient (FIQ), eye status during the scan (open or closed), and every additional information provided by patients.

The vast majority of patients are males as shown in the histogram \ref{fig:mf_site}, for a total amount of 1804 males and 422 females , while control/asd patient number, for each site is almost  for each site as
while the control/asd case per site is more or less balanced for almost every site, with the exception of KKI-1 that provided two times more controls than asd patients, and KUL-3 and NYU-2 that only provided ASD cases. For a visual comparison the number of controls/asd for each site is displayed on the histogram in figure \ref{fig:controlcase_site}

\begin{figure}[h]
\centering
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/controlcase_site.png}
\caption{}
\label{fig:controlcase_site}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\includegraphics[width=\textwidth]{abide/mf_site.png}
\caption{}
\label{fig:mf_site}
\end{subfigure}
\caption{}
\label{}
\end{figure}


Patients in ABIDE dataset have ages ranging from 4 to $>$ 50 years old, but as shown in figure \ref{fig:abideages} the vast majority of participant are younger than 40, precisely $>$ 97 $\%$ of participant are under 40 y.o. so we can restrict our further analysys to patients belonging to this group.
% HOW IS FIQ ASSESSED?? HOW IS AUTISM DIAGNOSIED (di martino et al.)????????
Intelligence quotient, whose distribution is shown in figure \ref{fig:abidefiq}, was not provided for every participant, in fact 171 out of 2226 values were not provided, and before proceding they were replaced by the mean of all the other values.
The lack of a common acquisition protocol is also evident from the eye status at scan feature: as showed in figure \ref{fig:abideeyesite} each site acquired scans either with open eyes or with closed, without a common method, and sometimes this information is not even specified.
In its entirely, the whole dataset consists of more than 70\% of patient acquired with open eyes, as showed in figure \ref{fig:abideeye}.




\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{abide/abide_ages.png}
\caption{}
\label{fig:abideages}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{abide/abide_fiq.png}
\caption{}
\label{fig:abidefiq}
\end{subfigure}

\caption{}
\label{}
\end{figure}



\begin{figure}
\begin{subfigure}{0.4\linewidth}
\includegraphics[width=\linewidth]{abide/abide_eye.png}
\caption{}
\label{fig:abideeye}
\end{subfigure}
\begin{subfigure}{0.7\linewidth}
\includegraphics[width=\linewidth]{abide/eye_site.png}
\caption{}
\label{fig:abideeyesite}
\end{subfigure}
\hspace{0.1 cm}
\caption{}
\label{}
\end{figure}


For each patient in ABIDE both structural and functional images were provided.







\section{Image preprocessing}
\subsection{General overview}
For a better undertanding of how MRI and f-MRI and signal detection work, we refer to appendix..
(talk briefly about echo time and TR ?)

During an MRI and f-MRI scan session, data are usually acquired slice by slice, and the thinner is the slice, the more spatial resolution we can accomplish.
But there's a trade-off between spatial and temporal resolution: decrease the slice's thickness would lead to a better space resolution but at the cost of increasing repetition time to mantain the same Signal to Noise ratio (SNR).
This is because the signal is proportional to the number of hydrogen nuclei, which is proportional to the slice volume.

In order to obtain a strong BOLD signal, echo time plays a significant role as well: to obtain the maximum strenght signal it has to be set $T_E = T_2^{\ast}$ so in the case of BOLD signal it should be around 30 ms.
Images acquired using a shorter echo time have a weaker BOLD signal because of the lack of signal to detect. \cite{Triantafyllou2011}

Acquired data suffer from different source of noise and artifacts deriving from both hardware and physiologigal sources.
As an instance, breath rate can affect BOLD signal because of the induced local motion of the brain's vessels, or the change in blood oxygenation and pressure

One of the main artifact intrinsit to the signal's nature is distortion and field inhomogeneities, deriving from making the scan sensitive to the BOLD signal which intrinsecally is the detection of a signal loss due to field distortion.
This could be corrected by employing small coils inside the scan to smooth magnetic field differences, this process called shimming still left some artifacts, therefore common data preprocessing pipelines use extra acquisitions to create a field map of the remaining field inhomogeneitie and a shift and stretching voxel to correct for them.

Head motion is due to the physical movement of the patient inside the scanner. It results in a misalinement from one acquired volume to the next.
Physiological noise is caused by the cardiac cycle and the breathing of the patient. They respectively provoke pulsatile motion of arteries, CSF and tissues and small head motion. As a second effect, the variable amount of air in the chest affect the magnetic field $B_0$.
To correct for head motion, motion correction steps are performed at the beginning. It works by spatially applying transformations as rotation or translation volume by volume, aiming to overlap every acquired slice to a chosen reference volume, like the first or that in the middle.

For EPI data, slice timing correction is usually performed as well. It aims to correct artifacts deriving from the sequentiality of acquisition for each slice of the brain is acquired at different time.
The entire time elapsed to acquire all the slices is called repetition-time, and it usually is from 1 to 3 seconds.
Slice timing correction uses interpolation in time to shift the BOLD timeseries of each voxel, in order to align them to a reference starting time.
The use of interpolation, though can lead to a slight loss of high frequencies information.

A further common step is spatial smoothing of both structural and functional data. It operates calculating a weighted average of each pixel over neighbored voxels. To this end, a gaussian kernel with a chosen FWHM is applied to create the weights.
Spatial smoothing is useful to avoid abrupt changes of signal between two neighbouring voxels.

Band-pass temporal filtering is commonly applyed to BOLD data, aiming to reduce artifacts from hardware like the slow changing in the baseline of the BOLD signal over time. A low pass filter removes high frequencies above a cut off frequency, it is commonly applied in processing resting state fmri data because the physiological signal is driven by low frequences oscillation while high ones are associated to noise.

A common further step is nuisance regression, which aims to reduce the structured noise: it works computing timecourses called nuisance regressors,
These signals include motion and BOLD signal fluctuation from white matter or cerebro spinal fluid.
From them, the variance is computed and this value is removed from the data using multi linear regression analysis.

When we need to run a group analysis, meaning analyze and compare different patient's images, one of the most relevant step is \emph{registration}: structural (T1) data are aligned over a standard coordinates system space to universally describe location of the different brain parts, to make sure that the same voxel coordinate corresponds to the same brain area for all the subjects.
The most common spaces is Tailarch and Monreal Neurological Institute's MNI-152, obtained from linearly coregistrating 152 T1 brain scans.
A second registration step occurs within each patient to align EPI (functional) data to the structural image of that subject.

Looking at picture \ref{fig:registrationchart} we can see an example of step by step functional and structural image registration to a MNI152 template: first the functional image is registered to the structural and next, they are both registered to MNI template.

Once the structural and functional images were registered to a standard space, it's possible to extract brain region information using an atlas.
Atlases are in the same space as the template image (MNI space for example) and are brain images where different brain areas are marked with different color intensities to associate each area to a label.
This division into areas and the subsequent labeling, is called parcellization.

One of the most popular atlas is Harvard Oxford atlas obtained by manually labelling 37 MRI scans, aligning them to the standard MNI template and finally averaging each transformed label \cite{chappell-neuroimaging}.


\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{mri/registrationchart.png}
\caption{Registration steps:
 starting from the first row there are the acquired functional and structural images and the MNI template respectively
 The second row shows the functional image registered on the structural space
 The third row shows the same images as above, but overlaid with red boundaries extracted from the structural image
 The fourth row shown the final images both registered to the MNI 152 template
}
\label{fig:registrationchart}
\end{figure}



\subsection{CPAC, preprocessing pipeline, atlases}

ABIDE I data are available in a preprocessed format, the Neuro Bureau Preprocessing Initiative took care of data preprocessing and shared its results making them publicly available.
Four preprocessing software and aproaches were employed each from a different group and every one publicly available \footnote{http://preprocessed-connectomes-project.org/abide/cpac.html} for download.
They consist of data from ABIDE I preprocessed by using
\begin{itemize}
\item Connectome Computation System (CCS)
\item Configurable Pipeline for the Analysis of Connectomes (CPAC)
\item Data Preprocess Assistant for Resting-State f-MRI (DPARSF)
\item Neuroimaging Analysis Kit (NIAK)
\end{itemize}
The preprocessing steps implemented by the different softwares are similar, they differ on their foundational software (Python, MATLAB..) the algorithm implementation and their parameters and preprossessing steps' order.

But since only ABIDE I dataset is avaible preprocessed, we need to repeat the preprocessing procedures in order to obtain a dataset including both ABIDE I and II preprocessed with the same pipeline

In our work, we choose to preprocess data using CPAC (Configurable Pipeline for the Analysis of Connectomes): a configurable, open source pipeline, based on Nipype platform.
CPAC was run on a Docker container installed on a computer and our hardware and software setup consisted on:
\begin{itemize}
\item 16-core Intel i7-5960X processor and 64 Gb RAM
\item Ubuntu 20.04 operating System
\item CPAC 1.8.1 installed on Docker v. ???
\end{itemize}

We were allowed to run 3 patients in parallel, reserving 4 cores for each partecipant and up to 12 Gb memory to each patient, necessary to save intermediate-steps outputs.

We choose to employ CPAC because, basing on machine learning classification results, \cite{Yang2020} CPAC prove to be the most efficient preprocessing pipeline to preprocess ABIDE dataset.
CPAC employs tools like AFNI, FSL and ANTS to perform image correction of structural MRI and rs f-MRI.
Data were preprocessed following the same steps as the pipeline employed to create the ABIDE-preprocessed dataset.
This pipeline includes both antomical and functional preprocessing.
Anatomical pipeline speps consist in:
\begin{itemize}
\item Skull removal using AFNI's 3dSkullStrip
\item Tissue segmentation using FSL-FAST, to separate gray matter, white matter and CSF, uging a thresholding probability map whose threshold's values were set the same as ABIDE-preprocessed pipeline values
\item Registration to a standard template using ANTS, with a spatial resolution of 2mm
\end{itemize}
Functional pipeline consists of the following steps
\begin{itemize}
\item Slice timing correction using AFNI-3dTshift
\item Motion estimate and correction using AFNI-3dvolreg
\item Distortion correction using PhaseDiff and AFNI 3dQWarp
\item Create a brain-only mask of the functional data using AFNI 3DAUTOMASK
\end{itemize}
At the end of functional preprocessing steps, timeseries are extracted from each patient, making use of different atlases such as Automated Anatomical Labeled (AAL), Harvard-Oxford (HO), CC200, CC400, DesikanKilliany.
An atlas is a 3D standard brain template, each one including a different parcelization of the brain, which is obtained divding it into N labeled ROIs (Regions Of Interest) for anatomical and/or functional analysis.
For example the DesikanKlein atlas employed in c-pac consists of 94 ROIs of which 32 cortical regions each side, 3 ROIs belonging to cerebellar vermis and 29 subcortical regions.
According to previous studies, the atlas that gave best classification performances was HO, so this was the atlas we choose to employ for our work.
The Harvard-Oxford atlas consists of a subcortical and a cortical atlas, with a total of 117 ROIs of which 21 subcortical and 96 corticals (48 for each emishpere).
The HO atlas employed by CPAC is provided by FSL library \footnote{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/}, and as includes both subcortical and cortical atlases, even though there's only 111 ROIs, namely 14 subcortical and 97 corticals.
The lacking ROI in the subcortical areas are L/R cerebral white matter, L/R cerebral cortex, L/R lateral ventricle and the brain stem \footnote{Subcortical ROIs https://neurovault.org/images/1700/ \quad Cortical ROIs https://neurovault.org/images/1705/ }.
In this unified HO atlas (subcortical and cortical) the entire cerebral cortex area was removed because it's replaced by the finer parcellization of the cortical areas from the cortical atlas.
The extra cortical ROI in this H-O atlas is present in the 83th position and it's named 3455; but there are no information abut this ROI neither on Harvard-Oxford documentation nor on CPAC's/FSL's documentation, and because it's only made of 2 voxels it was excluded for our further analysis.

\begin{figure}[h]
\centering
\includegraphics[width=.4\textwidth]{mri/hoatlas.png}
\caption{Harvard-Oxford subcortical and cortical atlas}
\label{fig:hoatlas}
\end{figure}


As it's possible to notice from figure \ref{fig:confrontoabidepreproc} timeseries extracted after our preprocessing pipeline do not exactly match those from ABIDE preprocessed, this is most likely due to two main reasons: the differeces in software version, and the lacking of a detailed step-by-step pipeline legend showing the value of all the sub parameters employed during the CPAC analysis pipeline.
Abide preproccessed data were obtained using one of the first version of CPAC; nowadays, after more than 7 years, CPAC and the libraries it relies on were upgraded several times and this may have slightly affected the final outcome of the process.

\begin{figure}[h]
\centering
\includegraphics[width=.4\textwidth]{abide/timeseries51456.png}
\caption{Comparison of 4 timeseries between ABIDE-preprocessed and our timeseries. Data show 4 randomly chosen ROIs: ROI 0, 6, 50, 108, belonging to patient 51456 from ABIDE I.
It appears clear that the trend is the same, but there are some local differencies between the two plots due to differencies in }
\label{fig:confrontoabidepreproc}
\end{figure}













\section{Data: Pearson and Wavelets}
Once all 110 timeseries were extracted from each patient, we need to create a correlation matrix, comparing each timeseries with all the others.
The total number of combination that is possible to obtain from n timeseries is given by
\begin{equation}
N_{comb} = \frac{n\cdot(n-1)}{2}
\end{equation}
So in the case of HO atlas with 110 ROIs we obtain 5995 combination each one expressed by a correlation coefficient.
\subsection{Correlation and Z-Fisher transform}

Pearson correlation often simply called correlation coefficient is the measure of a linear relation lying between two sets of data $x_1$ and $x_2$, it's defined as the covariance of (x1, x2) over the product of the standard deviation of the two sets and has the important properties to be scale-invariant in magnitude.
%standard deviation of samples or of mean?
\begin{equation}
Corr(x, y) = r_{xy}=  \frac{Cov\left( x, y\right)}{\sigma_x \sigma_y} = \frac{E\left[ \left( x - \mu_x \right) \left( y - \mu_y \right)\right]}{\sigma_{x} \sigma_{y}}
\end{equation}
This correlation coefficient can assume values between -1 and +1 where the extremes correspond to exact anti-correlation or correlation respectively, so that, if a linear relation lies between the two sets, a high absolute value indicates that the two series tend to be simultaneously greater or lower than their respective means

Given two series x and y, of length n correlation can be easily computed by
\begin{equation}
r_{xy} = \frac{\sum_{i = 1}^n \left( x_i - \mu_{x_i}\right)\left( y_i - \mu_{y_i}\right)}{\sqrt{\sum_{i = 1}^n \left( x_i -\mu_{x_i}\right)^2}\sqrt{\sum_{i = 1}^n \left( y_i -\mu_{y_i}\right)^2}}
\end{equation}
For each patient the Pearson correlation coefficient was computed for each timeseries' pair. Before further analysis, a common way to proceed is to transform each coefficient with Fisher z-transformation. The reason behind this is obtaining a variable which is normally distribuited even when correlation coefficients are leaning toward the extremes.

\begin{equation}
z = \frac{1}{2}ln\left(\frac{1+r}{1-r}\right) = arctanh(r)
\end{equation}

At the end of this analysis, we obtain correlation matrices like that showed as an example in fig \ref{fig:corrmatrices}, referred to patient 20243 from ABIDE I dataset.
\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{corrmatrix.png}
\caption{}
\label{ref:corrmatrix}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{zscorematrix.png}
\end{subfigure}
\caption{Correlation and z-values matrix computed from timeseries extracted using Harvard-Oxford atlas, from patient 20243 belonging to ABIDE I dataset}
\label{fig:corrmatrices}
\end{figure}



\subsection{Wavelet analysis}
A different aproach to compute a correlation coefficient is by making use of wavelet analysis.
Wavelet is a mathematical tool for analyzing time series or images, and provides a comprehensive way for investigating the bivariate relationship between timeseries both in time (or space) and frequency domain.
%A second type of correlation analysis was performed on timeseries by using wavelet analysis.
To understand wavelet transformation it could be helpful to compare it with Fourier analysis.
Fourier analysis allows us to expand a periodic function f(t) into series, ideally infinite sums of weighted sines and cosine with different frequencies
\begin{equation}
f(t) = a_0 + \sum_{k = 1}^\infty (a_k cos(2\pi k t/T) + b_k sin (2\pi k t /T))
\end{equation}
terms corresponding to the k-th frequency are called harmonics and are multiples of the fundamental frequency with k = 1.

The Fourier Transform is a natural extension of the Fourier series to aperiodic functions defined over the real axis, which doesn't allow a discrete superposition of sines and cosines terms, but it needs to be represented by a continue superposition.
The FT transform takes a function from time or space domain and turn it into spatial or temporal frequency domain.
It's a complex function because sines and cosines terms can be represented by a complex exponential.
\begin{equation}
\tilde{F}(\omega) = \int\limits_{-\infty}^{+\infty} f(t)e^{-i \omega t} dx \longrightarrow F(k) = \sum_{n = 0}^{N-1}x_n e^{\frac{-2\pi i k n}{N}}
\end{equation}
To deal with discrete signal, for example a sequence $x_n$ obtained from a discrete sampling of the continuous signal x(t), it's possible to make use of the Discrete Time Fourier Transform, wich allows us to write the $\tilde F(\omega) = $

On the other hand, the Discrete Fourier Transform, is useful to analyze discrete periodic signal, it acts on a function defined on a finite domain returning a sequence of samples of the discrete fourier transform sampled at an interval equals to the reciprocal of the duration of the input sequence.

\begin{equation}
\tilde{x_k} = \sum_{n=0}^{N-1} x_n \cdot e^{i 2\pi kn/N}
\end{equation}
where N in the total length of the timeseries an corresponds to the period of $x_n$ as well; the frequencies at which samplings are computed are $\omega_k = 2\pi k/N$

One of the main drawback of FT though, is the lacking of spatial information, for example for a non stationary signal, it's possible that the signal contains a variable frequency component.

A time-frequency analysis provides for this lacking computing both frequency and spatial information from a signal, allowing us to analyse non stationary signals and obtain informations both on time (or space) and frequency domain which provides information about frequencies and their spatial location by convolving the signal x(t) with a function w(t) called windowing function usually centered at zero which rapidly decays symmetrically.
\begin{equation}
\tilde{x}(F, T) = \int_{-\infty}^{\infty} x(t)w(t-T) e^{-2\pi i F t} dt
\end{equation}
The most used functions w(t) are
\begin{enumerate}
\item Windowed Fourier Transform: $\psi_{ab}(x) = e^{ix/a}\phi(x-b)$ where $\phi(x)$ is a window function
\item wavelet transform: $\psi_{a, b}\left(t\right) = \frac{1}{\sqrt{a}}\left(\frac{t-b}{a}\right)$ where a is a positive real number and defines the scale, while b is any real number and defines the shift parameters: it specifies the location of the wavelet in space or time.
\end{enumerate}



A wavelet, literally \textquotedblleft small wave \textquotedblright is a wave function limited both in space and period: begins at zero, grows and decrease in a limited time period and returns to zero. So that it's a function local in both the temporal and frequency domain.

To be defined as so, a wavelet is required to satisfy two properties: to have zero mean (it must be oscillating) and unitary squared norm \cite{percival2013}.

\begin{equation}\label{eq:waveletproperties}
\int_{-\infty}^{\infty}\psi(x)dx = 0 \qquad \qquad  \int_{-\infty}^{\infty} |\psi(x)|^2 dx = 1
\end{equation}

It is defined over the space $L^2$ of Lebesgue measurable functions that are both absolutely integrable and square integrable. In this space the two properties can be satisfied. A wavelet transform is then formally a mapping from $L^2(\Re) \rightarrow L^2(\Re^2)$.
%This kind of transformation of going in a higher dimensional space is a redundant operation


There are different wavelets that satisfy properties \ref{eq:waveletproperties}, such as Morlet, Meyer or Mexican hat.
%Figures?
One of the most convenient for our analysis is the Morlet wavelet given by eq \ref{eq:morletwavelet}
\begin{equation}\label{eq:morletwavelet}
\psi\left(t\right) = \pi^{-1/4}e^{iw_0 t}e^{-t^2/2}
\end{equation}

\includegraphics[width=.4\linewidth]{wavelet/morletwavelet}


Two main type of analysis can be performed using wavelets: continuous wavelet transform (CWT) and discrete wavelet transform (DWT) and the CWT is what we employed for our analysis.

Continuous wavelet transform decomposes a time series in time-frequency domain by successively convolving the timeseries with several scaled and translated version of the mother wavelets $\psi_{a, b}\left(t\right) = \frac{1}{\sqrt{a}}\left(\frac{t-b}{a}\right)$.
If the timeseries is described by a function f, assumed to be real, in the equation below, the convolution of f and $\psi_{a, b}\left(t\right)$ is

\begin{equation}
W_{a,b}(f) = \int_{-\infty}^{+\infty}  f(t) \cdot \psi_{a, b}^\ast \left(t\right)   dt
\end{equation}

It can be useful to visualize this as: given a and b, a wavelet is slided across the signal and for each time step a coefficient is extracted by multiplying the wavelet and the signal, and this operation is repeated for different values of a and b.

To computationally implement CWT, a discretized version was implemented on MATLAB tools Wavelet toolbox, but it does not be confused with the Discrete wavelet transform \footnote{https://it.mathworks.com/help/wavelet/gs/continuous-and-discrete-wavelet-transforms.html}

The difference between the continuous wavelet transform implemented on MATLAB and discrete wavelet transform lies in how finely stretching and shifting parameters are sampled: the CWT discretizes scale more finely than the discrete wavelet transform.

In the CWT, parameters are discretised based on a fractional power of two, by setting a = $2^{j/\nu}$ \cite{liu-wavelet} \cite{tenlectuerswavelets} where $\nu$, j are integers. $\nu$ is often referred to as the number of “voices per octave” because increasing the scale by an octave (namely to double the frequency) requires $\nu$ intermediate steps, for example from $f = f_0 2^{\nu/\nu} \ to \ f = f_0 2^{2\nu/\nu}$ , $\nu$ steps are required.
The larger the value of v, the finer the discretization of the scale parameter.
In the DWT, the scale parameter is always discretized to integer powers of 2, $2^j$, j=1,2,3,..., so that the number of voices per octave is always 1


Using CWT it's possible to obtain singal's information on a matrix: a time-frequency plot also called scalogram by convolving the signal, using different wavelets parameters a and b from a grid.

One problem that arise from working on a timeseries which is a signal with a finite support, is that when a wavelet is located at the beginning of at the end of a signal it extends itself outside the boundary of our data, and the convolution requires nonexistent values beyond the boundary. The confidence value obtained is then less accurate than other obtained for central values of time/space location.
To overcome this problem it would be possible to accept this data information loss and truncate the values beyond boundaries; while an other aproach could be artificially extend data using methods such as zero padding which assumes that the signal is zero outside its original support, or symmetrization which extend the signal symmetrically outside the boundaries or smooth padding which recovers a signal by extrapolating values from the first derivative values or from the signal itself.
Symmetrization method is the one employed for the subsequent analysis.
Areas of the scalogram affected by these edge effects are indicated as outside the Cone of influence (COI).

Figure shows the timeserie belonging to ROI 308: left superior frontal gyrus of patient 51056 and its corresponding continuous wavelet transform.

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{timeseries.png}
\caption{}
\label{}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{cwt_timeseries.png}
\end{subfigure}
\caption{}
\label{}
\end{figure}

In cwt figure, x axis corresponds to space or time points and on the y axis frequencies or periods from "a' parameter are showed.
Values outside the COI (showed as a dotted white line) are shown with a lighter faded.


%The transform is in the form:
% \begin{equation}
% \frac{1}{\sqrt{a}}
% \end{equation}


Continuous wavelet transform can only be used to analyze one signal at a time; to analize and compare two signals an analysis called wavelet coherence has to be carried on.
From two CWTs it's possible to compute the Cross Wavelet Transform (XWT) which allow us to examine their cross-wavelet power and relative phases.

Using XWT is then possible to compute the Wavelet Coherence.

Denoting as $W^{X}(a, b)$ the continuous wavelet transform of a signal X, it's defined the \emph{wavelet power spectrum} of a signal X(n) as $W^{XX}(a, b) = W^X(a, b) \left(W^X(a, b)\right)^{\ast}$ where the $^{\ast}$ represent the conjugate transpose.
Similarly, the cross-wavelet spectrum of two time series X(n) and Y(n) is defined as $W^{XY}(a, b) = W^X(a, b)W^Y(a, b)$ whose module $|W^{XY}(a, b)|$ represents the amount of joint power between the two time series, and it's called \emph{cross wavelet power}
-
From it it's possible to compute the the complex argument $\Delta \phi (a, b) = arctan\left(\frac{Im\left[ W^{XY}(a, b)\right]}{Re\left[ W^{XY}(a, b)\right]} \right)$ which represents the phase between X(n) and Y(n).

A squared cross-wavelet coherence $R^2(n, s)$ is defined as reported in equation \ref{eq:WC}. This coefficient ranges in the interval (0, 1) and represents the localized correlation coefficient between x and y in the time-frequency domain.
\begin{equation}
R^2(a, b) = \frac{|S(W^{XY}(a,b))|^2}{S(|W(a,b)^X|^2) S(|W(a,b)^Y|^2)}
\label{eq:WC}
\end{equation}

To assess the level of significance over the noise, the statistical significance level of wavelet coherence has to be estimated using Monte Carlo methods generating a large (1000) samples of red noise for each time series, with the same 1-lag autoregressive coefficients (AR1 coefficient). For each pair of noise, a wavelet coherence matrix is calculated and stacked to obtain a $n\times m \times x \times 1000$ 3D noise matrix to extract the distribution of the entries. This null distribution of wavelet coherence coefficient is used to estimate the threshold to 5\% significance level for the subsequent analysis.

To obtain a correlation-like coefficient from every two timeseries we are going to analyze, we use wavelet coherence coefficients from the matrix and their phase information to compute the time of in-phase coherence which can be seen as the percentage of time synchronicity between the two ROIs, defined as

\begin{equation} \label{eq:wcohcoefficients}
c_{ij} = \frac{100}{N}\sum_{a, b}^N I\left\{ R_{ij}^2(a,b) > a_{95}\right\}\cdot I\left\{-\frac{\pi}{4}<arg_{ij}(W^{XY}(a, b)) < \frac{\pi}{4}  \right\}
\end{equation}

Where N is the total number of points inside the cone of influence (COI) $I\left\{ ...\right\}$ is either 0 or 1 depending on if the condition inside is satisfied; $a_{95}$ is the threshold value above which the computer wc coefficient is regarded as significative.

Similarily to this coefficient is the time of counter-phase coherence modifying from the formula above the phase condition into $I\left\{arg_{ij}(W^{XY}(a, b)) < -\frac{3\pi}{4} \ \lor \ arg_{ij}(W^{XY}(a, b)) >\frac{3\pi}{4}  \right\}$

In short with this analysis, we are just considering coefficients with an high significance level (above 95\%) and with a small, or big phase shift (between $-\frac{\pi}{4} and \frac{\pi}{4}$ and the opposite).

Wavelet coherence maps were calculated using MATLAB's Wavelet Toolbox, which employs the Morlet wavelet, and analyze the frequency range using 12 subscales per octave and 9 octave.

Figure \ref{fig:wcoherence} represents the wavelet coherence scalogram obtained from two ROIs of patient 51056.



\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{timeseries_for_wcoh.png}
\caption{}
\label{fig:wcoherence1}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{wcoherence.png}
\caption{}
\label{fig:wcoherence2}
\end{subfigure}
\caption{}
\label{fig:wcoherence}
\end{figure}



From equation \ref{eq:wcohcoefficients} both coefficients in-phase and anti-phase matrix coefficients were computed.
The correlation coefficients matrix for each subject, were then flattened and used as input to the neural networks.




\section{Machine learning}
%Machine learning introduction
Machine learning is a branch of Artificial Intelligence (AI) that aims to learn from dataand gradually improve its accuracy.
Algorithms are trained to make classifications or predictions and the bigger is the dataset to train the algorithm, the better accuracy and generalization can be achieved.

Machine leraning algorithms can be divided into two macro areas: supervised and unsupervised learning.
From a dataset, containing features, an unsupervised algorithm's goal is trying to learn the principal properties from the dataset's structure attempting to learn the probability distribution of all the features in it.
On the other hand, in supervised learning each input data is associated with a label, specifying the class that data belongs to, so the algorithm is to learn the common features for each class trying to predict the label given the corresponding input.

A sub-branch of Machine Learning is Deep Learning, the main difference lies on the algorithms' structure and the learning techniques, as an instance, traditional machine learning methods consist on algorithms like Random Forest, Support Vector Machines (SVM), K-nearest neighbor (KNN) and several more models, while deep learning models include Artificial neural networks (ANN), Convolutional neural networks (CNN) and more.

\subsection{Random forest}
Random Forest classifier belongs to the category of \emph{ensemble} classifiers, it combines multiple decision tree models to create a more poverful model, so to understand a random forest we need to start understanding how a Decision Tree classifier works.

A decision tree aims to learn distictive traits from input data by asking yes/no questions. Focusing on the valus of a single input feature, the classifier splits the dataset based on the value of that feature according to an if/else statement like \textquotedblleft if feature-i > a \textquotedblright.
Considering different features, the process is then recursively repeated untill a single data point (and then a single class) is left on a so called \textquotedblleft leaf \textquotedblright.
\begin{enumerate}
\item Randomly select n features
\item Split the node using the feature that provides the best split
\end{enumerate}
An example of a decision tree dealing with flower classification is showed in figure \ref{fig:decisiontree}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{ml/decisiontree_example.png}
\caption{}
\label{fig:decisiontree}
\end{figure}


Usually though, continue this iterative process untill it reaches a leaf, brings to the creation of a very complex model that overfits the training data, that is, the model accurately learned the training dataset but is not able to generalize well to a test dataset.
To prevent overfitting, one possible strategy is to early stop the iteration towards the leaf, and leave a node with more than one class.
An other strategy, which leads to the construction of a Random Forest, is the creation of a stronger model, more prone to generalize data without overfitting.
A Random Forest is a collection of decision trees where each tree is slightly different from the others, each tree tends to overfit, but it does so in a in a different way, so that we can reduce overfitting by averaging different results.
So the necessary steps to build a random forest are

\begin{enumerate}
\item Randomly select n samples from the input dataset
\item Grow a decision tree from this sample
\item Putting all the predictions together it's possible to assign a class label by majority vote.
\end{enumerate}

This steps are implemented in the Python's package Scikit-Learn



\subsection{Deep Learning: ANN}
Deep learning is a subset of machine learning whose algorithm try to emulate the learning process of the human brain.
This kind of algorithms are called Artificial Neural Networks (ANN) and, such as a neuronal structure, they are made of several neurons organized into layers; each layer containsa certain amount of neurons each one linked to each others.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\includegraphics[width=1\linewidth]{ml/perceptron_example}
\caption{Perceptron}
\label{fig:perceptron}
\end{wrapfigure}

The simplest model of a neural network is the \emph{perceptron} showed in figure , corresponging to a single neuron which takes as input a data, for example an n-dimensional array $\textbf{x} = \left\{ x_1, x_2, ..., x_n\right\}$ and returns an output: a real number computed by applying a function to a linear combination of all the inputs $ y = f(z) = f \left ( \sum_i w_{i} x_i + b \right )$
This function that determines the outpu is called \emph{activation function}.



\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\includegraphics[width=1\linewidth]{ml/ann_example}
\caption{Artificial Neural Network}
\label{fig:ann}
\end{wrapfigure}


A deep neural network is a hyerarchical organization of neurons into layer, connected to each other.
Input data are passed to the first input layer where each neutron acting like a perceptron, produces an output using the activation function which is the same for every neurons of the layer, but can differ from the activation function of other layers.
Once collected each and every output from all the first layer neutrons, they are passed to the second layer and become the input to each neutron belonging to this layer.
This is reiterated through all the layers up to the final output layer.
As schematized in figure \ref{fig:ann} an artificial neural network is mainly composed of three parts: an input layer, come middle layers also  called \emph{hidden layer} and a final layer. Each neuron of a layer is linked to all the neurons of the previous and next layer.
The connection between two neurons is weighted, according to an initial setup of the network during which weights are randomly set.

When reach the end of this process, we obtain the output: a numerical value related to every single input data $\textbf{x}$.

Using matrix formalism the entire input-output process can be written as

$\textbf{y} = \sum_j^n w_{ij} x_j + b_i $ where $\textbf{x} \in \Re^n \ \textbf{y} \in \Re^m $ being m the output dimension given by the number of neutrons in the output layer.


During the training process, given an input, the algorithm calculate an output (prediction) and this process is referred as  \emph{forward propagation}.
This prediction is then compared with the actual value this output should be, and during the \emph{backpropagation} the algorithm modifies its weights in order to minimize the difference between the actual and the predicted value.

The goal of a machine/deep learning algorithm is to learn from data using a train dataset, and generalize it in order to perform well on an unseen dataset called Test dataset.
Performing well means to produce a low error on the Test dataset after minimizing the error on train dataset.


To fully define a neural network we just need some parameters called hyperparameters, the principals of which are:

\begin{itemize}
\item Number of layers
\item Number of neutron for each layer
\item Activation function for each layer
\item Number of epochs (or iterations)
\item Learning rate
\end{itemize}




One of the most important issue in machine learning is the concept of underfitting and overfitting.

Underfitting occurs when the model isn't able to learn the required amount of informations during train; this usually happens when the model has a small number of parameters in regard to the number needed to explain the input features.

On the contrary, overfitting occurs when the model has too many parameters compared to those required to learn the input features. As a result, the model performs well on the train dataset and has low performances in the Test one.

As an example in a two-dimensional space, if we aim to fit a dataset consisting of some points sampled from a quadratic curve, using three different functions: a linear function, a quadratic function and a polynomial function with grade equal to the number of data points.
As showed in figure \ref{fig:underoverfitting} the linear model is not able to describe the data's distribution while the polynomial function has too many parameters, and it is perfectly able to fit our distribution, but it would perform very poorly to describe a dataset sampled from the same quadratic distribution, furthermore, if the number of parameters (the grade of the polynomial in this case) is greater or equal to the number of data points, we obtain that an infinite number of different curves are suitable to fit our data, and finding the best one, which performs well on the test dataset is an hard task.
We should therefore pay attention when choosing the number of parameters on a model to avoid under- or more likely over- fitting.


\begin{figure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{ml/underoverfitting}
\caption{}
\label{}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=\textwidth]{ml/underoverfitting_grafico}
\caption{}
\label{}
\end{subfigure}
\caption{}
\label{fig:underoverfitting}
\end{figure}



To monitor the evolution of the model's performances during train is a common practise to split the train dataset into two subsets: one that will be the actual train dataset, and a smaller one, called validation dataset used to make constant checkups on how well the model is learning with the train set, by making constant test of its performances.

The validation dataset is used for the fine tuning of hyperparameters and it typically consists of 10-30\% of the whole train dataset.
When assessed the best hyperparameters combination for our model, it's possible to actually train the model using the entire Train dataset.
Even if we manage to use the entire Train dataset for the training process,
However, when dealing with small datasets, dividing it into Train and Test can be a non trivial issue because of the shortage of data for a proper train procedure.

To work this out, a prodecure called k-fold cross validation can be implemented, at the cost of increasing computational costs.
It consists in the creation of k different partition of the main whole dataset (before splitting it into train + test).
From these partitions, k-1 are used as train, and the last is used as Test. Moving forward every combination of partitions is used so that each different partition is used as test, and the others as train.
Then for each run k-th there are two subsets (a train and a test) from the original dataset. This way, for each iteration, the Train dataset is different, and the model is tested on a different dataset each time.
In practise what is usually done, is not a sequential partitioning the dataset, to avoid creating folder containing all the same label if the dataset was ordered, but randomically picking data in order to create subsets containing the same proportion between classes as the main dataset.


\begin{figure}
\includegraphics[width=0.4\linewidth]{ml/kfold_example}
\caption{Schematic representation of a k-fold cross validation procedure}
\label{fig:kfold}
\end{figure}


\subsection{Activation functions}
The activation function of a neuron, and consequently of a layer defines the output of each neuron belonging to that layer, there are different activation function, linear or non-linear.
A linear activation function outputs a value $f(x) = w^T x + b$ where $w^T$ indicates the transpose of the weigths vector.
In practise, however, it would not be very useful to introduce a linear activation function since we aim to introduce non-linearity in our model to tackle more complex models.
There are several non-linear function available and, depending on what data we are working on, or on what kind of classification task we are performing, some of the most popular are


\begin{figure}[h!]
\centering
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ml/relu}
\caption{ReLU}
\label{}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ml/sigmoid}
\caption{Sigmoid}
\label{}
\end{subfigure}
\caption{}
\label{}
\end{figure}




\begin{itemize}

\item The ReLU functions $\phi(z) = max(0, z) = \begin{cases} 0 \ for \ z<0 \\ z \ for \ z>0 \end{cases}$ which returns the maximum value between the input and zero, it essentially puts to zero all negative inputs and leaves the positives untouched.
A modified version of the ReLU function called Leaky ReLU was introduced defined as
$\phi_{leaky}(z) = \begin{cases} \alpha z \ for \ z<0 \\ z \ for \ z>0 \end{cases}$ where $\alpha$ is a coefficient usually $< 1$, typically of the order of $10^{-2}$


\item The sigmoid function, or logistic function is defined as $f(x) = \frac{1}{1+e^{-x}}$ and outputs a real number between 0 and 1. For this reason is a common choice when we have to predict a probability, for example in a binary classification problem it can be interpreted as the probability that output belongs to the class 0 or 1.


\item The softmax function for multiclass classification, gives the probability of the input belonging to each (mutually exclusive) class. The softmax can't be applied independently to each output $z_i$, since it depends on all elements of $\textbf{z}$ The probability of belonging to the i-th class over a total of M classes is
$p(i = i|z) = \phi(z) = \frac{e^z_i}{\sum_{j = 1}^M e^{z_j}}$ being M the total number of classes
\end{itemize}


\subsection{loss functions}
In order to train a network and improve its performances we need to compare the predicted (output) value with the actual value and compute some sort or error, or distance between these two values, and try to gradually minimize it.
The function to minimize is denoted as the \emph{loss function}
Computing the loss, returns a value of how much the prediction is far from the actual label, and then, we try to modify the network's weights to reduce this value searching for a minimum of this function.

This algorithm employed by a feedforward network for weights' fine tuning is called \emph{backpropagation} (backward propagation of errors).
and the main technique to do so, is the likelihooh maximization

The most common loss function is the cross-entropy loss, it relies on the concept of cross-entropy between two distributions $\hat y \ and y$ and is defined as
$H(y, \hat y) = -\sum_i y_i \cdot log(\hat y_i)$
Where the sum is intended over all the possible values a variable y can assume.
For a classification problem it correspond to the different class our data belong. In case of multi-class classification we call this function categorical crossentropy, while if we only have two classes, (we are then performing a binary classification),the loss is (if we explicit the sum for two $i = \left\{0, 1\right\}$)
\begin{equation}
H_p(q) = -\frac{1}{N} \sum_{i= 1}^N y_i log(p(y_i)) + (1-y_i)log(1-p(y_i))
\end{equation}

The estimation of the minimum of this function can be seen as the maximization of the likelihood:
Given a probabilistic model of m different parameters $\theta_1 ... \theta_m$, the likelihood is the probability of observing a value d given a set of parameters $\left\{\theta_i \right\} \ L(\theta_1, ..., \theta_m | d) = P(d|\theta_1, ...\theta_m)$.
For a subset of observed value we can write the likelihood as $L = P(x_1, ..., x_n | \theta_1, ...\theta_m)$ that in case we are dealing with independent variables is just the product of the single probabilities $L =\prod_{i=1}^n p(x_i|\theta_1, ...\theta_m)$.


%Here I'm calling x_i ... the labels, so maybe dovrò cambiarli in y_i quando rendo tutto consistente e y_i è la probabilità di ottenere...
Since we are interested in maximize the likelihood, the best parameters are those that maximize the likelihood:
$\begin{cases} \frac{\delta L}{\delta \theta_1}(\theta_1, ..., \theta_m) = 0 \\ ... \\ \frac{\delta L}{\delta \theta_1}(\theta_1, ..., \theta_m) = 0 \end{cases}$
To compute the derivative of a product is a nontrivial task, hence it's much simpler to compute the logarithm (since the logarithm is a monotonic function, so compute the maximum of a function is the same as computing the maximum of the logarithm of that function).

We obtain then $log(L) = \sum_{i=1}^m log(p_i)$.
For a binary classification problem, where we only have two labels 0 and 1, the probability of pbserving our data under our model is given by $f(x_i)^y_i (1-f(x_i)^{i-y_i})$




\subsection{Backpropagation}

To accomplish this, even if it'd be theoretically possible to find a minimum by using an analythical way, in practise, in all the neural network applocation, the number of weigths is so big that a numerical methods must be employed, the most popular of which is the \emph{gradient descent}.
Given a generic function $J(\textbf{w})$ where $\textbf{w}$ is a vector of weights, the minimum of this functiont corresponding to the vector $w_0$ can be found by following these simple steps:
\begin{enumerate}
\item Choose a random initial guess for \textbf{$w_0$} and start iterating
%\item At iteration i+1 we have a weights vector \textbf{$w_{i+1}$} given by the formula \textbf{$w_{i+1}$} = \textbf{$w_i$} - $\gamma \nabla$ J(\textbf{$w_i$}) where the $\nabla$ indicate the gradient of the cost function with respect to \textbf{w_i} components
\end{enumerate}


After the computation of the loss value, the next step is the process called backpropagation, a process whereby network's weights are updated.
Let's denote the loss function as $J(w, b) = \frac{1}{n}\sum_{i = 1}^n L(y_i, \hat y_i)$. To find rhe right variation to the weights we can use the gradient descent algorithm, so we compute the gradient of the cost function $\nabla J(w)$ so that we can compute the new weight $w = w + \Delta w$ where $\Delta w = - \eta \nabla J(w)$ which expressed in component becomes $\Delta w_j = -\eta \frac{\partial J(w)}{\partial w_j} = -\eta \cdot \sum (\phi(z^{(i)}) - y^{(i)} )x_j^{(i)}$.
A drawback of this gradient descent algorithm is that the weiths are updated after computing a summation, namely, after the whole dataset is been seen, so that optimization moves with big leaps
A common implementation of this gradient descent is an optimized version called stochastic gradient descend, whose difference is that the upgrade of the weigths is sequential, for each training sample, so that the summation is not computed.
Each step of this weght updating is then $\eta (\phi(z^{(i)}) - y^{(i)} )x_j^{(i)})$.
Even for this implementation though, there's a drowback due to the fact that the steps towards the minimum are small and are influenced by all the unavoidable fluctuations in data training (some of them results in a move downward and some upward).
A compromize between these two opposite way of work is mini batch gradient descent, which compute a step only after computed the gradient over a small subset of the training data.
An other important weights optimization alghoritm made from an extension of SGD is the Adam algorithm.
It makes use of two different optimization algorithms: AdaGrad and RMSProp. It adapts the learning rate based on the first moment (the average) of the recent gradient magnitudes, and of the second moments (the uncentered variance) as well.

\subsection{How to assess network's performances}
Once the model has been trained, it is finally applied to the test set to assess the performances of the model to an previously unseen dataset.
A comomon metric for model performances' evaluation is Accurcay: it is simply defined as the ratio between the total number of correct prediction and the total number of predictions (total number of the test's data), so, using T and F for true and false, and P and N for positive and negative, accuracy is so defined
\begin{equation}
Accuracy = \frac{TP + TN}{TP+TN+FP+FN}
\end{equation}
Even though accuracy may seem a good parameter, it isn't the most accurate one, when we're dealing with a unbalanced test dataset, if for example, in a binary classification with A and B classes, if just 2 out of 10 samples belongs to a different class, let's say A and B, and the model predicts every data belonging to the class B, we will get a score of 80 \% accuracy, which is not a thruthful result because the model isn't making any distinction between the two classes.
There are then other ways for quality assessment that overcome this problem. One of this is based on the introduction of two quantities: precision and recall.
Precision is the ratio between true positive and total positive cases (TP + FP), which is a measure of how many positive predicted cases were actually positive, while recall measures the percentage of actual positives that were correctly classified, or in other words it represents the true positive rate.
\begin{equation}
Precision = \frac{TP}{TP+FP} \qquad \qquad Recall = \frac{TP}{TP+FN}
\end{equation}
Unfortunatelly precision and recall are linked in a sense that often enhancing one decreases the other and vice-versa.
%esempio page 283
There's therefore a trade off between recall and precision. If we want for instance a model to score more true positive, risking to have more false positives, we can set a threshold.
To reach less the x\% positive missing means set the recall to (100-x)\%, this operation is called setting the operating point and it's done by setting a threshold.
This threshold though is not always estblished at the beginning, and operating point that would be the best suited for our classification goals is not clear a priori.
One way to summarize the information a precision-recall curve can provide us, is to compute the area under the curve, known as \textquotedblleft average precition \textquotedblright
What is done is study the model under all the possible thresolds.
This is achieved by studying the precision-recall curve of which is reported an example in figure \ref{fig:precisionrecall}.
The closer a curve lies on the upper right corner (high precision and high recall), the more correctly the model is working.

Besides the precision-recall curve, a similar curve is usually employed to study different thresolds' effect, it's called the receiver operating characteristics curve, usually referred as ROC curve.
% Insert ROC curve computed using the random forest already implemented in my data.
The ROC curve is build from the true positive rate (recall) and the false positive rate (FPR) and shows the evolution of TPR vs FPR.
The ideal curve would be close to the top left (high tpr and low fpr) and the less accurate is the model, the more this curve tend to lay down to the bisector line.

To summarize the model's performances with a single number using ROC's curve information, we compute the Area Under the Curve AUC.
The reference value for an AUC is 0.5 which is obtained when a model is just randomly predicting and it corresponds to a curve laying on the bisector.
The AUC can be explained as the probability that randomly picked point from the positive class, will have an higher score (according to the model) than a randomly picked point from the negative class so that the percentage value of AUC is an estimate of the probability that the model is able to distinguish between the two classes.

An example of ROC curve is showed in figure \ref{fig:roccurve}


\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=1.\textwidth]{ml/precisionrecall_example.png}
\caption{}
\label{fig:precisionrecall}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=1.\textwidth]{ml/roccurve_example.png}
\caption{}
\label{fig:roccurve}
\end{subfigure}
\caption{}
\label{}
\end{figure}




\section{Harmonization}
\subsection{Harmonization - theory}
The issue of collecting a large dataset from multiple sources has a positive side because it facilitate the generalization and the robustness of the model, but, as a drowback, a level out procedure is necessary to avoid the model to learn differences deriving from inomogenity of data because of multiple site differences in data acquisition procedures.
To try to reduce multi-site and multi-scan variety, prodecures called harmonization are become necessary.
The state-of-art procedure is called ComBat (meaning of combat?), specifically adapted for functional connectivity to mitigate scanner effects, is based on a previous method initially proposed [reference to johnson, fortin] in 2007, for gene expression studies to compute batch effect corrections.
ComBat technique is based on the empirical Bayes method: it aims to reduce inter-site variability while preserving biological variability such as differences due to sex, age.. it assumes that the erroes introduced in the imaging features can be standardized by adjusting the mean and the variance across the batches
Let $y_{ijf}$ be the numeric value of the feature f for the patient i acquired with the scan or from the site j, so that i = 1, 2, ..., K indexes the scanner, j = 1, 2, ..., $N_i$ indexes the total number of subjects acquired for each scanner i, and f ranges from 1 to F being F the total number of features. Besides, let's assume that this value y, can be written as
\begin{equation}
y_{ijf} = \alpha_f + x_{ij}^T \beta _f + \gamma_{if} + \delta_{if} \epsilon_{ijf}
\end{equation}
where $\alpha_f$ is the mean value for the feature f, $x_{ij}$ is the entry of the matrix X for the covariates of interest such as age, gender, $\beta_f$ is the vector of regression coefficients corresponding to X, the terms $\gamma_{if}$ and $\delta_{if}$ represent the additive and multiplicative terms for site-i effects related to feature f respectively, and $\epsilon_{ijf}$ is a error term which is assumed to follow a normal distribution with zero mean and variance $\sigma_f^2$. To extimate feature-dependent parameters, data are standardized feature-wise, so that every feature has similar overall mean and variance. A least square regression, is then performed to determine parameters $\hat \alpha_f, \hat \beta_f, \hat \gamma_{if}$ and subsequently, $\hat \sigma^2_f = \frac{1}{N} \sum_ij \left ( y_{ijf} - \hat \alpha_f - X \hat \beta_f - \hat \gamma_if \right )^2$
being N the total number of patients.
The standardized data are then calculated by \ref{eq:harmonization_std_data}

\begin{equation}\label{eq:harmonization_std_data}
Z_{ijf} = \frac{y_{ijg}-\hat \alpha_f - X \hat \beta_f}{\hat \sigma_f}
\end{equation}

Is futhermore assumed that standardized data follow a normal distribution $Z_{ijf} \sim N(\gamma_{if}, \delta^2_{if}) $ and that the distribution
of batch normal distribution follow a normal distribution: $\gamma_{if} \sim N(Y_i, \tau^2_i)$ and an inverse gamma distribution
$\delta^2_{if} \sim \frac{\beta^{\alpha}}{\Gamma (\alpha)}(1/x)^{\alpha +1}\cdot e^{-b/x}$

The hyperparameters $Y_i,  \tau^2_i, $ are then estimated using the moment methos
Under the distribution assumption above, the empirical batched parameters for batch effect $\gamma_{if}, \delta^2_{if}$ are estimaterg leadind to the estimate of the empirical batch adjusted data: $y_{ijf}^{ComBat}$
\begin{equation}
y^{ComBat}_{ijf}
\end{equation}
%HOW it works? covariates, what is FIQ?

\subsection{Harmonization - results}

%write about NYU_2, KUL_3 that only have asd data and result in NaN values replaced by original ones.
% data do not present a clear age-trend  (and FIQ?)
Harmonization procedure was tested on the whole dataset to have a visual representation of how features were modified once the harmonization was applied

To test harmonization procedure and evaluate its performances in making sites' feature uniform, we used NeuroHArmonize package on 1494 patients coming from ABIDE I and ABIDE II of which 722 controls and 772 cases,
We splitted them into train and test groups, then, for each train dataset, using NeuroHarmonize, the harmonization model was estimated on control subjects only, so these subjects were harmonized to remove site-dependent features and the chosen covariates were the age of the subject and the full intellective quotient. The model was then applied to the case subjects belonging to train, and to both case and controls subjects of the test set.
A random forest regressor was trained on this dataset attempting to make a binary classification of the site, for each pair of sites.

This aproach, allows train and test dataset to be still independent, because harmonization model was only trained on Train subjects and once learnt the parameters, is applied to the Test dataset.

Figure \ref{fig:heatmap_harmonization} shows the result of classification on the dataset described above. Scores are reported in terms of AUC for the classification of each pair of sites, and sites along the x and y axes are ordered by incresging average age of patients coming from that site.
Specifically, fig \ref{fig:heatmap_harmonization_noharmon} shows the AUC score of the model trained with the raw, not harmonized dataset, it is clear from this binary classification that almost every site is well distinguishable from the others by the value of AUC, almost uniformly near to 1.
On the contrary, in fig \ref{fig:heatmap_harmonization_harmon} values are lower, meaning that sites are not so distinguishable anymore, however, a certain distinctive features are still present for sites presenting high difference in mean age.
This is a consequence of choosing age as covariate, which preserves differences in age-dependent features.




\begin{figure}
\centering
\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/matrix}
   \caption{}
   \label{fig:heatmap_harmonization_harmon}
\end{subfigure}

\begin{subfigure}[b]{0.8\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/harmon_matrix}
   \caption{}
   \label{fig:heatmap_harmonization_noharmon}
\end{subfigure}

\caption{Binary classification}
\label{fig:heatmap_harmonization}
\end{figure}

\cite{pomponio}
To have a visual representation of how much each feature is modified in respect of its sourced site, in fig \ref{fig:features_raw-harmo} are shown two features among 5995: \emph{feature 324} and \emph{feature 2800} as a function of sites.
For a clearer idea of what is shown, feature 324 represents the correlation between right and left inferior frontal gyrus, and feature 2800 that between left precuneous cortex and the left inferior frontal gyrus
Their values are plotted as a box along the y axis and sites are indicated along the x-axis, it is clear how harmonization is a powerful tool to level up feature and remove site-dependent information from them.

\begin{figure}
\centering
\begin{subfigure}[b]{1.\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/Feature324raw-harmo}
   \caption{}
   \label{fig:feature324}
\end{subfigure}
\begin{subfigure}[b]{1.\textwidth}
   \includegraphics[width=1\linewidth]{/home/federico/magistrale/Imagess/Feature2800raw-harmo}
   \caption{}
   \label{fig:feature2800}
\end{subfigure}
\caption{Features}
\label{fig:features_raw-harmo}
\end{figure}


\section{Domain-adversarial NN}
An other approch is trying to train a neural network to recognise discriminative features, making train as much site-independent as possible.
The genral idea underlying this network comes from a reviewed and readapted version of the solution of the problem tackled in Ganin - Ustonova article
%Insert article reference
Their work takes place when we possess two different datased let's call them a Source and a Target dataset, containing features following two different distributions.
Moreover, source dataset is labeled while Target is unlabeled, so their goal was to create a network able to distinguish relevant features from the labeled dataset and make them domain independent.
We illustrate the main aspects of this problem using a shallow neural network with a single hidden layer consisting of D nodes.
Let's suppose the network takes as input an m-dimensional features vector;
the hidden layer is then a function $G_f : R^m \rightarrow R^D$ of the type $G_f(x; \textbf{W}, \textbf{b}) = f(W\cdot x + b)$, where $\textbf{W}$ and $\textbf{b}$ are the matrix and vector parameters to be optimized.
Similarly the output will be a function $G_y:R^D \rightarrow [0, 1]$ of the type $G_y(G_f(x; W, b); V, c) = f'(V\cdot G_f(x) + c)$.
Where V and c are parameters to be optimized: a vector and a scalar respectively.
A usual train carried on only on the (labeled) Source domain, will therefore bring to the minimization of the quantity
\begin{equation}
\underset{W, b, V, c}{min} \left[ \frac{1}{n}\sum_1^n L_y^i(Gy(G_f(x_i)), y_i | W, b, V, c)\right]
\end{equation}.
To tackle the domain independence problem, an other term is added to the summation: a term coming from a domain classifier layer: $G_d:R^D \rightarrow [0, 1]$ of the type $G_d(G_f(x; W, b); u, z) = f'(V\cdot G_f(x) + c)$ that learns the vector and scalar parameters u and z, by trying to classify the domain.
With this addition, the complete quantity to be optimized becomes
\begin{equation}
%complete this equation
E(W, b, V, c, u, z) = \frac{1}{n} \sum_{i = 1}^n
\end{equation}
So the purpose of this kind of training is to find a saddle point by finding minimizing parameters for feature loss and maximizing those related to the site loss, namely
\begin{equation}
(\hat \theta_f, \hat \theta_y) = \underset{\theta_f, \theta_y}{argmin} E(\theta_f, \theta_y, \hat \theta_d) \qquad \qquad \hat \theta_d = \underset{\theta_d}{argmax} E(\hat \theta_f, \hat \theta_y, \theta_d)
\end{equation}

This could be achieved by training a network in an adversarial way, trying to minimize the category label loss and maximize site label loss.
this is achieved by adding a domain classifier branch just after a main feature extractor branch.
The domain classifier branch is characterized by a \emph{gradient reversal layer} at the top of it.
This layer has no parameters and don't act during forward propagation, but, during backpropagation, it inverst all the weight from the subsequent layer before passing them to the preceding one.
So as shown in figure -insert figure- the network is forked after the feature extractor branch, on one side, he starts the label predictor branch, and on the other, there's the domain classifier branch with the gradient reversal layer at the beginning.
The setup of this work, differs from Ganin's because we don't have a labeled Source and an unlabeled Target domain, but they are both labeled, so there's no need to keep them separated and also, in our case, the Target domain is not just made of two domains, but of as many sites our data belong to.

In this work, the label classifier branch ends with a sigmoid function wich return only one class label $y_i \in [0, 1]$ while the domain classifier branch ends with a multi-node layer of a number equal to the total sites' number.
An adversarial loss is also implemented, aimed to aviod that if, for instance, the domain classifier, correctly classify one input, the computed error would be low, and it would have a low contribution during the back propagation, and leaving the weights almost unmodified after the backpropagation, and therefore leaning towards that site.

\section{Results of deep learning}

In our work we employed a simple deep neural network architecture illustrated in figure .. with 2 hidden made up of 64, and 32 nodes respectively activated by a ReLU function and each layer separated from the next by a Batch-Normalization and a 0.3-Dropout layer. After the 32-node layer we used a single output layer with a single node activated by a sigmoid function, for classification.
Since we are working in an underlying overfitting condition we choose this network structure after a grid search to find the minimal parameters' configuration which allows best classification scores.
Overfitting is clear if we take a look at the learning curves in figure \ref{fig:learningcurve} which shows the training and validation AUC curves for two model: one of them (\ref{fig:auc_no3-2-1}) even if too much simplified, shows the classical trend of an overfitting state, however, it performes a bit worse than the other more complex models.

\begin{figure}
\centering
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no64-8-1}
   \caption{AUC learning curve corresponding to a model with a structure 64-8-1 nodes trained on the entire not-harmonized dataset}
   \label{fig:auc_no64-8-1}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no8-8-1}
   \caption{AUC learning curve corresponding to a model with a structure 8-8-1 nodes trained on the entire not-harmonized dataset}
   \label{fig:auc_no8-8-1}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
   \includegraphics[width=1\linewidth]{ml/auc_no3-2-1}
   \caption{AUC learning curve corresponding to a model with a structure 3-2-1 nodes trained on the entire not-harmonized dataset}
   \label{fig:auc_no3-2-1}
\end{subfigure}
\caption{}
\label{fig:learningcurve}
\end{figure}




In our model, as loss function we used binary crossentropy and the neural network's weights were optimized via Adam optimizer with a learning rate of $10^{-4}$ and model's performancies were assessed with a 10 or 5 k-fold Cross validation, collecting the AUC score for each fold and then computing mean and std deviation of these results.

The other neural network employed was the site-adversarial neural network: following a similar structure as the DNN, it can be divided into three main components: the first feature extraction branch comprising the DNN up to the 32-node layer;
from this point the network is splitted into two branches: one categorical classificator, along the lines of the DNN consists of the last outpput layer with a sigmoiod activation function, and the other branch, the site classifier consisting of a gradient reversal layer as the first layer followed by a layer with 16 nodes, and after a batch Batch-Normalization layer, the multi-output layer with N nodes, being N the total number of sites input data belong to.
This final layer is activated by a softmax function, and the loss function employed for site classification is the categorical crossentropy implemented using a custom function to deal with a possible low crossentropy score when by chance the site classifier, categorize one or more input data's sitelabel correctly, which would have low impact on the whole branch classifier making the adversarial strategy useless.


We mainly carried out three different kind of analysis using this DNNs, and compared them with the scores obtained from the adversarial network.
Specifically we performed: 1) classification of raw (not harmonized) data, 2) classification on the whole harmonized dataset, creating the harmonization model on all control subjects and applying the model on all ASD subjects, and 3) classification with harmonization implemented inside a K-fold cross-validation, creating the model on controls and applying it to ASD, and 4) classification using the domain adversarial neural network.

Following this procedure we implement a k-fold cross validation using stratifiedKFold provided by sklearn library. Data were splitted into a train and a test dataset, therefore using train data we computed the harmonization model only on control patients and then apply the model to asd patient and both Control and ASD Test data.
This is the best way to mantain train and test data as much separated as possible and avoid data leakage between train and test.

Analysis were carried out using different selection criterias and thresholds which brought us to different datasets combination:

The first classification attempt was using the whole dataset from ABIDE 1 and 2, consisting of only male patients aged between 5 and 40 years old, without any further constraint on eye status at scan.

Then, with these same constraint we tried to classify separately ABIDE 1 and ABIDE 2 datasets.

The second analysis was performed on a dataset with the same constraint as above, but with an additional condition on eye status choosing to select only patient who kept eyes open throughout the entire scan time.

The latter analysis were carried out with these same constraints, but separately on ABIDE 1 and ABIDE 2 dataset.

A further analysys concerns the feature's dimensionality reduction, that is the reduction of features from the input dataset using PCA
For each one of the analysis above, PCA was implemented to restrain the effect of overfitting.

From the initial input data consisting of 5995 features, we tried to classify using 20, 50, 100 and 200 principal components

\subsection{Results with pearson coefficients}

%If the table is too wide, replace \begin{table}[!htp]...\end{table} with
%\begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]...\end{threeparttable}\end{adjustwidth}
\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lccc}\toprule
Dataset &Harmon K-fold &Harmon. upstream &No harmon. \\\cmidrule{1-4}
AB I+II &70 $\pm$ 2 &74 $\pm$ 2 &70 $\pm$ 1 \\\cmidrule{1-4}
AB I &70$\pm$ 3 &71 $\pm$3 &73$\pm$ 5 \\\cmidrule{1-4}
AB 2 &63 $\pm$5 &68$\pm$ 6 &64$\pm$ 4 \\\cmidrule{1-4}
AB I + II - EYE open &70 $\pm$4 &71$\pm$ 4 &70 $\pm$3 \\\cmidrule{1-4}
AB I EYE open &69$\pm$ 4 &73$\pm$ 7 &71 $\pm$4 \\\cmidrule{1-4}
AB II EYE open &69 $\pm$2 &69$\pm$ 6 &69 $\pm$3 \\\cmidrule{1-4}
AB I + II AGE=all, EYE=all &70$\pm$ 1 &75 $\pm$1 &70 $\pm$2 \\\midrule
\bottomrule
\end{tabular}
\caption{AUC score obtained from three different kind of analysis}\label{tab:aucpearson}
\end{table}





\subsection{Results with Wavelet coefficients}








\section{Explainable AI (XAI) with SHAP}
\subsection{SHAP - theory}
SHAP (SHapley Additive exPlanation) is a technique based on game theory wich provides a method for model explanability.
Let's say we have a coalition game: N players, and different subsets created from these players. Each subset of players S gain a payoff at the end of the game. A function $\nu$ maps every subset to its payoff. $\nu(S_i) = payoff_i$. Based on which player had more influence in the final score, we ask how to split this payoff in a fair way between each player of the subset.
To this end we use \textbf{shapley values}: a solution concept (i.e. a formal rule to predict how a game will be played) $\phi_i(\nu)$. It represents the incremental benefit of including player i in a the subset and average over all subsets that include player i.
They are specific for each player i and represents the marginal contribution of that player to the final score, in other words they are the difference between the group's score with player i and the group's score without player i.
To apply this concept to a machine learning model we just substitute the payout is the model prediction and each player is the value of a single feature.
Shap value is then a weighted average of the marginal contribution of a feature (player) across all possible coalitions

For each player i and a map function $\nu$ shap values are defined as:

%Need to write \ symbol
\begin{equation}
\phi_i(\nu) = \frac{1}{N} \sum_{S\subseteq N\ \setminus \left \{ i \right \} }  {N- 1 \choose \left |S \right |}^{-1} \left ( \nu (S u \left\{ i \right\}) - \nu(S)\right)
\end{equation}
To get the idea of how we can explain a model, we start asking: how much is a prediction driven by the fact that a feature has a certain value A, instead of its baseline value $\bar A$? the baseline value $\bar A$ is the result of
To answer this question we introduce a coalition vector z' = $\left\{ 0, 1 \right\}^F$ : where F is the total number of features; we have then a binar vector made by zeros and ones where zero means that we withheld the corrisponding variable from our analysys, and the one that we are including it.
To map the x' vector to the corresponding feature values vector a function $h_x(x')$ = x which returns the actual features' value for the corresponding input vector x. Thus we aim to study a simplified model g(z') which is equal to f($h_x(z')$) if z' $\approx$ x'. The most important properties of this models is called local accuracy and states that g and f must give the same result when passing all the features: g has to match f when x = $h_x(x')$.
We attribute a contribution $\phi_i$ to each simplified vector $z_i'$
\begin{equation}
g(z') = \phi_0 + \sum_{i = 0}^N \phi_i z_i'
\end{equation}
The solution to this problem exists and is unique, it's shapley values.
To assign the importance value to a feature we need to train two different models: one with that feature included and the second one with that feature excluded and thereafter compute the difference between the two models' prediction. computing this differences for all possible subsets S$\subseteq F \setminus \left\{i\right\}$.
We are then going to use them to explain a deep learning model with the SHAP method. There are different way of approximating the computation of SHAP values like KernelSHAP which is a model agnostic explanation method, or Deep SHAP which employs shapley values into DeepLIFT algorithm.
DeepLIFT aims to compute the difference in output deriving with an input different from some \textquotedblleft reference \textquotedblright input chosen depending on the data we are working on.
Let t represent the output of some inner neuron and let x1, x2, ... , $x_n$ be some preceding neurons necessary to compute t, if we label $t_0$ the reference output, we can compute the value $\Delta t = t-t_0$ and use them to define the DeepLIFT values $C_{\Delta x_i \Delta t}$ so that $\sum_i = 1 ^n C_{\Delta x_i \Delta t} = \Delta t$. For a given input neuron x with difference from reference $\Delta x$ and a target neuron t, is defined a multiplier $m_{\Delta x \Delta t} = \frac{C_{\Delta x \Delta t}}{\Delta x}$ which represents the contribution of $\Delta x$ to $\Delta t$.
%maybe if I want I can continue explaining DeepLIFT
Deep SHAP combines shap values for smaller components of a network to compute valuer for the whole network, it does so recursively passing Deep LIFT multipliers during backpropagation: we can indeed re-writing multiplier values in terms of shap values, for a target neuron f3 we have $m_{x_j f_3} = \frac{\phi_i(f_3, x)}{x_j - E[x_j]}$ obtaining $\phi_i(f_3, y) \approx m_{y_i, f_3}(y_i - E[y_i])$

To express the significance of a feature we plotted them using SHAP feature importance, based on the relation between feature importance and shap value: feature with high absolute shapley value are important. The importance value for a feature j is then calculated from $I_j = \frac{1}{n} \sum_{j = 1}^n \|\phi_j^{(i)}\|$

\subsection{SHAP - results}


\appendix

\section{Principles of MRI imaging}
Nuclear magnetic resonance imaging, is a widespread imaging technique It's widely employed in radiology to obtain anatomical images or study physiological processes, taking advantage of its flexible sensitivity to different tissues
It's based on the interaction of a nuclear spin of any tissue's molecule, with an external magnetic field $\textbf B_0$.
The principal molecule we are referring to is water, and subsequently, hydrogen is the dominant nucleus in MRI.
When a nucleus undergoes an external magnetic field, the interaction would make the angular moment $\overrightarrow{\mu}$ align with the magnetic field,
% Talk about energy levels. Review lines below.
but, since protons have a thermal energhy associated with the temperature T, they are not static, so they start precessing around the symmetry axes given by the direction of the field $\overrightarrow{B_0}$.
The force applied to a spinning proton under a magnetic field is given by
\begin{equation}
\overrightarrow{\tau} = \overrightarrow{\mu} \times \overrightarrow{B}
\end{equation}
The rotation frequency $\omega_0$ of the spinning proton is given by the Larmor equation
\begin{equation}\label{eq:larmorfrequency}
\omega_0 = \gamma B_0
\end{equation}
where $\gamma$ is a constant related to the proton, in wates it has a value of $\gamma = 2.7\cdot 10^8 \frac{rad\cdot s}{T}$ and $B_0$ is the external magnetic field. For a typical scanner with a magnetic field of 2-3 T, Larmor frequency assumes values of $\sim MHz$ just below radio waves.
proton spins can assume two values: up and down in relation to the external field direction ; after a body  underwent a magnetic field, its total magnetization is given by the net difference between spins up and down
% spin excess? p.4
% write abou quantum energy levels
This net magnetization causes a flux which can be detected by an external coil only if it is perturbated from its equilibrium state. This is achieved by using another external magnetic field under the form of a radiofrequency impulse, with a frequence resonting with the Larmor frequence of preprocessing. This flips the spin in a direction usually perpendicular to the original $B_0$ one, which is supposed to be along $\hat z$ axis. Thus protons will start rotating in the x-y plane, still continuing preceding around their symmetry axis.
Doing so, they gradually lose their initial energy and tend to realign along z-axis.
This process is called relaxation and it follows an exponential decay with a time constant $T_1$ called longitudinal relaxation (longitudinal in relation to the original $B_0 \hat z$ direction), or spin-lattice relaxation, referring to the loss of thermal energy through interaxtion with surroading lattice. The total magnetization along z-axis during time, after the rf impulse, will then regrow along z-axis following:
\begin{equation}
M_z(t) = M_0 (1-e^{-t/T_1})
\end{equation}.
An other effect to be take into account is the interaction of spin with a the local field where the external field and the neighbor's one coexist. This spin-spin interaction causes the dephasing of spin. This brings to two different relaxation velocities along z-axis and x-y plane called transversal relaxation which occurs without energy exchange, and it's characterized by a time constant $T_2$. The two relaxation processes are described by Bloch equations and once integrated bring to the equation of the evolution of magnetization over time.
\begin{equation}
M_{x, y}(t) = M_{x,y}(0)\cdot e^{-t/T_2}
\end{equation}
In a real system, however, there's and additional dephasing source, coming from external field inhomogeneities.
This effect is often taken into account by the introduction of a different decay time $T_2'$ which along with $T_2$ bring to a overall time costant given by $\frac{1}{T_2*} = \frac{1}{T_2}+\frac{1}{T_2'}$.
Sometimes, though, this additional $T_2'$ is so small compared to $T_2$, that dominates over it, resulting in a rapid loss of information.
This can be avoided employing a specific rf pulse sequence called spin-echo method.
\subsection{spin-echo method}
The spin echo method employes two rf pulses the first with an angle of $\pi/2$ and the second of $\pi$.
\begin{enumerate}
% Add a figure like on page 123
\item The first radio-frequency pulse is applied with an angle of $\pi/2$ and this produces the spin to flip over a transversal plane and they gradually start dephasing because they undergo small field nonuniformities different from point to point, so they start fanning out, some move faster and some are delayed in relation to the average magnetization vector
\item The second rf impulse is sent, with an angle of $\pi$ to the transversal magnetization axis y'. This ensures that all the accumulated phases flip and become negative,
\item Late and early phases continue to accumulate delay, but now this delay pushes them towards the main magnetization vector,and the phase will therefore return zero. The elapsed time up to this moment is called \emph{Time to echo} $T_E$.
\end{enumerate}
\subsection{Image acquisition and k-space}
For imaging acquisition, both effects must be taken into account. Moreover, it is necessary to relate a signal with a spatial position.
In order to do so, in addition to the initial static magnetic field $B_0$ there's another field with lower intensity than $B_0$ which is not uniform in intensity, it follows a spatial gradient so that the total magnetic field along $\hat z$-axis is given by the sum of this two contributes, and the signal contains space-varying frequency components according to equation \ref{eq:larmorfrequency} which can be rewritten as $\omega(z) = \gamma B(z)$ being x the spatial coordinate and with $B_z$ now given by the relation $B_z(z, t) = B_0 + z\cdot G(t)$.
The physical process underlying signal acquisition is Faraday induction, according to which an electrical potential is related to the variation of a magnetic flox over time $fem = -\frac{d\Phi}{dt}$ being $\Phi$ the varying flux through the receiving coil.
The varying flux is obtained just after the application of the rf pulse, while tipped spins are precessing and realigning to z-axis, and all this process is called free induction decay (FID). This coil detect only the signal deriving from transversal plane because the one along longitudinal axes would be saturated from the strong magnetic field of the main $B_0$ coil. Changes in acquisition sequences and rf pulses allow us to emphasise one of the three fundamental parameters $T_1, T_2, \rho$ where $\rho$ is the proton concentration inside the tissue.

An image acquired with a spin echo sequence can be T1, T2 or proton density weighted

The acquired signal is examined by inverse Fourier transform, exploiting the differences from Larmor frequency which cause a phase displacement along z-axis $\phi_G(z, t)$
\begin{equation}
s(t) = \int_0^tdz\rho \ (9.14) \ pag \ 145
\end{equation}

Data are acquired under the form of a matrix called k-space.
K-space is a coordinate system used to store spatial frequencies information before applying the inverse fourier transform.
Low spatial frequencies, corresponding to large object across the whole real image, are encoded at the center of the matrix and high spatial frequencies corresponding to small objects and finer detailes, are encoded in the peripheries.

The construction of k-space is done step by step in relation to the gradient applied time by time, producing a trajectory on the k-space.
Each spatial line of k-space is acquired after TR, so the signal has to be recreated each time.
Starting from k = 0, the center of k-space, after a preencoding gradient, we move toward negative position of both k-phase and k-frequency; subsequently a frequency encoding gradient (also called readout gradient) is applied and the signal is collected along the frequency encoding direction.
The signal is then reformed and the process is repeated, from k = 0, to a new k-phase position and an other line of k-frequency is acquired.



\subsection{fMRI}
Functional MRI aims to measure brain activity by detecting changes in blood linked to a neuronal activation.

This kind of analysis is referred as blood-oxygen-level dependent (BOLD) imaging.


The process that from a neuronal stimulus leads to a measurable blood signal is governed by the hemodynamic response function (HRF).
As shown in figure \ref{fig:hrf} after a neuronal stimulus on a timescale of $\approx$ 10-100 ms, the BOLD signal takes $\approx$ 6s to reack a peak and a total of 20-30 s to return to its zero baseline.

\begin{figure}[h]
\includegraphics[width=8cm]{mri/hrf}
\caption{}
\label{fig:hrf}
\end{figure}



Signal comes from changes of blood-oxygen level of hemoglobin: an iron molecule carying oxygen throughout veins and capillaries.
Hemoglobin can be either oxyhemoglobin or deoxyhemoglobin, these two molecules differs in their magnetic susceptibility as oxyhemoglobin is diamagnetic while deoxyhemoglobin is paramagnetic.
The presence of deoxy-hemoglobin causes local field inhomogeneities, leading to a reduction of signal from water molecules, because molecules go out of phase with one another more quickly, reducing the total magnetization.

This effect is characterized by $T_2^{\ast}$ relaxation time so this is the tuning parameter related to this kind of acquisition; this gives the alternative name of BOLD-images as $T_2^{\ast}$-weighted images.


Three different types of tissues are present in the brain: cerebrospinal fluid (CSF), and gray and white matter.
\subsubsection{Gradient Echo sequences}
The sequence employed to acquire functional imaging is the Echo Planar Imging (EPI) which is a particular sequence from the family of gradient echo sequences.
Gradient echo sequences differs from spin echo because they allow a faster acquisition time because they use an excitation flip angle less than 90 deg, this allows a minor time interval to make spins back to their longituginal component as they are not completely flipped on the transverse plane.
The transverse component just created decay and dephase according to T2*. If a gradient is applied, they dephase faster, and it is then reversed to make them rephase.
In an gradient echo sequence, the echo peak lies on T2* decay curve while in a spin echo, it lies on T2 decay curve; and thanks to whis it is susceptible to any uniformity variation among which that due to hemoglobin variation.
Since TR are short there is some transversal magnetization left after the acquisition, in \emph{spoiled} Gradient Echo, the residual magnetization is wiped out.
Ultra fast gradient echo start with a 180 degrees pulse, to invert longitudinal magnetization, in order to increase T1 weighting, and allk-space lines are acquired after this impulse. This first pulse is followed by minor pulses, and k-space is acquired after each echo.


\subsubsection{EPI}
During an EPI session, a single rf inpulse of 90 degrees is applied and the whole k-space is acquired in a single acquisition:
as shown in figure \ref{fig:epi-gradients} after the initial RF pulse, together with the gradient for slice selection (G-Select), the acquisition of k-space information starts from the left bottom as in figure \ref{fig:epi-kspace} and each line along k-frequency corresponging to the readout gradient (G-Readout) is acquired.
The period of the G-Readout is the echo time $T_E$, and it's modulated such that is sensible to $T_2^{\ast}$ setting $T_E \approx T_2^{\ast}$.
During the acquisition of the whole k-space each line is shifted from the other along k-phase direction by applying brief pulses of the phase encoding gradient (G-Select) called blip, creating a snake-like path during the acquisition of the k-space matrix.
%shmitt tuner -echo planar Imaging


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{mri/epi-gradients}
\caption{}
\label{fig:epi-gradients}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{mri/epi-kspace}
\caption{}
\label{}
\end{subfigure}
\caption{}
\label{fig:epi}
\end{figure}


Pixel-to-pixel frequency difference in the phase-encoding direction emphasizes chemical shift artifact.
EPI generates gradient echoes under the FID curve created by the RF flip. As the curve decays according to T2*, gradient EPI sequences willl be T2* weighted.
Sensitivity to T2*, of course, introduces sensitivity to artifacts caused by changes in magnetic susceptibility (eg, air/tissue interfaces) and imperfect magnet shim





\bibliographystyle{plain}
\bibliography{biblio.bib}



\end{document}
